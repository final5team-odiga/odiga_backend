import os
import json
import re
from typing import Dict, List
from crewai import Agent, Task, Crew
from custom_llm import get_azure_llm
from utils.pdf_vector_manager import PDFVectorManager

class OrgAgent:
    """PDF ë²¡í„° ë°ì´í„° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        self.llm = get_azure_llm()
        self.vector_manager = PDFVectorManager()
        
    def create_layout_analyzer_agent(self):
        """ë ˆì´ì•„ì›ƒ ë¶„ì„ ì—ì´ì „íŠ¸"""
        return Agent(
            role="Magazine Layout Analyzer",
            goal="PDF ë²¡í„° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ì— ìµœì í™”ëœ ë§¤ê±°ì§„ ë ˆì´ì•„ì›ƒ ì¶”ì²œ",
            backstory="""ë‹¹ì‹ ì€ ë§¤ê±°ì§„ ë ˆì´ì•„ì›ƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ì‹¤ì œ ë§¤ê±°ì§„ PDFì—ì„œ ì¶”ì¶œí•œ ë²¡í„° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ 
            ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ì— ê°€ì¥ ì í•©í•œ ë ˆì´ì•„ì›ƒ êµ¬ì¡°ë¥¼ ì°¾ì•„ë‚´ê³ ,
            ì „ë¬¸ì ì¸ ë§¤ê±°ì§„ ìˆ˜ì¤€ì˜ í…ìŠ¤íŠ¸ ë°°ì¹˜ë¥¼ ì„¤ê³„í•˜ëŠ” ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.""",
            llm=self.llm,
            verbose=True
        )
    
    def create_content_editor_agent(self):
        """ì½˜í…ì¸  í¸ì§‘ ì—ì´ì „íŠ¸"""
        return Agent(
            role="Magazine Content Editor",
            goal="ë²¡í„° ë°ì´í„° ê¸°ë°˜ ë ˆì´ì•„ì›ƒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ë§¤ê±°ì§„ ìŠ¤íƒ€ì¼ë¡œ í¸ì§‘",
            backstory="""ë‹¹ì‹ ì€ ë§¤ê±°ì§„ ì½˜í…ì¸  í¸ì§‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ì‹¤ì œ ë§¤ê±°ì§„ì˜ ë ˆì´ì•„ì›ƒ íŒ¨í„´ì„ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ
            í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ í•´ë‹¹ ë ˆì´ì•„ì›ƒì— ìµœì í™”í•˜ì—¬ í¸ì§‘í•˜ë©°,
            ë…ìì˜ ì‹œì„ ì„ ì‚¬ë¡œì¡ëŠ” ë§¤ë ¥ì ì¸ ë§¤ê±°ì§„ ì½˜í…ì¸ ë¥¼ ë§Œë“œëŠ” ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
            íŠ¹íˆ ì„¤ëª… í…ìŠ¤íŠ¸ë‚˜ ì§€ì‹œì‚¬í•­ì„ í¬í•¨í•˜ì§€ ì•Šê³  ìˆœìˆ˜í•œ ì½˜í…ì¸ ë§Œ ìƒì„±í•©ë‹ˆë‹¤.""",
            llm=self.llm,
            verbose=True
        )
    
    def process_content(self, magazine_content, available_templates: List[str]) -> Dict:
        """PDF ë²¡í„° ë°ì´í„° ê¸°ë°˜ ì½˜í…ì¸  ì²˜ë¦¬"""
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
        all_content = self._extract_all_text(magazine_content)
        content_sections = self._analyze_content_structure(all_content)
        
        print(f"OrgAgent: ì²˜ë¦¬í•  ì½˜í…ì¸  - {len(all_content)}ì, {len(content_sections)}ê°œ ì„¹ì…˜")
        
        # ì—ì´ì „íŠ¸ ìƒì„±
        layout_analyzer = self.create_layout_analyzer_agent()
        content_editor = self.create_content_editor_agent()
        
        # ê° ì„¹ì…˜ë³„ë¡œ ë²¡í„° ê¸°ë°˜ ë ˆì´ì•„ì›ƒ ë¶„ì„ ë° í¸ì§‘
        refined_sections = []
        
        for i, section_content in enumerate(content_sections):
            if len(section_content.strip()) < 50:
                continue
            
            print(f"ğŸ“„ ì„¹ì…˜ {i+1} ì²˜ë¦¬ ì¤‘...")
            
            # 1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬í•œ ë ˆì´ì•„ì›ƒ ì°¾ê¸°
            similar_layouts = self.vector_manager.search_similar_layouts(
                section_content[:500],  # ì²˜ìŒ 500ìë¡œ ê²€ìƒ‰
                "magazine_layout",
                top_k=3
            )
            
            # 2ë‹¨ê³„: ë ˆì´ì•„ì›ƒ ë¶„ì„
            layout_analysis_task = Task(
                description=f"""
                ë‹¤ìŒ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ì™€ ìœ ì‚¬í•œ ë§¤ê±°ì§„ ë ˆì´ì•„ì›ƒì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”:
                
                **ë¶„ì„í•  ì½˜í…ì¸ :**
                {section_content}
                
                **ìœ ì‚¬í•œ ë§¤ê±°ì§„ ë ˆì´ì•„ì›ƒ ë°ì´í„°:**
                {self._format_layout_data(similar_layouts)}
                
                **ë¶„ì„ ìš”êµ¬ì‚¬í•­:**
                1. **ë ˆì´ì•„ì›ƒ íŒ¨í„´ ë¶„ì„**
                   - í…ìŠ¤íŠ¸ ë¸”ë¡ì˜ ìœ„ì¹˜ì™€ í¬ê¸° íŒ¨í„´
                   - ì œëª©ê³¼ ë³¸ë¬¸ì˜ ë°°ì¹˜ ê´€ê³„
                   - ì—¬ë°±ê³¼ ê°„ê²©ì˜ í™œìš© ë°©ì‹
                
                2. **ì½˜í…ì¸  ì í•©ì„± í‰ê°€**
                   - í˜„ì¬ ì½˜í…ì¸ ì™€ ë ˆì´ì•„ì›ƒì˜ ë§¤ì¹­ë„
                   - í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ë ˆì´ì•„ì›ƒ ìš©ëŸ‰ì˜ ì í•©ì„±
                   - ì½˜í…ì¸  ì„±ê²©ì— ë§ëŠ” ë ˆì´ì•„ì›ƒ ìŠ¤íƒ€ì¼
                
                3. **í¸ì§‘ ì „ëµ ìˆ˜ë¦½**
                   - ë§¤ë ¥ì ì¸ ì œëª© ìƒì„± ë°©í–¥
                   - ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë¶„í•  ë° êµ¬ì¡°í™” ë°©ì•ˆ
                   - ë…ì ëª°ì…ë„ í–¥ìƒì„ ìœ„í•œ í…ìŠ¤íŠ¸ ë°°ì¹˜
                
                **ì¶œë ¥ í˜•ì‹:**
                ì œëª©: [êµ¬ì²´ì ì´ê³  ë§¤ë ¥ì ì¸ ì œëª©]
                ë¶€ì œëª©: [ê°„ê²°í•˜ê³  í¥ë¯¸ë¡œìš´ ë¶€ì œëª©]
                í¸ì§‘ë°©í–¥: [ì „ì²´ì ì¸ í¸ì§‘ ë°©í–¥ì„±]
                """,
                agent=layout_analyzer,
                expected_output="ë²¡í„° ë°ì´í„° ê¸°ë°˜ ë ˆì´ì•„ì›ƒ ë¶„ì„ ë° í¸ì§‘ ì „ëµ"
            )
            
            # 3ë‹¨ê³„: ì½˜í…ì¸  í¸ì§‘
            content_editing_task = Task(
                description=f"""
                ë ˆì´ì•„ì›ƒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ ì „ë¬¸ ë§¤ê±°ì§„ ìˆ˜ì¤€ìœ¼ë¡œ í¸ì§‘í•˜ì„¸ìš”:
                
                **ì›ë³¸ ì½˜í…ì¸ :**
                {section_content}
                
                **ë§¤ê±°ì§„ ìŠ¤íƒ€ì¼ í¸ì§‘ ì§€ì¹¨:**
                1. **ì‹œê°ì  ê³„ì¸µ êµ¬ì¡°**: ì´ë¯¸ì§€ í¬ê¸°ì™€ ë°°ì¹˜ì— ë§ëŠ” í…ìŠ¤íŠ¸ êµ¬ì¡° ìƒì„±
                2. **ë‹¤ì´ë‚˜ë¯¹í•œ ë ˆì´ì•„ì›ƒ**: ëŒ€í˜•/ì¤‘í˜•/ì†Œí˜• ì´ë¯¸ì§€ì™€ ì¡°í™”ë˜ëŠ” í…ìŠ¤íŠ¸ ë°°ì¹˜
                3. **ë§¤ê±°ì§„ íŠ¹ìœ ì˜ ë¦¬ë“¬**: ê¸´ ë¬¸ë‹¨ê³¼ ì§§ì€ ë¬¸ë‹¨ì˜ ì¡°í™”ë¡œ ì‹œê°ì  ë¦¬ë“¬ ìƒì„±
                4. **ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ìƒí˜¸ì‘ìš©**: ì´ë¯¸ì§€ ì£¼ë³€ì— ë°°ì¹˜ë  í…ìŠ¤íŠ¸ì˜ í†¤ê³¼ ê¸¸ì´ ì¡°ì ˆ
                5. **í¸ì§‘ ë””ìì¸ ê³ ë ¤**: ì‹¤ì œ ë§¤ê±°ì§„ì²˜ëŸ¼ í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ì§€ì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì–´ìš°ëŸ¬ì§€ë„ë¡
                
                **ë²¡í„° ë°ì´í„° ê¸°ë°˜ ìµœì í™”:**
                - ê²€ìƒ‰ëœ ë§¤ê±°ì§„ ë ˆì´ì•„ì›ƒì˜ í…ìŠ¤íŠ¸ ë°°ì¹˜ íŒ¨í„´ ì ìš©
                - ì´ë¯¸ì§€ í¬ê¸°ë³„ í…ìŠ¤íŠ¸ ë¶„ëŸ‰ê³¼ ìŠ¤íƒ€ì¼ ì¡°ì ˆ
                - ë§¤ê±°ì§„ íŠ¹ìœ ì˜ ë¹„ëŒ€ì¹­ ê· í˜•ê° ë°˜ì˜
                
                **ì¶œë ¥:** ë§¤ê±°ì§„ ë ˆì´ì•„ì›ƒì— ìµœì í™”ëœ í¸ì§‘ ì½˜í…ì¸ 
                """,
                agent=content_editor,
                expected_output="ë§¤ê±°ì§„ ìŠ¤íƒ€ì¼ ë ˆì´ì•„ì›ƒì— ìµœì í™”ëœ ì „ë¬¸ ì½˜í…ì¸ ",
                context=[layout_analysis_task]
            )

            
            # Crew ì‹¤í–‰
            crew = Crew(
                agents=[layout_analyzer, content_editor],
                tasks=[layout_analysis_task, content_editing_task],
                verbose=True
            )
            
            try:
                result = crew.kickoff()
                
                # ê²°ê³¼ íŒŒì‹±
                analysis_result = str(layout_analysis_task.output) if hasattr(layout_analysis_task, 'output') else ""
                edited_content = str(result.raw) if hasattr(result, 'raw') else str(result)
                
                # ì œëª©ê³¼ ë¶€ì œëª© ì¶”ì¶œ
                title, subtitle = self._extract_clean_title_subtitle(analysis_result, i)
                
                # í¸ì§‘ëœ ì½˜í…ì¸ ì—ì„œ ì„¤ëª… í…ìŠ¤íŠ¸ ì œê±°
                clean_content = self._remove_meta_descriptions(edited_content)
                
                refined_sections.append({
                    "title": title,
                    "subtitle": subtitle,
                    "content": clean_content,
                    "layout_info": similar_layouts[0] if similar_layouts else {},
                    "original_length": len(section_content),
                    "refined_length": len(clean_content)
                })
                
                print(f"âœ… ì„¹ì…˜ {i+1} í¸ì§‘ ì™„ë£Œ: {len(section_content)}ì â†’ {len(clean_content)}ì")
                
            except Exception as e:
                print(f"âš ï¸ ì„¹ì…˜ {i+1} í¸ì§‘ ì‹¤íŒ¨: {e}")
                # í´ë°±: ê¸°ë³¸ ì²˜ë¦¬
                refined_sections.append({
                    "title": f"ë„ì¿„ ì—¬í–‰ ì´ì•¼ê¸° {i+1}",
                    "subtitle": "íŠ¹ë³„í•œ ìˆœê°„ë“¤",
                    "content": section_content,
                    "layout_info": {},
                    "original_length": len(section_content),
                    "refined_length": len(section_content)
                })
        
        # í…œí”Œë¦¿ ë§¤í•‘
        text_mapping = self._map_to_templates(refined_sections, available_templates)
        
        total_refined_length = sum(section["refined_length"] for section in refined_sections)
        print(f"âœ… OrgAgent ì™„ë£Œ: {len(refined_sections)}ê°œ ì„¹ì…˜, ì´ {total_refined_length}ì")
        
        return {
            "text_mapping": text_mapping,
            "refined_sections": refined_sections,
            "total_sections": len(refined_sections),
            "total_content_length": total_refined_length,
            "vector_enhanced": True
        }
    
    def _extract_clean_title_subtitle(self, analysis_result: str, index: int) -> tuple:
        """ë¶„ì„ ê²°ê³¼ì—ì„œ ê¹¨ë—í•œ ì œëª©ê³¼ ë¶€ì œëª© ì¶”ì¶œ"""
        title_pattern = r'ì œëª©[:\s]*([^\n]+)'
        subtitle_pattern = r'ë¶€ì œëª©[:\s]*([^\n]+)'
        
        title_match = re.search(title_pattern, analysis_result)
        subtitle_match = re.search(subtitle_pattern, analysis_result)
        
        title = title_match.group(1).strip() if title_match else f"ë„ì¿„ ì—¬í–‰ ì´ì•¼ê¸° {index + 1}"
        subtitle = subtitle_match.group(1).strip() if subtitle_match else "íŠ¹ë³„í•œ ìˆœê°„ë“¤"
        
        # ì„¤ëª… í…ìŠ¤íŠ¸ ì œê±°
        title = self._clean_title_from_descriptions(title)
        subtitle = self._clean_title_from_descriptions(subtitle)
        
        # ì œëª© ê¸¸ì´ ì¡°ì •
        if len(title) > 40:
            title = title[:37] + "..."
        if len(subtitle) > 30:
            subtitle = subtitle[:27] + "..."
        
        return title, subtitle
    
    def _clean_title_from_descriptions(self, text: str) -> str:
        """ì œëª©ì—ì„œ ì„¤ëª… í…ìŠ¤íŠ¸ ì œê±°"""
        patterns_to_remove = [
            r'\(í—¤ë“œë¼ì¸\)',
            r'\(ì„¹ì…˜ íƒ€ì´í‹€\)',
            r'ë° ë¶€.*?ë°°ì¹˜.*?ìˆìŒ',
            r'í•„ì ì •ë³´.*?ìˆìŒ',
            r'í¬í†  í¬ë ˆë”§.*?ìˆìŒ',
            r'ê³„ì¸µì .*?ìˆìŒ',
            r'ê³¼ ë³¸ë¬¸.*?ê´€ê³„',
            r'ë°°ì¹˜.*?ê´€ê³„',
            r'ìƒë‹¨.*?ë°°ì¹˜',
            r'ì¢Œìƒë‹¨.*?ë°°ì¹˜',
            r'í˜¹ì€.*?ë°°ì¹˜',
            r'ì—†ì´.*?ì§‘ì¤‘',
            r'ê·¸ ì•„ë˜ë¡œ.*?ìˆìŠµë‹ˆë‹¤'
        ]
        
        clean_text = text
        for pattern in patterns_to_remove:
            clean_text = re.sub(pattern, '', clean_text, flags=re.IGNORECASE | re.DOTALL)
        
        # ì—°ì†ëœ ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        clean_text = re.sub(r'\s+', ' ', clean_text)
        clean_text = re.sub(r'^[,\s:]+|[,\s:]+$', '', clean_text)
        
        return clean_text.strip() if clean_text.strip() else "ë„ì¿„ ì—¬í–‰ ì´ì•¼ê¸°"
    
    def _remove_meta_descriptions(self, content: str) -> str:
        """ì½˜í…ì¸ ì—ì„œ ë©”íƒ€ ì„¤ëª… ì œê±°"""
        patterns_to_remove = [
            r'\*ì´ í˜ì´ì§€ì—ëŠ”.*?ì‚´ë ¸ìŠµë‹ˆë‹¤\.\*',
            r'ë¸”ë¡ì€ ê· í˜•.*?ì¤„ì—¬ì¤ë‹ˆë‹¤',
            r'\(ì‚¬ì§„ ìº¡ì…˜\)',
            r'ì‹œê°ì  ë¦¬ë“¬ê³¼.*?ì‚´ë ¸ìŠµë‹ˆë‹¤',
            r'ì¶©ë¶„í•œ ì—¬ë°±.*?ì™„ì„±í•©ë‹ˆë‹¤',
            r'ì‚¬ì§„ì€ ë³¸ë¬¸.*?ì™„ì„±í•©ë‹ˆë‹¤',
            r'ì´ ì½˜í…ì¸ ëŠ”.*?ë””ìì¸ë˜ì—ˆìŠµë‹ˆë‹¤'
        ]
        
        clean_content = content
        for pattern in patterns_to_remove:
            clean_content = re.sub(pattern, '', clean_content, flags=re.IGNORECASE | re.DOTALL)
        
        return clean_content.strip()
    
    def _format_layout_data(self, similar_layouts: List[Dict]) -> str:
        """ë ˆì´ì•„ì›ƒ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        if not similar_layouts:
            return "ìœ ì‚¬í•œ ë ˆì´ì•„ì›ƒ ë°ì´í„° ì—†ìŒ"
        
        formatted_data = []
        for i, layout in enumerate(similar_layouts):
            formatted_data.append(f"""
            ë ˆì´ì•„ì›ƒ {i+1} (ìœ ì‚¬ë„: {layout.get('score', 0):.2f}):
            - ì¶œì²˜: {layout.get('pdf_name', 'unknown')} (í˜ì´ì§€ {layout.get('page_number', 0)})
            - í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {layout.get('text_content', '')[:200]}...
            - ì´ë¯¸ì§€ ìˆ˜: {len(layout.get('image_info', []))}ê°œ
            - ë ˆì´ì•„ì›ƒ íŠ¹ì§•: {self._summarize_layout_info(layout.get('layout_info', {}))}
            """)
        
        return "\n".join(formatted_data)
    
    def _summarize_layout_info(self, layout_info: Dict) -> str:
        """ë ˆì´ì•„ì›ƒ ì •ë³´ ìš”ì•½"""
        text_blocks = layout_info.get('text_blocks', [])
        images = layout_info.get('images', [])
        tables = layout_info.get('tables', [])
        
        summary = []
        if text_blocks:
            summary.append(f"í…ìŠ¤íŠ¸ ë¸”ë¡ {len(text_blocks)}ê°œ")
        if images:
            summary.append(f"ì´ë¯¸ì§€ {len(images)}ê°œ")
        if tables:
            summary.append(f"í…Œì´ë¸” {len(tables)}ê°œ")
        
        return ", ".join(summary) if summary else "ê¸°ë³¸ ë ˆì´ì•„ì›ƒ"
    
    def _extract_all_text(self, magazine_content) -> str:
        """ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if isinstance(magazine_content, dict):
            all_text = ""
            
            # ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            priority_fields = [
                "integrated_content", "essay_content", "interview_content", 
                "sections", "content", "body", "text"
            ]
            
            for field in priority_fields:
                if field in magazine_content:
                    value = magazine_content[field]
                    if isinstance(value, str) and value.strip():
                        all_text += value + "\n\n"
                    elif isinstance(value, dict):
                        for sub_key, sub_value in value.items():
                            if isinstance(sub_value, str) and sub_value.strip():
                                all_text += sub_value + "\n\n"
                    elif isinstance(value, list):
                        for item in value:
                            if isinstance(item, dict):
                                for sub_key, sub_value in item.items():
                                    if isinstance(sub_value, str) and sub_value.strip():
                                        all_text += sub_value + "\n\n"
                            elif isinstance(item, str) and item.strip():
                                all_text += item + "\n\n"
            
            return all_text.strip()
        else:
            return str(magazine_content)
    
    def _analyze_content_structure(self, content: str) -> List[str]:
        """ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„ ë° ì§€ëŠ¥ì  ë¶„í• """
        if not content:
            return []
        
        sections = []
        
        # 1. í—¤ë” ê¸°ë°˜ ë¶„í• 
        header_sections = self._split_by_headers(content)
        if len(header_sections) >= 3:
            sections.extend(header_sections)
        
        # 2. ë¬¸ë‹¨ ê¸°ë°˜ ë¶„í• 
        if len(sections) < 5:
            paragraph_sections = self._split_by_paragraphs(content)
            sections.extend(paragraph_sections)
        
        # 3. ì˜ë¯¸ ê¸°ë°˜ ë¶„í• 
        if len(sections) < 6:
            semantic_sections = self._split_by_semantics(content)
            sections.extend(semantic_sections)
        
        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
        unique_sections = []
        seen_content = set()
        
        for section in sections:
            section_clean = re.sub(r'\s+', ' ', section.strip())
            if len(section_clean) >= 100 and section_clean not in seen_content:
                unique_sections.append(section)
                seen_content.add(section_clean)
        
        return unique_sections[:8]  # ìµœëŒ€ 8ê°œ ì„¹ì…˜
    
    def _split_by_headers(self, content: str) -> List[str]:
        """í—¤ë” ê¸°ë°˜ ë¶„í• """
        sections = []
        header_pattern = r'^(#{1,3})\s+(.+?)$'
        current_section = []
        
        lines = content.split('\n')
        for line in lines:
            if re.match(header_pattern, line.strip()):
                if current_section:
                    section_content = '\n'.join(current_section).strip()
                    if len(section_content) > 50:
                        sections.append(section_content)
                current_section = [line]
            else:
                current_section.append(line)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì¶”ê°€
        if current_section:
            section_content = '\n'.join(current_section).strip()
            if len(section_content) > 50:
                sections.append(section_content)
        
        return sections
    
    def _split_by_paragraphs(self, content: str) -> List[str]:
        """ë¬¸ë‹¨ ê¸°ë°˜ ë¶„í• """
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip() and len(p.strip()) > 100]
        
        # ë¬¸ë‹¨ì„ ê·¸ë£¹í™”í•˜ì—¬ ì ì ˆí•œ ê¸¸ì´ì˜ ì„¹ì…˜ ìƒì„±
        sections = []
        current_group = []
        current_length = 0
        
        for paragraph in paragraphs:
            if current_length + len(paragraph) > 800 and current_group:
                sections.append('\n\n'.join(current_group))
                current_group = [paragraph]
                current_length = len(paragraph)
            else:
                current_group.append(paragraph)
                current_length += len(paragraph)
        
        if current_group:
            sections.append('\n\n'.join(current_group))
        
        return sections
    
    def _split_by_semantics(self, content: str) -> List[str]:
        """ì˜ë¯¸ ê¸°ë°˜ ë¶„í• """
        # ì—¬í–‰ ê´€ë ¨ í‚¤ì›Œë“œ ê·¸ë£¹
        keyword_groups = {
            "arrival": ["ë„ì°©", "ê³µí•­", "ì²«ì¸ìƒ", "ì‹œì‘", "ì¶œë°œ"],
            "exploration": ["íƒí—˜", "ê±·ê¸°", "ë°œê²¬", "ê±°ë¦¬", "êµ¬ê²½"],
            "culture": ["ë¬¸í™”", "ì—­ì‚¬", "ì „í†µ", "ì˜ˆìˆ ", "ë°•ë¬¼ê´€"],
            "food": ["ìŒì‹", "ë§›", "ë ˆìŠ¤í† ë‘", "ì¹´í˜", "ë¨¹"],
            "people": ["ì‚¬ëŒ", "ë§Œë‚¨", "ëŒ€í™”", "ì¹œêµ¬", "í˜„ì§€ì¸"],
            "reflection": ["ìƒê°", "ëŠë‚Œ", "ê°ì •", "ì˜ë¯¸", "ë§ˆë¬´ë¦¬"]
        }
        
        sentences = [s.strip() + '.' for s in content.split('.') if s.strip() and len(s.strip()) > 30]
        sections = {group: [] for group in keyword_groups}
        unclassified = []
        
        for sentence in sentences:
            classified = False
            for group, keywords in keyword_groups.items():
                if any(keyword in sentence for keyword in keywords):
                    sections[group].append(sentence)
                    classified = True
                    break
            
            if not classified:
                unclassified.append(sentence)
        
        # ë¶„ë¥˜ëœ ì„¹ì…˜ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        result_sections = []
        for group, group_sentences in sections.items():
            if group_sentences:
                section_text = ' '.join(group_sentences)
                if len(section_text) > 100:
                    result_sections.append(section_text)
        
        # ë¶„ë¥˜ë˜ì§€ ì•Šì€ ë¬¸ì¥ë“¤ë„ ì¶”ê°€
        if unclassified:
            unclassified_text = ' '.join(unclassified)
            if len(unclassified_text) > 100:
                result_sections.append(unclassified_text)
        
        return result_sections
    
    def _map_to_templates(self, refined_sections: List[Dict], available_templates: List[str]) -> List[Dict]:
        """ì •ì œëœ ì„¹ì…˜ì„ í…œí”Œë¦¿ì— ë§¤í•‘"""
        text_mapping = []
        
        for i, section in enumerate(refined_sections):
            template_name = available_templates[i] if i < len(available_templates) else f"Section{i+1:02d}.jsx"
            
            text_mapping.append({
                "template": template_name,
                "title": section["title"],
                "subtitle": section["subtitle"],
                "body": section["content"],
                "tagline": "",
                "content_length": section["refined_length"],
                "layout_source": section.get("layout_info", {}).get("pdf_name", "default")
            })
        
        return text_mapping
