import asyncio
import json
import numpy as np
import open_clip
import time
import torch
from typing import Dict, List, Any, Optional
from sklearn.metrics.pairwise import cosine_similarity
from custom_llm import get_azure_llm
from utils.log.hybridlogging import get_hybrid_logger
from utils.isolation.ai_search_isolation import AISearchIsolationManager
from utils.data.pdf_vector_manager import PDFVectorManager
from utils.isolation.session_isolation import SessionAwareMixin
from utils.isolation.agent_communication_isolation import InterAgentCommunicationMixin
from agents.Editor.image_diversity_manager import ImageDiversityManager
from utils.log.logging_manager import LoggingManager
from db.magazine_db_utils import MagazineDBUtils

class SemanticAnalysisEngine(SessionAwareMixin, InterAgentCommunicationMixin):
    """ì˜ë¯¸ì  ë¶„ì„ ì—”ì§„ - í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ì˜ ì˜ë¯¸ì  ì—°ê´€ì„± ë¶„ì„ (AI Search í†µí•©)"""
    
    def __init__(self):
        self.llm = get_azure_llm()
        self.logger = get_hybrid_logger(self.__class__.__name__)
        # AI Search ê²©ë¦¬ ì‹œìŠ¤í…œ ì¶”ê°€
        self.isolation_manager = AISearchIsolationManager()
        # PDF ë²¡í„° ë§¤ë‹ˆì € ì¶”ê°€ (ê²©ë¦¬ í™œì„±í™”)
        self.vector_manager = PDFVectorManager(isolation_enabled=True)
        self.logging_manager = LoggingManager(self.logger)
        self.__init_session_awareness__()
        self.__init_inter_agent_communication__()
        self.image_diversity_manager = ImageDiversityManager(self.logger)

        self._setup_logging_system()

    def _setup_logging_system(self):
        """ë¡œê·¸ ì €ì¥ ì‹œìŠ¤í…œ ì„¤ì •"""
        self.log_enabled = True
        self.response_counter = 0

    async def process_data(self, input_data):
        # ì—ì´ì „íŠ¸ ì‘ì—… ìˆ˜í–‰
        result = await self._do_work(input_data)
        
        # âœ… ì‘ë‹µ ë¡œê·¸ ì €ì¥
        await self.logging_manager.log_agent_response(
            agent_name=self.__class__.__name__,
            agent_role="ì—ì´ì „íŠ¸ ì—­í•  ì„¤ëª…",
            task_description="ìˆ˜í–‰í•œ ì‘ì—… ì„¤ëª…",
            response_data=result,  # ì‹¤ì œ ì‘ë‹µ ë°ì´í„°ë§Œ
            metadata={"additional": "info"}
        )

    async def _log_semantic_analysis_response(self, analysis_result: Dict) -> str:
        """ì˜ë¯¸ì  ë¶„ì„ ê²°ê³¼ ë¡œê·¸ ì €ì¥ (BindingAgent ë°©ì‹ ì ìš©)"""
        if not self.log_enabled:
            return "logging_disabled"
        
        try:
            response_data = {
                "agent_name": "SemanticAnalysisEngine",
                "analysis_type": "text_image_semantics",
                "text_sections": len(analysis_result.get("text_semantics", [])),
                "image_sections": len(analysis_result.get("image_semantics", [])),
                "mapping_confidence": analysis_result.get("analysis_metadata", {}).get("mapping_confidence", 0.0),
                "ai_search_enhanced": analysis_result.get("analysis_metadata", {}).get("ai_search_enhanced", False),
                "timestamp": time.time(),
                "session_id": self.current_session_id
            }
            
            response_id = f"SemanticAnalysis_{int(time.time() * 1000000)}"
            
            # self.store_result(response_data)  # ì„¸ì…˜ ì €ì¥ ì œê±° - ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
            
            self.logger.info(f"ğŸ“¦ SemanticAnalysisEngine ì‘ë‹µ ì €ì¥: {response_id}")
            return response_id
            
        except Exception as e:
            self.logger.error(f"ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return "log_save_failed"
        
    async def analyze_text_image_semantics(self, magazine_content: Dict, image_analysis: List[Dict]) -> Dict:
        """ì˜ë¯¸ì  í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë§¤ì¹­ (êµ¬ì¡° í†µì¼)"""
        
        try:
            # magazine_idê°€ ìˆìœ¼ë©´ Cosmos DBì—ì„œ ìµœì‹  ë°ì´í„° ì¡°íšŒ
            if "magazine_id" in magazine_content:
                magazine_data = await MagazineDBUtils.get_magazine_by_id(magazine_content["magazine_id"])
                if magazine_data:
                    magazine_content = magazine_data.get("content", magazine_content)
                    
                    # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë„ Cosmos DBì—ì„œ ì¡°íšŒ (ë³€ê²½ëœ ì €ì¥ ë°©ì‹ ì ìš©)
                    image_analysis = await MagazineDBUtils.get_images_by_magazine_id(magazine_content["magazine_id"])
            
            sections = magazine_content.get("sections", [])
            if not sections:
                return self._generate_clean_fallback_result(magazine_content, image_analysis)
            
            if not image_analysis:
                # âœ… ì´ë¯¸ì§€ê°€ ì—†ì–´ë„ í…ìŠ¤íŠ¸ ë¶„ì„ì€ ìˆ˜í–‰
                text_semantics = await self._extract_text_semantics_with_vector_search(magazine_content)
                return {
                    "text_semantics": text_semantics,
                    "semantic_mappings": self._create_text_only_mappings(text_semantics),
                    "analysis_metadata": {
                        "sections_processed": len(sections),
                        "images_processed": len(image_analysis) if image_analysis else 0,
                        "success": True,
                        "text_only_mode": True
                    }
                }
            
            # âœ… ì •ìƒì ì¸ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë¶„ì„
            # CLIP ëª¨ë¸ ì´ˆê¸°í™” í™•ì¸
            await self._ensure_clip_initialization()
            
            # í…ìŠ¤íŠ¸ ì˜ë¯¸ ë¶„ì„
            text_semantics = await self._extract_text_semantics_with_vector_search(magazine_content)
            
            # ì´ë¯¸ì§€ ì˜ë¯¸ ë¶„ì„
            image_semantics = await self._extract_image_semantics_with_layout_patterns_batch(image_analysis)
            
            # ì˜ë¯¸ì  ë§¤ì¹­ ìˆ˜í–‰
            semantic_mappings = await self._perform_enhanced_semantic_matching(
                text_semantics, image_semantics, sections, image_analysis
            )
            
            # âœ… í†µì¼ëœ êµ¬ì¡°ë¡œ ë°˜í™˜
            result = {
                "text_semantics": text_semantics,
                "image_semantics": image_semantics,
                "semantic_mappings": semantic_mappings,
                "optimal_combinations": await self._generate_optimal_combinations_with_ai_search(semantic_mappings),
                "analysis_metadata": {
                    "sections_processed": len(sections),
                    "images_processed": len(image_analysis),
                    "text_sections_analyzed": len(text_semantics),
                    "image_sections_analyzed": len(image_semantics),
                    "mappings_created": len(semantic_mappings),
                    "clip_available": getattr(self, 'clip_available', False),
                    "success": True
                }
            }
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ ë§¤ê±°ì§„ì— ì—…ë°ì´íŠ¸
            if "magazine_id" in magazine_content:
                await MagazineDBUtils.update_magazine_content(magazine_content["magazine_id"], {
                    "content": magazine_content,
                    "semantic_analysis": result,
                    "status": "analyzed"
                })
            
            return result
            
        except Exception as e:
            self.logger.error(f"ì˜ë¯¸ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._generate_clean_fallback_result(magazine_content, image_analysis)

    def _create_text_only_mappings(self, text_semantics: List[Dict]) -> List[Dict]:
        """í…ìŠ¤íŠ¸ë§Œ ìˆì„ ë•Œì˜ ë§¤í•‘ ìƒì„±"""
        mappings = []
        for text_section in text_semantics:
            mappings.append({
                "text_section_index": text_section["section_index"],
                "text_title": text_section["title"],
                "image_matches": [],  # ì´ë¯¸ì§€ ì—†ìŒ
                "text_only": True
            })
        return mappings

    async def _ensure_clip_initialization(self):
        """CLIP ëª¨ë¸ ì´ˆê¸°í™” ë³´ì¥"""
        if not hasattr(self, 'clip_available'):
            try:
                import torch
                import open_clip
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
                self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(
                    'ViT-B-32', pretrained='laion2b_s34b_b79k', device=self.device
                )
                self.clip_model.eval()
                self.clip_available = True
                self.logger.info("âœ… CLIP ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ CLIP ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨, í’ˆì§ˆ ê¸°ë°˜ ë§¤ì¹­ ì‚¬ìš©: {e}")
                self.clip_available = False

    async def _perform_enhanced_semantic_matching(self, text_semantics: List[Dict], 
                                                image_semantics: List[Dict],
                                                sections: List[Dict], 
                                                image_analysis: List[Dict]) -> List[Dict]:
        """ê°•í™”ëœ ì˜ë¯¸ì  ë§¤ì¹­ (ë¹ˆ ê²°ê³¼ ë°©ì§€)"""
        
        if not text_semantics:
            self.logger.warning("í…ìŠ¤íŠ¸ ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ ì—†ìŒ. ë¹ˆ ë§¤í•‘ ë°˜í™˜.")
            return [] # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        # CLIP ê¸°ë°˜ ë§¤ì¹­ ì‹œë„
        if self.clip_available and image_analysis and image_semantics: # image_semanticsë„ í™•ì¸
            try:
                # image_analysis ëŒ€ì‹  image_semanticsë¥¼ ì „ë‹¬í•˜ì—¬ ë¶„ì„ëœ ì •ë³´ë¥¼ í™œìš©í•˜ë„ë¡ ê³ ë ¤ ê°€ëŠ¥
                # ì—¬ê¸°ì„œëŠ” image_analysis (ì›ë³¸ ì´ë¯¸ì§€ ì •ë³´ ë¦¬ìŠ¤íŠ¸)ë¥¼ ê³„ì† ì‚¬ìš©
                clip_mappings = await self._perform_clip_based_matching(text_semantics, image_analysis, sections)
                if clip_mappings: # CLIP ë§¤ì¹­ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ë°˜í™˜
                    return clip_mappings
                else:
                    self.logger.warning("CLIP ë§¤ì¹­ ê²°ê³¼ê°€ ë¹„ì—ˆìŒ. í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ í´ë°± ì‹œë„.")
            except Exception as e:
                self.logger.error(f"CLIP ë§¤ì¹­ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}. í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ í´ë°± ì‹œë„.")
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ (CLIP ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ ì‹œ)
        # image_analysis ëŒ€ì‹  image_semanticsë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•  ìˆ˜ ìˆìœ¼ë‚˜, í˜„ì¬ êµ¬ì¡° ìœ ì§€
        if image_analysis:
            self.logger.info("í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ ìˆ˜í–‰.")
            keyword_mappings = await self._perform_keyword_based_matching(text_semantics, image_analysis)
            if keyword_mappings:
                return keyword_mappings
            else:
                self.logger.warning("í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ ê²°ê³¼ë„ ë¹„ì—ˆìŒ. í…ìŠ¤íŠ¸ ì „ìš© ë§¤í•‘ìœ¼ë¡œ í´ë°±.")
        
        # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ê°€ ì•„ì˜ˆ ì—†ê±°ë‚˜ ëª¨ë“  ë§¤ì¹­ ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš°, í…ìŠ¤íŠ¸ ì „ìš© ë§¤í•‘ ìƒì„±
        self.logger.info("í…ìŠ¤íŠ¸ ì „ìš© ë§¤í•‘ ìƒì„±.")
        return self. _create_text_only_mappings(text_semantics)

    async def _perform_clip_based_matching(self, text_semantics: List[Dict], 
                                            image_analysis: List[Dict], # ì „ì²´ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ (URL ë“± í¬í•¨)
                                            sections: List[Dict]) -> List[Dict]: # ì›ë³¸ í…ìŠ¤íŠ¸ ì„¹ì…˜ (ì°¸ê³ ìš©)
        """CLIP ê¸°ë°˜ ì˜ë¯¸ì  ë§¤ì¹­. ê° í…ìŠ¤íŠ¸ ì„¹ì…˜ê³¼ ì „ì²´ ì´ë¯¸ì§€ í’€ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ë§¤í•‘."""
        
        if not self.clip_available:
            self.logger.warning("CLIP model not available for _perform_clip_based_matching. Returning empty list.")
            return []

        # 1. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        section_texts_for_embedding = []
        valid_text_indices = [] # ì„ë² ë”© ìƒì„±ì´ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì„¹ì…˜ì˜ ì›ë³¸ ì¸ë±ìŠ¤
        for i, ts_item in enumerate(text_semantics):
            title = ts_item.get("title", "")
            # content_previewê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ sectionsì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
            content_preview = ts_item.get("content_preview", "")
            if not content_preview and i < len(sections):
                content_preview = sections[i].get("content", "")[:200]
            
            text_for_embedding = (title + " " + content_preview).strip()[:500] # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ì˜ˆ: 500ì)
            if not text_for_embedding: 
                 text_for_embedding = "empty section" # ë‚´ìš©ì´ ì „í˜€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            section_texts_for_embedding.append(text_for_embedding)
            valid_text_indices.append(ts_item.get("section_index", i)) # ì›ë³¸ section_index ì‚¬ìš©

        if not section_texts_for_embedding:
             self.logger.warning("No text content available in text_semantics for CLIP matching. Returning empty list.")
             return []

        text_embeddings_np = await self._generate_clip_text_embeddings(section_texts_for_embedding)
        if text_embeddings_np.size == 0 or text_embeddings_np.shape[0] != len(section_texts_for_embedding):
            self.logger.error("Failed to generate text embeddings or shape mismatch. Returning empty list.")
            return []

        # 2. ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±
        if not image_analysis: 
            self.logger.info("No images provided for CLIP matching. Returning text-only mappings based on input text_semantics.")
            # ì´ ê²½ìš°, semantic_mappingsì— image_matchesë¥¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì±„ì›Œì„œ ë°˜í™˜í•  ìˆ˜ ìˆìŒ
            # ë˜ëŠ” _create_text_only_mappingsì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ ì—¬ê¸°ì„œ ì§ì ‘ ìƒì„±
            # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ _perform_enhanced_semantic_matchingì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
            return [] 

        self.logger.info(f"Generating image embeddings for {len(image_analysis)} images for CLIP matching.")
        image_embeddings_np = await self._generate_clip_image_embeddings_from_data(image_analysis)
        
        if image_embeddings_np.size == 0 or image_embeddings_np.shape[0] != len(image_analysis):
            self.logger.warning("Image embeddings array is empty or shape mismatch for CLIP matching. Returning empty list.")
            return []

        # 3. ì„ë² ë”© ì°¨ì› í™•ì¸ ë° 2Dë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
        if text_embeddings_np.ndim == 1: text_embeddings_np = text_embeddings_np.reshape(1, -1)
        if image_embeddings_np.ndim == 1: image_embeddings_np = image_embeddings_np.reshape(1, -1)
        
        if text_embeddings_np.shape[1] != image_embeddings_np.shape[1]:
            self.logger.error(
                f"Embedding dimension mismatch: Texts {text_embeddings_np.shape[1]}, Images {image_embeddings_np.shape[1]}. Cannot compute similarity."
            )
            return []

        # 4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ëª¨ë“  í…ìŠ¤íŠ¸ ì„¹ì…˜ vs ëª¨ë“  ì´ë¯¸ì§€)
        try:
            # similarity_matrix[i, j]ëŠ” ië²ˆì§¸ í…ìŠ¤íŠ¸ì™€ jë²ˆì§¸ ì´ë¯¸ì§€ ê°„ì˜ ìœ ì‚¬ë„
            similarity_matrix = cosine_similarity(text_embeddings_np, image_embeddings_np) 
        except ValueError as ve:
            self.logger.error(f"Error calculating cosine similarity: {ve}. Check embedding dimensions and content. Returning empty list.")
            return []

        # 5. semantic_mappings ìƒì„±
        semantic_mappings_result = []
        for i, original_text_section_info in enumerate(text_semantics): # ì›ë³¸ text_semantics ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒ
            # text_embeddings_np[i]ì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ ì„¹ì…˜ì€ section_texts_for_embedding[i]
            # original_text_section_infoëŠ” ì´ í…ìŠ¤íŠ¸ ì„¹ì…˜ì˜ ì›ë³¸ ë©”íƒ€ë°ì´í„° (title, section_index ë“±)
            
            # í˜„ì¬ í…ìŠ¤íŠ¸ ì„¹ì…˜ê³¼ ëª¨ë“  ì´ë¯¸ì§€ ê°„ì˜ ìœ ì‚¬ë„ ì ìˆ˜ ë²¡í„°
            image_scores_for_this_text_vector = similarity_matrix[i] 
            
            image_matches_for_section = []
            for j, image_data_from_analysis in enumerate(image_analysis): # ì „ì²´ image_analysis ë¦¬ìŠ¤íŠ¸ ìˆœíšŒ
                similarity_score = float(image_scores_for_this_text_vector[j])
                
                # image_data_from_analysisëŠ” ì´ë¯¸ image_name, image_url ë“±ì˜ ì •ë³´ë¥¼ í¬í•¨
                # ì—¬ê¸°ì— CLIP ìœ ì‚¬ë„ ì ìˆ˜ ë° ì›ë³¸ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œì˜ ì¸ë±ìŠ¤ ì¶”ê°€
                matched_image_info = {
                    **image_data_from_analysis, 
                    "similarity_score": round(similarity_score, 4), # ì†Œìˆ˜ì  4ìë¦¬
                    "image_index": j # image_analysis ë¦¬ìŠ¤íŠ¸ì—ì„œì˜ ì¸ë±ìŠ¤ (í‚¤ ì´ë¦„ ë³€ê²½)
                }
                image_matches_for_section.append(matched_image_info)

            # ê° ì„¹ì…˜ë³„ ì´ë¯¸ì§€ í›„ë³´ë“¤ì„ ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            image_matches_for_section.sort(key=lambda x: x["similarity_score"], reverse=True)
            
            # (ì„ íƒì ) ìƒìœ„ Nê°œ ì´ë¯¸ì§€ í•„í„°ë§ (ì˜ˆ: ìƒìœ„ 10ê°œ ë˜ëŠ” íŠ¹ì • ì„ê³„ê°’ ì´ìƒ)
            # top_n_filter = 15
            # image_matches_for_section = image_matches_for_section[:top_n_filter]

            semantic_mappings_result.append({
                "text_section_index": original_text_section_info.get("section_index"), # ì›ë³¸ text_semanticsì˜ section_index ì‚¬ìš©
                "text_title": original_text_section_info.get("title"), # ì›ë³¸ title ì‚¬ìš©
                "image_matches": image_matches_for_section # ì´ í…ìŠ¤íŠ¸ ì„¹ì…˜ì— ëŒ€í•œ ëª¨ë“  ì´ë¯¸ì§€ í›„ë³´êµ° (ì •ë ¬ë¨)
            })
        
        return semantic_mappings_result

    async def _perform_keyword_based_matching(self, text_semantics: List[Dict], 
                                                image_analysis: List[Dict]) -> List[Dict]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ë¯¸ì  ë§¤ì¹­ (CLIP ëŒ€ì•ˆ)"""
        
        semantic_mappings = []
        
        for text_section in text_semantics:
            # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            text_keywords = self._extract_keywords_from_text(text_section)
            
            # ì´ë¯¸ì§€ì™€ í‚¤ì›Œë“œ ë§¤ì¹­
            image_matches = []
            for i, img in enumerate(image_analysis):
                match_score = self._calculate_keyword_similarity(text_keywords, img)
                
                if match_score > 0.1:  # ìµœì†Œ ì„ê³„ê°’
                    match = img.copy()
                    match["image_index"] = i
                    match["similarity_score"] = match_score
                    image_matches.append(match)
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 3ê°œ ì„ íƒ
            image_matches.sort(key=lambda x: x["similarity_score"], reverse=True)
            
            semantic_mappings.append({
                "text_section_index": text_section["section_index"],
                "text_title": text_section["title"],
                "image_matches": image_matches[:3]
            })
        
        return semantic_mappings

    def _extract_keywords_from_text(self, text_section: Dict) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        title = text_section.get("title", "")
        keywords.extend(title.split())
        
        # ì˜ë¯¸ ë¶„ì„ ê²°ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        semantic_analysis = text_section.get("semantic_analysis", {})
        
        if isinstance(semantic_analysis, dict):
            # ì£¼ìš” ì£¼ì œ
            main_topics = semantic_analysis.get("ì£¼ìš”_ì£¼ì œ", [])
            if isinstance(main_topics, list):
                keywords.extend(main_topics)
            
            # ì‹œê°ì  í‚¤ì›Œë“œ
            visual_keywords = semantic_analysis.get("ì‹œê°ì _í‚¤ì›Œë“œ", [])
            if isinstance(visual_keywords, list):
                keywords.extend(visual_keywords)
            
            # ë¬¸í™”ì  ìš”ì†Œ
            cultural_elements = semantic_analysis.get("ë¬¸í™”ì _ìš”ì†Œ", [])
            if isinstance(cultural_elements, list):
                keywords.extend(cultural_elements)
        
        return [k.strip() for k in keywords if k.strip()]

    def _calculate_keyword_similarity(self, text_keywords: List[str], image: Dict) -> float:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not text_keywords:
            return 0.0
        
        # ì´ë¯¸ì§€ ì •ë³´ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        image_text = ""
        image_text += image.get("location", "") + " "
        image_text += image.get("description", "") + " "
        image_text += image.get("city", "") + " "
        image_text += image.get("country", "") + " "
        
        image_keywords = image_text.lower().split()
        
        # ê³µí†µ í‚¤ì›Œë“œ ê³„ì‚°
        text_keywords_lower = [k.lower() for k in text_keywords]
        common_keywords = set(text_keywords_lower) & set(image_keywords)
        
        if not text_keywords_lower:
            return 0.0
        
        similarity = len(common_keywords) / len(text_keywords_lower)
        return min(similarity, 1.0)

    async def _generate_clip_text_embeddings(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•œ CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        if not self.clip_available or not texts:
            self.logger.warning("CLIP model not available or no texts provided for text embedding.")
            # ì ì ˆí•œ ì°¨ì›ì˜ 0 ë²¡í„° ë˜ëŠ” ë¹ˆ ë°°ì—´ ë°˜í™˜
            output_dim = 512 # ê¸°ë³¸ê°’
            if hasattr(self, 'clip_model') and hasattr(self.clip_model, 'text_projection') and self.clip_model.text_projection is not None:
                 # ëª¨ë¸ì—ì„œ ì‹¤ì œ output ì°¨ì›ì„ ê°€ì ¸ì˜¤ë ¤ê³  ì‹œë„ (ì˜ˆì‹œ, ì‹¤ì œ ëª¨ë¸ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¦„)
                 if isinstance(self.clip_model.text_projection, torch.Tensor): # Check if it's a Tensor
                    output_dim = self.clip_model.text_projection.shape[-1]
                 elif isinstance(self.clip_model.text_projection, torch.nn.Parameter):
                    output_dim = self.clip_model.text_projection.shape[-1]
                 # ë‹¤ë¥¸ ê°€ëŠ¥í•œ ì†ì„±ë“¤ (ì˜ˆ: self.clip_model.transformer.width)
                 elif hasattr(self.clip_model, 'transformer') and hasattr(self.clip_model.transformer, 'width'):
                     output_dim = self.clip_model.transformer.width


            return np.zeros((len(texts), output_dim), dtype=np.float32) if texts else np.array([])

        try:
            with torch.no_grad():
                    # `open_clip.tokenize`ëŠ” í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ë°›ìŒ
                text_tokens = open_clip.tokenize(texts).to(self.device)
                text_features = self.clip_model.encode_text(text_tokens)
                text_features /= text_features.norm(dim=-1, keepdim=True)
                return text_features.cpu().numpy().astype(np.float32)
        except Exception as e:
            self.logger.error(f"Error generating CLIP text embeddings: {e}")
            output_dim = 512 # ê¸°ë³¸ê°’
            if hasattr(self, 'clip_model') and hasattr(self.clip_model, 'text_projection') and self.clip_model.text_projection is not None:
                if isinstance(self.clip_model.text_projection, torch.Tensor): # Check if it's a Tensor
                    output_dim = self.clip_model.text_projection.shape[-1]
                elif isinstance(self.clip_model.text_projection, torch.nn.Parameter):
                    output_dim = self.clip_model.text_projection.shape[-1]
                elif hasattr(self.clip_model, 'transformer') and hasattr(self.clip_model.transformer, 'width'):
                    output_dim = self.clip_model.transformer.width

            return np.zeros((len(texts), output_dim), dtype=np.float32)


    async def _generate_clip_image_embeddings_from_data(self, images: List[Dict]) -> np.ndarray:
        """
        ì£¼ì–´ì§„ ì´ë¯¸ì§€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì—ì„œ CLIP ì´ë¯¸ì§€ ì„ë² ë”©ì„ ìƒì„±.
        ìºì‹œëœ ì„ë² ë”©ì´ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ê³„ì‚°.
        ê°œë³„ ì´ë¯¸ì§€ ë¡œë“œ/ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ì´ë¯¸ì§€ì— ëŒ€í•´ì„œëŠ” 0 ë²¡í„°ë¥¼ ì‚¬ìš©.
        """
        if not self.clip_available or not hasattr(self, 'clip_model') or not hasattr(self, 'clip_preprocess'):
            self.logger.warning("CLIP model not available for generating image embeddings.")
            output_dim = 512 # Fallback dimension if model is not fully available
            if hasattr(self, 'clip_model') and hasattr(self.clip_model, 'visual') and hasattr(self.clip_model.visual, 'output_dim'):
                output_dim = self.clip_model.visual.output_dim
            return np.zeros((len(images), output_dim), dtype=np.float32)

        all_embeddings = [None] * len(images) # Initialize with placeholders
        
        # Attempt to get output dimension from the model
        try:
            output_dim = self.clip_model.visual.output_dim
        except AttributeError:
            self.logger.error("Cannot determine CLIP model output dimension. Using fallback 512.")
            output_dim = 512 # Fallback dimension
            # If we can't get output_dim, it's a critical model issue, return all zeros
            return np.zeros((len(images), output_dim), dtype=np.float32)

        pil_images_to_process = []
        indices_for_pil_images = [] # Tracks original indices of images that will be processed by CLIP

        for idx, image_data in enumerate(images):
            image_url = image_data.get("image_url")
            if not image_url:
                self.logger.warning(f"Image at index {idx} (name: {image_data.get('image_name', 'N/A')}) has no URL. Using zero vector.")
                all_embeddings[idx] = np.zeros(output_dim, dtype=np.float32)
                continue

            # TODO: Implement caching mechanism if needed
            # cached_embedding = self.vector_manager.get_cached_image_embedding(image_url)
            # if cached_embedding is not None:
            #     all_embeddings[idx] = cached_embedding
            #     continue
            
            # If not cached, prepare for CLIP processing
            try:
                # Lazily import requests and PIL here if they are not commonly used elsewhere in this class
                from PIL import Image
                import requests
                from io import BytesIO

                response = requests.get(image_url, timeout=10)
                response.raise_for_status() # Raise an exception for bad status codes
                pil_img = Image.open(BytesIO(response.content)).convert("RGB")
                pil_images_to_process.append(pil_img)
                indices_for_pil_images.append(idx)
            except requests.exceptions.RequestException as e:
                self.logger.error(f"Failed to download image from URL {image_url} (index {idx}): {e}. Using zero vector.")
                all_embeddings[idx] = np.zeros(output_dim, dtype=np.float32)
            except Image.UnidentifiedImageError: # PIL anead PIL.Image.UnidentifiedImageError
                self.logger.error(f"Cannot identify image file from URL {image_url} (index {idx}). It might be corrupted or not an image. Using zero vector.")
                all_embeddings[idx] = np.zeros(output_dim, dtype=np.float32)
            except Exception as e:
                self.logger.error(f"An unexpected error occurred while loading or preprocessing image {image_url} (index {idx}): {e}. Using zero vector.")
                all_embeddings[idx] = np.zeros(output_dim, dtype=np.float32)

        if pil_images_to_process: # If there are any images to process with CLIP
            try:
                image_inputs = torch.stack([self.clip_preprocess(img) for img in pil_images_to_process]).to(self.device)
                
                with torch.no_grad(), torch.cuda.amp.autocast():
                    image_features = self.clip_model.encode_image(image_inputs)
                    image_features /= image_features.norm(dim=-1, keepdim=True)
                
                batch_embeddings_np = image_features.cpu().numpy().astype(np.float32)

                # Place computed embeddings into the correct original positions
                for i, original_idx in enumerate(indices_for_pil_images):
                    all_embeddings[original_idx] = batch_embeddings_np[i]
            
            except Exception as e:
                self.logger.error(f"Error during CLIP model image encoding batch: {e}. Using zero vectors for these images.")
                for original_idx in indices_for_pil_images: # For images that were intended for this batch
                    all_embeddings[original_idx] = np.zeros(output_dim, dtype=np.float32)
        
        # Ensure all placeholders are filled (should be, due to error handling above)
        for idx_fill in range(len(all_embeddings)): # Renamed idx to idx_fill to avoid conflict
            if all_embeddings[idx_fill] is None: # Should not happen if logic is correct
                self.logger.error(f"Embedding for image at index {idx_fill} was unexpectedly None post-processing. Filling with zero vector.")
                all_embeddings[idx_fill] = np.zeros(output_dim, dtype=np.float32)
        
        return np.array(all_embeddings).astype(np.float32)


    async def _generate_optimal_combinations_with_ai_search(self, semantic_mappings: List[Dict]) -> List[Dict]:
        """
        ì˜ë¯¸ì  ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ë³„ ìµœì  ì´ë¯¸ì§€ ì¡°í•© ìƒì„± (ì¤‘ë³µ ì—†ì´)
        """
        optimal_combinations = []
        used_images = set()
        for mapping in semantic_mappings:
            section_index = mapping["text_section_index"]
            section_title = mapping["text_title"]
            best_images = []
            for image_match in mapping["image_matches"]:
                if image_match["image_index"] not in used_images:
                    best_images.append(image_match)
                    used_images.add(image_match["image_index"])
            optimal_combinations.append({
                "section_index": section_index,
                "section_title": section_title,
                "assigned_images": best_images,
                "total_similarity_score": sum(img["similarity_score"] for img in best_images),
                "ai_search_enhanced": True,
                "optimization_notes": f"{len(best_images)}ê°œ ì´ë¯¸ì§€ í• ë‹¹ë¨ (ì¤‘ë³µ ì—†ì´ ì˜ë¯¸ì  ë§¤ì¹­)"
            })
        return optimal_combinations
    
    async def _extract_text_semantics_with_vector_search(self, content: Dict) -> List[Dict]:
        """AI Search ë²¡í„° ê²€ìƒ‰ì„ í™œìš©í•œ í…ìŠ¤íŠ¸ ì˜ë¯¸ ì¶”ì¶œ"""
        
        text_sections = []
        
        if isinstance(content, dict) and "sections" in content:
            for i, section in enumerate(content["sections"]):
                # ì„¹ì…˜ë³„ ì˜¤ì—¼ ê²€ì‚¬
                if self.isolation_manager.is_contaminated(section, f"text_section_{i}"):
                    self.logger.warning(f"í…ìŠ¤íŠ¸ ì„¹ì…˜ {i}ì—ì„œ ì˜¤ì—¼ ê°ì§€, ì›ë³¸ ë°ì´í„° ì‚¬ìš©")
                    section = self.isolation_manager.restore_original_content(section)
                
                # AI Search ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬í•œ ë§¤ê±°ì§„ íŒ¨í„´ ì°¾ê¸°
                section_content = section.get("content", "")
                similar_patterns = await self._search_similar_text_patterns(section_content)
                
                semantic_info = await self._analyze_text_section_with_patterns(section, i, similar_patterns)
                text_sections.append(semantic_info)
        
        return text_sections
    
    async def _search_similar_text_patterns(self, section_content: str) -> List[Dict]:
        """AI Searchì—ì„œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ê²€ìƒ‰"""
        
        try:
            # AI Search í‚¤ì›Œë“œ í•„í„°ë§
            clean_query = self.isolation_manager.clean_query_from_azure_keywords(section_content[:300])
            
            # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰ (í…ìŠ¤íŠ¸ íŒ¨í„´ ì¤‘ì‹¬)
            similar_patterns = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.vector_manager.search_similar_layouts(
                    clean_query, "text-semantic-patterns-index", top_k=8
                )
            )
            
            # ê²©ë¦¬ëœ íŒ¨í„´ë§Œ ë°˜í™˜
            isolated_patterns = self.isolation_manager.filter_contaminated_data(
                similar_patterns, "text_patterns"
            )
            
            self.logger.debug(f"í…ìŠ¤íŠ¸ íŒ¨í„´ ê²€ìƒ‰: {len(similar_patterns)} â†’ {len(isolated_patterns)}ê°œ")
            return isolated_patterns
            
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ íŒ¨í„´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def _analyze_text_section_with_patterns(self, section: Dict, index: int, patterns: List[Dict]) -> Dict:
        """AI Search íŒ¨í„´ì„ ì°¸ì¡°í•œ í…ìŠ¤íŠ¸ ì„¹ì…˜ ë¶„ì„"""
        
        section_content = section.get("content", "")
        title = section.get("title", "")
        
        # AI Search í‚¤ì›Œë“œ í•„í„°ë§
        filtered_content = self.isolation_manager.clean_query_from_azure_keywords(section_content)
        filtered_title = self.isolation_manager.clean_query_from_azure_keywords(title)
        
        # íŒ¨í„´ ê¸°ë°˜ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        pattern_context = ""
        if patterns:
            pattern_info = []
            for pattern in patterns[:3]:  # ìƒìœ„ 3ê°œ íŒ¨í„´ë§Œ ì‚¬ìš©
                pattern_info.append({
                    "ê¸€_í˜•íƒœ": pattern.get("text_structure", "ì¼ë°˜í˜•"),
                    "ë¬¸ì¥_ê¸¸ì´": pattern.get("sentence_length", "ì¤‘ê°„"),
                    "ê¸€_ë§ºìŒ": pattern.get("conclusion_style", "ìì—°ìŠ¤ëŸ¬ìš´"),
                    "ì„¹ì…˜_êµ¬ì¡°": pattern.get("section_format", "ì œëª©-ë³¸ë¬¸")
                })
            pattern_context = f"ì°¸ì¡° íŒ¨í„´: {json.dumps(pattern_info, ensure_ascii=False)}"
        
        analysis_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ ì„¹ì…˜ì˜ ì˜ë¯¸ì  ìš”ì†Œë¥¼ ë¶„ì„í•˜ì„¸ìš”:

ì œëª©: {filtered_title}
ë‚´ìš©: {filtered_content}

{pattern_context}

ë¶„ì„ í•­ëª©:
1. ì£¼ìš” ì£¼ì œ (êµ¬ì²´ì  í‚¤ì›Œë“œ ì¶”ì¶œ)
2. ê°ì •ì  í†¤ (ê¸ì •ì , ì¤‘ì„±ì , ì„±ì°°ì  ë“±)
3. ì‹œê°ì  ì—°ê´€ í‚¤ì›Œë“œ (ìƒ‰ìƒ, í’ê²½, ê±´ë¬¼, ì‚¬ëŒ ë“±)
4. ê³„ì ˆ/ì‹œê°„ëŒ€ ì •ë³´
5. ë¬¸í™”ì  ìš”ì†Œ
6. ê¸€ì˜ í˜•íƒœ (ì„œìˆ í˜•, ëŒ€í™”í˜•, ì„¤ëª…í˜• ë“±)
7. ë¬¸ì¥ ê¸¸ì´ íŠ¹ì„± (ì§§ì€/ì¤‘ê°„/ê¸´ ë¬¸ì¥ ë¹„ìœ¨)

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
"""
        
        try:
            response = await self.llm.ainvoke(analysis_prompt)
            
            #  ê°•í™”ëœ ì‘ë‹µ ì²˜ë¦¬
            if not response or not response.strip():
                self.logger.warning(f"í…ìŠ¤íŠ¸ ì„¹ì…˜ {index} ë¶„ì„ì—ì„œ ë¹ˆ ì‘ë‹µ ìˆ˜ì‹ ")
                return self._get_clean_section_fallback(index, title, section_content)
            
            #  ì‘ë‹µ ê¸¸ì´ ì²´í¬
            if len(response.strip()) < 5:
                self.logger.warning(f"í…ìŠ¤íŠ¸ ì„¹ì…˜ {index} ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ: {response}")
                return self._get_clean_section_fallback(index, title, section_content)
            
            #  ê°•í™”ëœ JSON ì¶”ì¶œ
            cleaned_response = self._extract_json_from_response(response.strip())
            
            #  JSON ê²€ì¦ ë° ìˆ˜ì •
            validated_json = self._validate_and_fix_json(cleaned_response)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                analysis_result = json.loads(validated_json)
            except json.JSONDecodeError as json_error:
                self.logger.error(f"í…ìŠ¤íŠ¸ ì„¹ì…˜ {index} JSON íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: {json_error}")
                self.logger.debug(f"ì›ë³¸ ì‘ë‹µ: {response[:200]}...")
                self.logger.debug(f"ì •ì œëœ ì‘ë‹µ: {cleaned_response[:200]}...")
                self.logger.debug(f"ê²€ì¦ëœ JSON: {validated_json[:200]}...")
                
                #  ìµœí›„ì˜ ìˆ˜ë‹¨: ì •ê·œì‹ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
                fallback_result = self._extract_keywords_with_regex(response)
                if fallback_result:
                    analysis_result = fallback_result
                else:
                    return self._get_clean_section_fallback(index, title, section_content)
            
            # ë¶„ì„ ê²°ê³¼ ì˜¤ì—¼ ê²€ì‚¬
            if self.isolation_manager.is_contaminated(analysis_result, f"analysis_result_{index}"):
                self.logger.warning(f"ë¶„ì„ ê²°ê³¼ {index}ì—ì„œ ì˜¤ì—¼ ê°ì§€, ê¸°ë³¸ê°’ ì‚¬ìš©")
                analysis_result = self._get_clean_analysis_fallback()
            
            return {
                "section_index": index,
                "title": title,
                "content_preview": section_content[:200],
                "semantic_analysis": analysis_result,
                "confidence_score": 0.8,
                "ai_search_patterns": len(patterns),
                "isolation_metadata": {
                    "patterns_referenced": len(patterns),
                    "contamination_detected": False,
                    "original_preserved": True
                }
            }
            
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ ì„¹ì…˜ {index} ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_clean_section_fallback(index, title, section_content)
    
    def _extract_keywords_with_regex(self, response: str) -> Dict:
        """ì •ê·œì‹ì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ìµœí›„ì˜ ìˆ˜ë‹¨)"""
        
        import re
        
        result = {
            "ì£¼ìš”_ì£¼ì œ": [],
            "ê°ì •ì _í†¤": "ì¤‘ì„±ì ",
            "ì‹œê°ì _í‚¤ì›Œë“œ": [],
            "ê³„ì ˆ_ì‹œê°„": "ì•Œ ìˆ˜ ì—†ìŒ",
            "ë¬¸í™”ì _ìš”ì†Œ": [],
            "ê¸€ì˜_í˜•íƒœ": "ì„œìˆ í˜•",
            "ë¬¸ì¥_ê¸¸ì´_íŠ¹ì„±": "ì¤‘ê°„"
        }
        
        try:
            # ì£¼ìš” ì£¼ì œ ì¶”ì¶œ
            topic_patterns = [
                r'ì£¼ìš”[_\s]*ì£¼ì œ[:\s]*([^\n\r,]+)',
                r'í‚¤ì›Œë“œ[:\s]*([^\n\r,]+)',
                r'ì—¬í–‰|ë¬¸í™”|ì˜ˆìˆ '
            ]
            
            for pattern in topic_patterns:
                matches = re.findall(pattern, response, re.IGNORECASE)
                if matches:
                    result["ì£¼ìš”_ì£¼ì œ"].extend([m.strip() for m in matches if m.strip()])
            
            # ê°ì •ì  í†¤ ì¶”ì¶œ
            tone_patterns = r'(ê¸ì •ì |ë¶€ì •ì |ì¤‘ì„±ì |ì„±ì°°ì |ê°ì„±ì |ë‚­ë§Œì )'
            tone_match = re.search(tone_patterns, response, re.IGNORECASE)
            if tone_match:
                result["ê°ì •ì _í†¤"] = tone_match.group(1)
            
            # ì‹œê°ì  í‚¤ì›Œë“œ ì¶”ì¶œ
            visual_patterns = r'(ìƒ‰ìƒ|í’ê²½|ê±´ë¬¼|ë°”ë‹¤|í•˜ëŠ˜|ê±°ë¦¬|ê´‘ì¥|ë‹¤ë¦¬)'
            visual_matches = re.findall(visual_patterns, response, re.IGNORECASE)
            if visual_matches:
                result["ì‹œê°ì _í‚¤ì›Œë“œ"] = visual_matches
            
            self.logger.info(f"ì •ê·œì‹ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ: {len(result['ì£¼ìš”_ì£¼ì œ'])}ê°œ ì£¼ì œ")
            return result
            
        except Exception as e:
            self.logger.error(f"ì •ê·œì‹ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None



    async def _extract_image_semantics_with_layout_patterns_batch(self, images: List[Dict]) -> List[Dict]:
        """AI Search ë ˆì´ì•„ì›ƒ íŒ¨í„´ì„ ì°¸ì¡°í•œ ì´ë¯¸ì§€ ì˜ë¯¸ ì¶”ì¶œ (ë°°ì¹˜ ì²˜ë¦¬)"""
        
        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ì œí•œ
        semaphore = asyncio.Semaphore(5)  # ìµœëŒ€ 5ê°œ ë™ì‹œ ì²˜ë¦¬
        
        async def process_single_image(i: int, image: Dict) -> Dict:
            async with semaphore:
                try:
                    layout_patterns = await self._search_image_layout_patterns(image)
                    return await self._analyze_image_with_layout_patterns(image, i, layout_patterns)
                except Exception as e:
                    self.logger.error(f"ì´ë¯¸ì§€ {i} ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    return self._get_clean_image_fallback(i, image.get("image_name", f"image_{i}"),
                                                       image.get("location", ""), image.get("image_url", ""))
        
        # ë³‘ë ¬ ì²˜ë¦¬
        tasks = [process_single_image(i, image) for i, image in enumerate(images)]
        image_semantics = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬ëœ ê²°ê³¼ í•„í„°ë§
        valid_results = []
        for i, result in enumerate(image_semantics):
            if isinstance(result, Exception):
                self.logger.error(f"ì´ë¯¸ì§€ {i} ì²˜ë¦¬ ì˜ˆì™¸: {result}")
                valid_results.append(self._get_clean_image_fallback(i, f"image_{i}", "", ""))
            else:
                valid_results.append(result)
        
        return valid_results
    
    async def _extract_image_semantics_with_layout_patterns(self, images: List[Dict]) -> List[Dict]:
        """AI Search ë ˆì´ì•„ì›ƒ íŒ¨í„´ì„ ì°¸ì¡°í•œ ì´ë¯¸ì§€ ì˜ë¯¸ ì¶”ì¶œ"""
        
        image_semantics = []
        
        for i, image in enumerate(images):
            # ì´ë¯¸ì§€ë³„ ë ˆì´ì•„ì›ƒ íŒ¨í„´ ê²€ìƒ‰
            layout_patterns = await self._search_image_layout_patterns(image)
            
            semantic_info = await self._analyze_image_with_layout_patterns(image, i, layout_patterns)
            image_semantics.append(semantic_info)
        
        return image_semantics
    
    async def _search_image_layout_patterns(self, image: Dict) -> List[Dict]:
        """ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ ìœ„í•œ AI Search ë ˆì´ì•„ì›ƒ íŒ¨í„´ ê²€ìƒ‰"""
        
        try:
            image_location = image.get("location", "")
            image_name = image.get("image_name", "")
            
            # ì´ë¯¸ì§€ íŠ¹ì„± ê¸°ë°˜ ì¿¼ë¦¬ ìƒì„±
            search_query = f"ì´ë¯¸ì§€ ë°°ì¹˜ ë ˆì´ì•„ì›ƒ {image_location} {image_name}"
            clean_query = self.isolation_manager.clean_query_from_azure_keywords(search_query)
            
            # ë ˆì´ì•„ì›ƒ íŒ¨í„´ ê²€ìƒ‰
            layout_patterns = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.vector_manager.search_similar_layouts(
                    clean_query, "magazine-vector-index", top_k=5
                )
            )
            
            # ê²©ë¦¬ëœ íŒ¨í„´ë§Œ ë°˜í™˜
            isolated_patterns = self.isolation_manager.filter_contaminated_data(
                layout_patterns, "image_layout_patterns"
            )
            
            return isolated_patterns
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ ë ˆì´ì•„ì›ƒ íŒ¨í„´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def _analyze_image_with_layout_patterns(self, image: Dict, index: int, patterns: List[Dict]) -> Dict:
        """ë ˆì´ì•„ì›ƒ íŒ¨í„´ì„ ì°¸ì¡°í•œ ì´ë¯¸ì§€ ë¶„ì„"""
        
        image_name = image.get("image_name", f"image_{index}")
        location = image.get("location", "")
        image_url = image.get("image_url", "")
        
        # íŒ¨í„´ ê¸°ë°˜ ë ˆì´ì•„ì›ƒ ì •ë³´ ìƒì„±
        layout_context = ""
        if patterns:
            layout_info = []
            for pattern in patterns[:3]:
                layout_info.append({
                    "ì´ë¯¸ì§€_í¬ê¸°": pattern.get("image_size", "ì¤‘ê°„"),
                    "ë°°ì¹˜_ìœ„ì¹˜": pattern.get("placement", "ìƒë‹¨"),
                    "í…ìŠ¤íŠ¸_ê°„ê²©": pattern.get("text_spacing", "ì ë‹¹í•¨"),
                    "ì´ë¯¸ì§€_ê°œìˆ˜": pattern.get("image_count", 1)
                })
            layout_context = f"ë ˆì´ì•„ì›ƒ ì°¸ì¡°: {json.dumps(layout_info, ensure_ascii=False)}"
        
        analysis_prompt = f"""
ë‹¤ìŒ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ë¯¸ì  ìš”ì†Œë¥¼ ë¶„ì„í•˜ì„¸ìš”:

ì´ë¯¸ì§€ëª…: {image_name}
ìœ„ì¹˜ ì •ë³´: {location}

{layout_context}

ë¶„ì„ í•­ëª©:
1. ì§€ë¦¬ì  íŠ¹ì„± (ë„ì‹œ, ìì—°, ê±´ë¬¼ ë“±)
2. ì‹œê°ì  íŠ¹ì§• (ìƒ‰ìƒ, êµ¬ë„, ë¶„ìœ„ê¸° ë“±)
3. ë¬¸í™”ì  ë§¥ë½
4. ê°ì •ì  ì„íŒ©íŠ¸
5. ì‹œê°„ëŒ€/ê³„ì ˆ ì¶”ì •
6. ì í•©í•œ ì´ë¯¸ì§€ í¬ê¸° (ì‘ì€/ì¤‘ê°„/í°)
7. ê¶Œì¥ ë°°ì¹˜ ìœ„ì¹˜ (ìƒë‹¨/ì¤‘ê°„/í•˜ë‹¨)
8. í…ìŠ¤íŠ¸ì™€ì˜ ì ì • ê°„ê²©

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
"""
        
        try:
            response = await self.llm.ainvoke(analysis_prompt)
            
            # âœ… ë™ì¼í•œ ê°•í™”ëœ JSON ì²˜ë¦¬ ë¡œì§ ì ìš©
            if not response or not response.strip():
                self.logger.warning(f"ì´ë¯¸ì§€ {index} ë¶„ì„ì—ì„œ ë¹ˆ ì‘ë‹µ ìˆ˜ì‹ ")
                return self._get_clean_image_fallback(index, image_name, location, image_url)
            
            cleaned_response = self._extract_json_from_response(response.strip())
            validated_json = self._validate_and_fix_json(cleaned_response)
            
            try:
                analysis_result = json.loads(validated_json)
            except json.JSONDecodeError as json_error:
                self.logger.error(f"ì´ë¯¸ì§€ {index} JSON íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: {json_error}")
                # ì´ë¯¸ì§€ìš© ì •ê·œì‹ í‚¤ì›Œë“œ ì¶”ì¶œë„ êµ¬í˜„ ê°€ëŠ¥
                return self._get_clean_image_fallback(index, image_name, location, image_url)
            
            return {
                "image_index": index,
                "image_name": image_name,
                "location": location,
                "image_url": image_url,
                "semantic_analysis": analysis_result,
                "confidence_score": 0.8,
                "layout_patterns": len(patterns),
                "isolation_metadata": {
                    "patterns_referenced": len(patterns),
                    "contamination_detected": False
                }
            }
            
        except Exception as e:
            self.logger.error(f"ì´ë¯¸ì§€ {index} ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_clean_image_fallback(index, image_name, location, image_url)
    
    async def _perform_semantic_matching_with_vectors(self, text_semantics: List[Dict],
                                                    image_semantics: List[Dict]) -> List[Dict]:
        """ë²¡í„° ë°ì´í„° ê¸°ë°˜ ì˜ë¯¸ì  ë§¤ì¹­"""
        
        mappings = []
        
        for text_section in text_semantics:
            section_mappings = []
            
            for image in image_semantics:
                # AI Search íŒ¨í„´ì„ ê³ ë ¤í•œ ìœ ì‚¬ë„ ê³„ì‚°
                similarity_score = await self._calculate_semantic_similarity_with_patterns(
                    text_section, image
                )
                
                section_mappings.append({
                    "image_index": image["image_index"],
                    "image_name": image["image_name"],
                    "similarity_score": similarity_score,
                    "matching_factors": self._identify_matching_factors_with_patterns(text_section, image),
                    "layout_recommendation": self._get_layout_recommendation(text_section, image)
                })
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            section_mappings.sort(key=lambda x: x["similarity_score"], reverse=True)
            
            mappings.append({
                "text_section_index": text_section["section_index"],
                "text_title": text_section["title"],
                "image_matches": section_mappings[:5]
            })
        
        return mappings
    
    def _extract_json_from_response(self, response: str) -> str:
        """LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ê°•í™”ëœ ë²„ì „)"""
        
        if not response or not response.strip():
            return "{}"  # ë¹ˆ ì‘ë‹µ ì‹œ ê¸°ë³¸ JSON ë°˜í™˜
        
        response = response.strip()
        
        # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (ë‹¤ì–‘í•œ íŒ¨í„´ ì§€ì›)
        patterns = [
            r'``````',  # ``````
            r'``````',      # ``````
            r'`(.*?)`',                # `...`
        ]
        
        for pattern in patterns:
            import re
            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
            if match:
                extracted = match.group(1).strip()
                if extracted and (extracted.startswith('{') or extracted.startswith('[')):
                    return extracted
        
        # 2. HTML/XML íƒœê·¸ ì œê±°
        if response.startswith('<'):
            # HTML/XML ì‘ë‹µì¸ ê²½ìš° JSON ë¶€ë¶„ ì°¾ê¸°
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json_match.group(0)
            else:
                return "{}"  # JSONì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ë¹ˆ ê°ì²´
        
        # 3. ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ì¤‘ê´„í˜¸ ì‚¬ì´ì˜ ë‚´ìš© ì¶”ì¶œ
        first_brace = response.find('{')
        last_brace = response.rfind('}')
        
        if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
            potential_json = response[first_brace:last_brace + 1]
            return potential_json
        
        # 4. ë°°ì—´ í˜•íƒœ JSON ì²˜ë¦¬
        first_bracket = response.find('[')
        last_bracket = response.rfind(']')
        
        if first_bracket != -1 and last_bracket != -1 and first_bracket < last_bracket:
            potential_json = response[first_bracket:last_bracket + 1]
            return potential_json
        
        # 5. ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë°˜í™˜
        return response.strip()

    def _validate_and_fix_json(self, json_str: str) -> str:
        """JSON ë¬¸ìì—´ ê²€ì¦ ë° ìˆ˜ì •"""
        
        try:
            # 1. ê¸°ë³¸ JSON íŒŒì‹± ì‹œë„
            json.loads(json_str)
            return json_str
        except json.JSONDecodeError:
            pass
        
        # 2. ì¼ë°˜ì ì¸ JSON ì˜¤ë¥˜ ìˆ˜ì • ì‹œë„
        fixed_json = json_str
        
        # ë”°ì˜´í‘œ ë¬¸ì œ ìˆ˜ì •
        fixed_json = fixed_json.replace("'", '"')  # ë‹¨ì¼ ë”°ì˜´í‘œë¥¼ ì´ì¤‘ ë”°ì˜´í‘œë¡œ
        fixed_json = fixed_json.replace('True', 'true')  # Python boolì„ JSON boolë¡œ
        fixed_json = fixed_json.replace('False', 'false')
        fixed_json = fixed_json.replace('None', 'null')
        
        # ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±°
        import re
        fixed_json = re.sub(r',\s*}', '}', fixed_json)
        fixed_json = re.sub(r',\s*]', ']', fixed_json)
        
        try:
            json.loads(fixed_json)
            return fixed_json
        except json.JSONDecodeError:
            pass
        
        # 3. ë” ê³µê²©ì ì¸ ìˆ˜ì • ì‹œë„
        try:
            # í‚¤ì— ë”°ì˜´í‘œ ì¶”ê°€
            fixed_json = re.sub(r'(\w+):', r'"\1":', fixed_json)
            json.loads(fixed_json)
            return fixed_json
        except:
            pass
        
        # 4. ëª¨ë“  ìˆ˜ì • ì‹œë„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ JSON ë°˜í™˜
        return '{"error": "JSON íŒŒì‹± ì‹¤íŒ¨", "original": "' + json_str.replace('"', '\\"')[:100] + '"}'




    async def _calculate_semantic_similarity_with_patterns(self, text_section: Dict, image: Dict) -> float:
        """AI Search íŒ¨í„´ì„ ê³ ë ¤í•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°"""
        
        text_analysis = text_section.get("semantic_analysis", {})
        image_analysis = image.get("semantic_analysis", {})
        
        similarity_factors = []
        
        # ê¸°ë³¸ ì˜ë¯¸ì  ë§¤ì¹­
        text_keywords = text_analysis.get("ì‹œê°ì _í‚¤ì›Œë“œ", [])
        image_features = image_analysis.get("ì§€ë¦¬ì _íŠ¹ì„±", [])
        keyword_match = len(set(text_keywords) & set(image_features)) / max(len(text_keywords), 1)
        similarity_factors.append(keyword_match * 0.3)
        
        # ê°ì •ì  í†¤ ë§¤ì¹­
        text_tone = text_analysis.get("ê°ì •ì _í†¤", "")
        image_impact = image_analysis.get("ê°ì •ì _ì„íŒ©íŠ¸", "")
        tone_match = 1.0 if text_tone == image_impact else 0.5
        similarity_factors.append(tone_match * 0.2)
        
        # AI Search íŒ¨í„´ ê¸°ë°˜ ë ˆì´ì•„ì›ƒ ì í•©ì„±
        layout_compatibility = self._calculate_layout_compatibility(text_section, image)
        similarity_factors.append(layout_compatibility * 0.3)
        
        # ë¬¸í™”ì /ì§€ë¦¬ì  ì—°ê´€ì„±
        cultural_match = self._calculate_cultural_relevance(text_analysis, image_analysis)
        similarity_factors.append(cultural_match * 0.2)
        
        # ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚°
        total_similarity = sum(similarity_factors)
        
        # ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš©
        confidence_weight = (text_section.get("confidence_score", 0.5) + 
                           image.get("confidence_score", 0.5)) / 2
        
        return min(total_similarity * confidence_weight, 1.0)
    
    def _calculate_layout_compatibility(self, text_section: Dict, image: Dict) -> float:
        """ë ˆì´ì•„ì›ƒ í˜¸í™˜ì„± ê³„ì‚°"""
        
        text_patterns = text_section.get("ai_search_patterns", 0)
        image_patterns = image.get("layout_patterns", 0)
        
        # íŒ¨í„´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜
        if text_patterns > 0 and image_patterns > 0:
            return 0.9
        elif text_patterns > 0 or image_patterns > 0:
            return 0.7
        else:
            return 0.5
    
    def _calculate_cultural_relevance(self, text_analysis: Dict, image_analysis: Dict) -> float:
        """ë¬¸í™”ì  ì—°ê´€ì„± ê³„ì‚°"""
        
        text_cultural = text_analysis.get("ë¬¸í™”ì _ìš”ì†Œ", [])
        image_cultural = image_analysis.get("ë¬¸í™”ì _ë§¥ë½", [])
        
        if not text_cultural and not image_cultural:
            return 0.5
        
        common_cultural = set(text_cultural) & set(image_cultural)
        total_cultural = set(text_cultural) | set(image_cultural)
        
        if not total_cultural:
            return 0.5
        
        return len(common_cultural) / len(total_cultural)
    
    def _identify_matching_factors_with_patterns(self, text_section: Dict, image: Dict) -> List[str]:
        """AI Search íŒ¨í„´ì„ ê³ ë ¤í•œ ë§¤ì¹­ ìš”ì¸ ì‹ë³„"""
        
        factors = []
        
        text_analysis = text_section.get("semantic_analysis", {})
        image_analysis = image.get("semantic_analysis", {})
        
        # ê¸°ë³¸ ë§¤ì¹­ ìš”ì¸
        text_keywords = set(text_analysis.get("ì‹œê°ì _í‚¤ì›Œë“œ", []))
        image_features = set(image_analysis.get("ì§€ë¦¬ì _íŠ¹ì„±", []))
        
        common_elements = text_keywords & image_features
        if common_elements:
            factors.append(f"ê³µí†µ_ìš”ì†Œ: {', '.join(common_elements)}")
        
        # AI Search íŒ¨í„´ ê¸°ë°˜ ìš”ì¸
        if text_section.get("ai_search_patterns", 0) > 0:
            factors.append("í…ìŠ¤íŠ¸_íŒ¨í„´_ì°¸ì¡°")
        
        if image.get("layout_patterns", 0) > 0:
            factors.append("ë ˆì´ì•„ì›ƒ_íŒ¨í„´_ì°¸ì¡°")
        
        # ë ˆì´ì•„ì›ƒ í˜¸í™˜ì„±
        layout_score = self._calculate_layout_compatibility(text_section, image)
        if layout_score > 0.8:
            factors.append("ë ˆì´ì•„ì›ƒ_ê³ ë„_í˜¸í™˜")
        elif layout_score > 0.6:
            factors.append("ë ˆì´ì•„ì›ƒ_ì ë‹¹_í˜¸í™˜")
        
        return factors
    
    def _get_layout_recommendation(self, text_section: Dict, image: Dict) -> Dict:
        """ë ˆì´ì•„ì›ƒ ì¶”ì²œ ìƒì„±"""
        
        image_analysis = image.get("semantic_analysis", {})
        
        return {
            "ê¶Œì¥_ì´ë¯¸ì§€_í¬ê¸°": image_analysis.get("ì í•©í•œ_ì´ë¯¸ì§€_í¬ê¸°", "ì¤‘ê°„"),
            "ê¶Œì¥_ë°°ì¹˜_ìœ„ì¹˜": image_analysis.get("ê¶Œì¥_ë°°ì¹˜_ìœ„ì¹˜", "ìƒë‹¨"),
            "í…ìŠ¤íŠ¸_ê°„ê²©": image_analysis.get("í…ìŠ¤íŠ¸ì™€ì˜_ì ì •_ê°„ê²©", "ì ë‹¹í•¨"),
            "íŒ¨í„´_ê¸°ë°˜": text_section.get("ai_search_patterns", 0) > 0 and image.get("layout_patterns", 0) > 0
        }
    
    async def _generate_optimal_combinations_with_ai_search(self, semantic_mappings: List[Dict]) -> List[Dict]:
        """AI Search ë°ì´í„°ë¥¼ í™œìš©í•œ ìµœì  ì¡°í•© ìƒì„± (ë‹¤ì–‘ì„± ìµœì í™” ì¸ì‹)"""
        
        optimal_combinations = []
        used_images = set()
        
        for mapping in semantic_mappings:
            section_index = mapping["text_section_index"]
            section_title = mapping["text_title"]
            
            # âœ… ê¸°ì¡´ AI Search íŒ¨í„´ ê¸°ë°˜ ì´ë¯¸ì§€ ì„ íƒ ë¡œì§ ìœ ì§€
            best_images = []
            semantic_matches = []  # âœ… ì˜ë¯¸ì  ë§¤ì¹­ ì •ë³´ë„ ë³„ë„ ë³´ê´€
            
            for image_match in mapping["image_matches"]:
                image_index = image_match["image_index"]
                
                # âœ… ì¤‘ë³µ ë°©ì§€ (ImageDiversityManagerì™€ í˜‘ë ¥)
                if image_index not in used_images:
                    semantic_matches.append(image_match)  # ì˜ë¯¸ì  ë§¤ì¹­ ì •ë³´ ë³´ê´€
                    
                    # âœ… ê¸°ì¡´ AI Search íŒ¨í„´ ìš°ì„ ìˆœìœ„ ë¡œì§ ìœ ì§€
                    layout_rec = image_match.get("layout_recommendation", {})
                    if layout_rec.get("íŒ¨í„´_ê¸°ë°˜", False):
                        best_images.insert(0, image_match)  # ì•ì— ì¶”ê°€
                    else:
                        best_images.append(image_match)
                    
                    used_images.add(image_index)
                    
                    # âœ… ê¸°ì¡´ ì œí•œ ë¡œì§ ìœ ì§€
                    if len(best_images) >= 3:
                        break
            
            # âœ… ê¸°ì¡´ êµ¬ì¡° ìœ ì§€ + ë‹¤ì–‘ì„± ì •ë³´ ì¶”ê°€
            optimal_combinations.append({
                "section_index": section_index,
                "section_title": section_title,
                "assigned_images": best_images,  # âœ… ê¸°ì¡´ í‚¤ ìœ ì§€ (í˜¸í™˜ì„±)
                "semantic_matches": semantic_matches,  # âœ… ì¶”ê°€ ì •ë³´
                "total_similarity_score": sum(img["similarity_score"] for img in best_images),
                "semantic_score": sum(match.get("similarity_score", 0) for match in semantic_matches),
                "ai_search_enhanced": any(img.get("layout_recommendation", {}).get("íŒ¨í„´_ê¸°ë°˜", False) for img in best_images),
                "diversity_aware": True,  # âœ… ë‹¤ì–‘ì„± ì¸ì‹ í‘œì‹œ
                "optimization_notes": f"{len(best_images)}ê°œ ì´ë¯¸ì§€ í• ë‹¹ë¨ (AI Search íŒ¨í„´ + ë‹¤ì–‘ì„± ì¸ì‹)"
            })
        
        self.logger.info(f"ì˜ë¯¸ì  ë¶„ì„ ì™„ë£Œ: {len(optimal_combinations)}ê°œ ì¡°í•©, "
                        f"{len(used_images)}ê°œ ê³ ìœ  ì´ë¯¸ì§€ (AI Search íŒ¨í„´ + ë‹¤ì–‘ì„± ìµœì í™”)")
        
        return optimal_combinations
    
    # ê¸°ì¡´ í—¬í¼ ë©”ì„œë“œë“¤ ìœ ì§€
    def _get_clean_analysis_fallback(self) -> Dict:
        """ì˜¤ì—¼ë˜ì§€ ì•Šì€ ê¸°ë³¸ ë¶„ì„ ê²°ê³¼"""
        return {
            "ì£¼ìš”_ì£¼ì œ": ["ì—¬í–‰"],
            "ê°ì •ì _í†¤": "ì¤‘ì„±ì ",
            "ì‹œê°ì _í‚¤ì›Œë“œ": ["í’ê²½"],
            "ê³„ì ˆ_ì‹œê°„": "ì•Œ ìˆ˜ ì—†ìŒ",
            "ë¬¸í™”ì _ìš”ì†Œ": [],
            "ê¸€ì˜_í˜•íƒœ": "ì„œìˆ í˜•",
            "ë¬¸ì¥_ê¸¸ì´_íŠ¹ì„±": "ì¤‘ê°„"
        }
    
    def _get_clean_section_fallback(self, index: int, title: str, content: str) -> Dict:
        """ì˜¤ì—¼ë˜ì§€ ì•Šì€ ê¸°ë³¸ ì„¹ì…˜ ê²°ê³¼"""
        return {
            "section_index": index,
            "title": title,
            "content_preview": content[:200],
            "semantic_analysis": self._get_clean_analysis_fallback(),
            "confidence_score": 0.3,
            "ai_search_patterns": 0,
            "isolation_metadata": {
                "patterns_referenced": 0,
                "contamination_detected": True,
                "fallback_used": True
            }
        }
    
    def _get_clean_image_fallback(self, index: int, image_name: str, location: str, image_url: str) -> Dict:
        """ì˜¤ì—¼ë˜ì§€ ì•Šì€ ê¸°ë³¸ ì´ë¯¸ì§€ ê²°ê³¼"""
        return {
            "image_index": index,
            "image_name": image_name,
            "location": location,
            "image_url": image_url,
            "semantic_analysis": {
                "ì§€ë¦¬ì _íŠ¹ì„±": ["ë„ì‹œ"],
                "ì‹œê°ì _íŠ¹ì§•": ["ì¼ë°˜ì "],
                "ë¬¸í™”ì _ë§¥ë½": [],
                "ê°ì •ì _ì„íŒ©íŠ¸": "ì¤‘ì„±ì ",
                "ì‹œê°„ëŒ€_ê³„ì ˆ": "ì•Œ ìˆ˜ ì—†ìŒ",
                "ì í•©í•œ_ì´ë¯¸ì§€_í¬ê¸°": "ì¤‘ê°„",
                "ê¶Œì¥_ë°°ì¹˜_ìœ„ì¹˜": "ìƒë‹¨",
                "í…ìŠ¤íŠ¸ì™€ì˜_ì ì •_ê°„ê²©": "ì ë‹¹í•¨"
            },
            "confidence_score": 0.3,
            "layout_patterns": 0,
            "isolation_metadata": {
                "patterns_referenced": 0,
                "contamination_detected": True
            }
        }
    
    def _generate_clean_fallback_result(self, content: Dict, images: List[Dict]) -> Dict:
        """ì™„ì „íˆ ì •í™”ëœ í´ë°± ê²°ê³¼"""
        return {
            "text_semantics": [],
            "image_semantics": [],
            "semantic_mappings": [],
            "optimal_combinations": [],
            "analysis_metadata": {
                "total_text_sections": 0,
                "total_images": 0,
                "mapping_confidence": 0.0,
                "ai_search_enhanced": False,
                "isolation_applied": True,
                "contamination_detected": True,
                "fallback_used": True
            }
        }
    
    def _calculate_overall_confidence(self, semantic_mappings: List[Dict]) -> float:
        """ì „ì²´ ë§¤í•‘ ì‹ ë¢°ë„ ê³„ì‚°"""
        
        if not semantic_mappings:
            return 0.0
        
        total_confidence = 0.0
        total_matches = 0
        
        for mapping in semantic_mappings:
            for image_match in mapping["image_matches"]:
                total_confidence += image_match["similarity_score"]
                total_matches += 1
        
        return total_confidence / max(total_matches, 1)
