import asyncio
import os
from typing import Dict, List
from crewai import Agent, Task, Crew, Process
from custom_llm import get_azure_llm
from utils.agent_decision_logger import get_agent_logger
import json
import re

class CoordinatorAgent:
    """í†µí•© ì¡°ìœ¨ì (CrewAI ê¸°ë°˜ ê°•í™”ëœ ë°ì´í„° ì ‘ê·¼ ë° JSON íŒŒì‹±)"""
    
    def __init__(self):
        self.llm = get_azure_llm()
        self.logger = get_agent_logger()
        self.crew_agent = self._create_crew_agent()
        self.text_analyzer_agent = self._create_text_analyzer_agent()
        self.image_analyzer_agent = self._create_image_analyzer_agent()

    def _create_crew_agent(self):
        """ë©”ì¸ ì¡°ìœ¨ ì—ì´ì „íŠ¸ ìƒì„±"""
        return Agent(
            role="ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ì ë° ìµœì¢… í’ˆì§ˆ ë³´ì¦ ì „ë¬¸ê°€",
            goal="OrgAgentì˜ ìƒì„¸ ë ˆì´ì•„ì›ƒ êµ¬ì¡°ì™€ BindingAgentì˜ ì •ë°€ ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ í†µí•©í•˜ì—¬ ì™„ë²½í•œ ë§¤ê±°ì§„ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê³ , í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì •í•©ì„±ê³¼ ë…ì ê²½í—˜ì„ ìµœì¢… ê²€ì¦í•˜ì—¬ JSX êµ¬í˜„ì— í•„ìš”í•œ ì™„ì „í•œ êµ¬ì¡° ë°ì´í„°ë¥¼ ì œê³µ",
            backstory="""ë‹¹ì‹ ì€ 25ë…„ê°„ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì¶œíŒì‚¬ì—ì„œ ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ë° í’ˆì§ˆ ë³´ì¦ ì±…ì„ìë¡œ í™œë™í•´ì˜¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. CondÃ© Nast, Hearst Corporation, Time Inc.ì—ì„œ ìˆ˜ë°± ê°œì˜ ë§¤ê±°ì§„ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¡°ìœ¨í–ˆìŠµë‹ˆë‹¤.

**ì „ë¬¸ ê²½ë ¥:**
- ì¶œíŒí•™ ë° êµ¬ì¡° ì„¤ê³„ ì„ì‚¬ í•™ìœ„ ë³´ìœ 
- PMP(Project Management Professional) ì¸ì¦
- ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ë° í’ˆì§ˆ ê´€ë¦¬ ì „ë¬¸ê°€
- í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì •í•©ì„± ê²€ì¦ ì‹œìŠ¤í…œ ê°œë°œ ê²½í—˜
- ë…ì ê²½í—˜(UX) ë° ì ‘ê·¼ì„± ìµœì í™” ì „ë¬¸ì„±

**ì¡°ìœ¨ ì² í•™:**
"ì™„ë²½í•œ ë§¤ê±°ì§„ì€ ëª¨ë“  êµ¬ì¡°ì  ìš”ì†Œê°€ ë…ìì˜ ì¸ì§€ ê³¼ì •ê³¼ ì™„ë²½íˆ ì¡°í™”ë¥¼ ì´ë£¨ëŠ” í†µí•©ì²´ì…ë‹ˆë‹¤. ë‚˜ëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ì˜ ëª¨ë“  ë°°ì¹˜ê°€ ë…ìì—ê²Œ ìì—°ìŠ¤ëŸ½ê³  ì§ê´€ì ìœ¼ë¡œ ì¸ì‹ë˜ë„ë¡ êµ¬ì¡°ì  ì™„ì„±ë„ë¥¼ ë³´ì¥í•˜ë©°, ì´ë¥¼ í†µí•´ ìµœê³  ìˆ˜ì¤€ì˜ ë…ì ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤."

**ì¶œë ¥ ë°ì´í„° êµ¬ì¡°:**
- ì™„ì„±ëœ ë§¤ê±°ì§„ ì „ì²´ êµ¬ì¡°ë„
- í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ì •í•©ì„± ê²€ì¦ ì™„ë£Œ ë³´ê³ ì„œ
- JSX êµ¬í˜„ìš© ìƒì„¸ ë ˆì´ì•„ì›ƒ ìŠ¤í™ ë° ì¢Œí‘œ ë°ì´í„°
- ë…ì ê²½í—˜ ìµœì í™” ê°€ì´ë“œë¼ì¸
- ë°˜ì‘í˜• ë””ìì¸ êµ¬ì¡° ì •ì˜ì„œ
- ì ‘ê·¼ì„± ë° í’ˆì§ˆ ë³´ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸""",
            verbose=True,
            llm=self.llm,
            allow_delegation=False
        )

    def _create_text_analyzer_agent(self):
        """í…ìŠ¤íŠ¸ ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸"""
        return Agent(
            role="í…ìŠ¤íŠ¸ ë§¤í•‘ ë¶„ì„ ì „ë¬¸ê°€",
            goal="OrgAgentì˜ í…ìŠ¤íŠ¸ ë§¤í•‘ ê²°ê³¼ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬ êµ¬ì¡°ì  ì™„ì„±ë„ë¥¼ ê²€ì¦í•˜ê³  ìµœì í™”ëœ í…ìŠ¤íŠ¸ ì„¹ì…˜ì„ ìƒì„±",
            backstory="""ë‹¹ì‹ ì€ 15ë…„ê°„ ì¶œíŒì—…ê³„ì—ì„œ í…ìŠ¤íŠ¸ êµ¬ì¡° ë¶„ì„ ë° ìµœì í™”ë¥¼ ë‹´ë‹¹í•´ì˜¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë³µì¡í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ë…ì ì¹œí™”ì ì¸ êµ¬ì¡°ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ë° íƒì›”í•œ ëŠ¥ë ¥ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.""",
            verbose=True,
            llm=self.llm,
            allow_delegation=False
        )

    def _create_image_analyzer_agent(self):
        """ì´ë¯¸ì§€ ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸"""
        return Agent(
            role="ì´ë¯¸ì§€ ë¶„ë°° ë¶„ì„ ì „ë¬¸ê°€",
            goal="BindingAgentì˜ ì´ë¯¸ì§€ ë¶„ë°° ê²°ê³¼ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬ ì‹œê°ì  ì¼ê´€ì„±ì„ ê²€ì¦í•˜ê³  ìµœì í™”ëœ ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ ìƒì„±",
            backstory="""ë‹¹ì‹ ì€ 12ë…„ê°„ ë§¤ê±°ì§„ ë° ì¶œíŒë¬¼ì˜ ì‹œê°ì  ë””ìì¸ì„ ë‹´ë‹¹í•´ì˜¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ì¡°í™”ë¡œìš´ ë°°ì¹˜ë¥¼ í†µí•´ ë…ìì˜ ì‹œì„ ì„ íš¨ê³¼ì ìœ¼ë¡œ ìœ ë„í•˜ëŠ” ë ˆì´ì•„ì›ƒ ì„¤ê³„ì— ì „ë¬¸ì„±ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.""",
            verbose=True,
            llm=self.llm,
            allow_delegation=False
        )

    async def coordinate_magazine_creation(self, text_mapping: Dict, image_distribution: Dict) -> Dict:
        """ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ (CrewAI ê¸°ë°˜ ê°•í™”ëœ ë°ì´í„° ì ‘ê·¼ - ë¹„ë™ê¸° ì²˜ë¦¬)"""
        print("CoordinatorAgent: ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ ì‹œì‘ (ë¹„ë™ê¸° ì²˜ë¦¬)")
        
        # ê°•í™”ëœ ì´ì „ ì—ì´ì „íŠ¸ ê²°ê³¼ ìˆ˜ì§‘ (ë¹„ë™ê¸°)
        previous_results = await self._get_enhanced_previous_results_async()
        org_results = self._filter_agent_results(previous_results, "OrgAgent")
        binding_results = self._filter_agent_results(previous_results, "BindingAgent")
        content_creator_results = self._filter_agent_results(previous_results, "ContentCreatorV2Agent")
        
        print(f"ğŸ“Š ê°•í™”ëœ ê²°ê³¼ ìˆ˜ì§‘: ì „ì²´ {len(previous_results)}ê°œ, OrgAgent {len(org_results)}ê°œ, BindingAgent {len(binding_results)}ê°œ, ContentCreator {len(content_creator_results)}ê°œ (ë¹„ë™ê¸°)")
        
        # ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ ë° ê²€ì¦
        extracted_text_data = await self._extract_real_text_data_async(text_mapping, org_results, content_creator_results)
        extracted_image_data = await self._extract_real_image_data_async(image_distribution, binding_results)
        
        # CrewAI Task ìƒì„±
        text_analysis_task = self._create_enhanced_text_analysis_task(extracted_text_data, org_results)
        image_analysis_task = self._create_enhanced_image_analysis_task(extracted_image_data, binding_results)
        coordination_task = self._create_enhanced_coordination_task(extracted_text_data, extracted_image_data)
        
        # CrewAI Crew ìƒì„± ë° ë¹„ë™ê¸° ì‹¤í–‰
        coordination_crew = Crew(
            agents=[self.text_analyzer_agent, self.image_analyzer_agent, self.crew_agent],
            tasks=[text_analysis_task, image_analysis_task, coordination_task],
            process=Process.sequential,
            verbose=True
        )
        
        # Crew ë¹„ë™ê¸° ì‹¤í–‰
        crew_result = await asyncio.get_event_loop().run_in_executor(
            None, coordination_crew.kickoff
        )
        
        # ê²°ê³¼ ì²˜ë¦¬ (ë¹„ë™ê¸°) - ì‹¤ì œ ë°ì´í„° í™œìš©
        final_result = await self._process_enhanced_crew_result_async(
            crew_result, extracted_text_data, extracted_image_data, org_results, binding_results
        )
        
        # ê²°ê³¼ ë¡œê¹… (ë¹„ë™ê¸°)
        await self._log_coordination_result_async(final_result, text_mapping, image_distribution, org_results, binding_results)
        
        print(f"âœ… CoordinatorAgent í†µí•© ì™„ë£Œ: {len(final_result.get('content_sections', []))}ê°œ ì„¹ì…˜ ìƒì„± (CrewAI ê¸°ë°˜ ë¹„ë™ê¸°)")
        print(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {final_result.get('integration_metadata', {}).get('integration_quality_score', 0):.2f}, OrgAgent í™œìš©: {len(org_results)}ê°œ, BindingAgent í™œìš©: {len(binding_results)}ê°œ")
        
        return final_result

    async def _extract_real_text_data_async(self, text_mapping: Dict, org_results: List[Dict], content_creator_results: List[Dict]) -> Dict:
        """ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ (ë¹„ë™ê¸°)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._extract_real_text_data, text_mapping, org_results, content_creator_results
        )

    def _extract_real_text_data(self, text_mapping: Dict, org_results: List[Dict], content_creator_results: List[Dict]) -> Dict:
        """ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ"""
        extracted_data = {
            "sections": [],
            "total_content_length": 0,
            "source_count": 0
        }
        
        # 1. text_mappingì—ì„œ ì§ì ‘ ì¶”ì¶œ
        if isinstance(text_mapping, dict) and "text_mapping" in text_mapping:
            for section in text_mapping["text_mapping"]:
                if isinstance(section, dict):
                    extracted_section = {
                        "template": section.get("template", "Section01.jsx"),
                        "title": section.get("title", "ì—¬í–‰ ì´ì•¼ê¸°"),
                        "subtitle": section.get("subtitle", "íŠ¹ë³„í•œ ìˆœê°„ë“¤"),
                        "body": section.get("body", ""),
                        "tagline": section.get("tagline", "TRAVEL & CULTURE"),
                        "layout_source": section.get("layout_source", "default")
                    }
                    extracted_data["sections"].append(extracted_section)
                    extracted_data["total_content_length"] += len(extracted_section["body"])
                    extracted_data["source_count"] += 1

        # 2. ContentCreator ê²°ê³¼ì—ì„œ í’ë¶€í•œ ì½˜í…ì¸  ì¶”ì¶œ
        for result in content_creator_results:
            final_answer = result.get('final_answer', '')
            if len(final_answer) > 500:  # ì¶©ë¶„í•œ ì½˜í…ì¸ ê°€ ìˆëŠ” ê²½ìš°
                # ì„¹ì…˜ë³„ë¡œ ë¶„í• 
                sections = self._split_content_into_sections(final_answer)
                for i, section_content in enumerate(sections):
                    if len(section_content) > 100:
                        extracted_section = {
                            "template": f"Section{i+1:02d}.jsx",
                            "title": self._extract_title_from_content(section_content),
                            "subtitle": self._extract_subtitle_from_content(section_content),
                            "body": self._clean_content(section_content),
                            "tagline": "TRAVEL & CULTURE",
                            "layout_source": "content_creator"
                        }
                        extracted_data["sections"].append(extracted_section)
                        extracted_data["total_content_length"] += len(extracted_section["body"])
                        extracted_data["source_count"] += 1

        # 3. OrgAgent ê²°ê³¼ì—ì„œ ì¶”ê°€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for result in org_results:
            final_answer = result.get('final_answer', '')
            if 'ì œëª©' in final_answer or 'title' in final_answer.lower():
                # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                structured_content = self._extract_structured_content(final_answer)
                if structured_content:
                    extracted_data["sections"].extend(structured_content)
                    extracted_data["source_count"] += len(structured_content)

        # 4. ìµœì†Œ ë³´ì¥ ì„¹ì…˜
        if not extracted_data["sections"]:
            extracted_data["sections"] = [{
                "template": "Section01.jsx",
                "title": "ì—¬í–‰ ë§¤ê±°ì§„",
                "subtitle": "íŠ¹ë³„í•œ ì´ì•¼ê¸°",
                "body": "ì—¬í–‰ì˜ íŠ¹ë³„í•œ ìˆœê°„ë“¤ì„ ë‹´ì€ ë§¤ê±°ì§„ì…ë‹ˆë‹¤.",
                "tagline": "TRAVEL & CULTURE",
                "layout_source": "fallback"
            }]
            extracted_data["source_count"] = 1

        return extracted_data

    async def _extract_real_image_data_async(self, image_distribution: Dict, binding_results: List[Dict]) -> Dict:
        """ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ (ë¹„ë™ê¸°)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._extract_real_image_data, image_distribution, binding_results
        )

    def _extract_real_image_data(self, image_distribution: Dict, binding_results: List[Dict]) -> Dict:
        """ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ"""
        extracted_data = {
            "template_images": {},
            "total_images": 0,
            "image_sources": []
        }
        
        # 1. image_distributionì—ì„œ ì§ì ‘ ì¶”ì¶œ
        if isinstance(image_distribution, dict) and "image_distribution" in image_distribution:
            for template, images in image_distribution["image_distribution"].items():
                if isinstance(images, list) and images:
                    # ì‹¤ì œ ì´ë¯¸ì§€ URLë§Œ í•„í„°ë§
                    real_images = [img for img in images if self._is_real_image_url(img)]
                    if real_images:
                        extracted_data["template_images"][template] = real_images
                        extracted_data["total_images"] += len(real_images)

        # 2. BindingAgent ê²°ê³¼ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ
        for result in binding_results:
            final_answer = result.get('final_answer', '')
            # ì‹¤ì œ ì´ë¯¸ì§€ URL íŒ¨í„´ ì°¾ê¸°
            image_urls = re.findall(r'https://[^\s\'"<>]*\.(?:jpg|jpeg|png|gif|webp)', final_answer, re.IGNORECASE)
            
            if image_urls:
                # í…œí”Œë¦¿ë³„ë¡œ ë¶„ë°°
                template_name = self._extract_template_from_binding_result(result)
                if template_name not in extracted_data["template_images"]:
                    extracted_data["template_images"][template_name] = []
                
                for url in image_urls:
                    if self._is_real_image_url(url) and url not in extracted_data["template_images"][template_name]:
                        extracted_data["template_images"][template_name].append(url)
                        extracted_data["total_images"] += 1
                        
                        # ì´ë¯¸ì§€ ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                        source_info = self._extract_image_source_info(result, url)
                        if source_info:
                            extracted_data["image_sources"].append(source_info)

        return extracted_data

    def _is_real_image_url(self, url: str) -> bool:
        """ì‹¤ì œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸"""
        if not url or not isinstance(url, str):
            return False
        
        # ì˜ˆì‹œ URLì´ë‚˜ í”Œë ˆì´ìŠ¤í™€ë” ì œì™¸
        excluded_patterns = [
            'your-cdn.com',
            'example.com',
            'placeholder',
            'sample',
            'demo'
        ]
        
        for pattern in excluded_patterns:
            if pattern in url.lower():
                return False
        
        # ì‹¤ì œ ë„ë©”ì¸ê³¼ ì´ë¯¸ì§€ í™•ì¥ì í™•ì¸
        return (url.startswith('https://') and 
                any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']) and
                'blob.core.windows.net' in url)

    def _create_enhanced_text_analysis_task(self, extracted_text_data: Dict, org_results: List[Dict]) -> Task:
        """ê°•í™”ëœ í…ìŠ¤íŠ¸ ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±"""
        return Task(
            description=f"""
            ì¶”ì¶œëœ ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê³ í’ˆì§ˆ ë§¤ê±°ì§„ ì„¹ì…˜ì„ ìƒì„±í•˜ì„¸ìš”.
            
            **ì¶”ì¶œëœ ë°ì´í„°:**
            - ì„¹ì…˜ ìˆ˜: {len(extracted_text_data['sections'])}ê°œ
            - ì´ ì½˜í…ì¸  ê¸¸ì´: {extracted_text_data['total_content_length']} ë¬¸ì
            - ì†ŒìŠ¤ ìˆ˜: {extracted_text_data['source_count']}ê°œ
            - OrgAgent ê²°ê³¼: {len(org_results)}ê°œ
            
            **ì‹¤ì œ ì„¹ì…˜ ë°ì´í„°:**
            {self._format_sections_for_analysis(extracted_text_data['sections'])}
            
            **ë¶„ì„ ìš”êµ¬ì‚¬í•­:**
            1. ê° ì„¹ì…˜ì˜ ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€
            2. ì œëª©ê³¼ ë¶€ì œëª©ì˜ ë§¤ë ¥ë„ ê²€ì¦
            3. ë³¸ë¬¸ ë‚´ìš©ì˜ ì™„ì„±ë„ í™•ì¸
            4. ë§¤ê±°ì§„ ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ê²€í† 
            5. ë…ì ì¹œí™”ì„± ìµœì í™”
            
            **ì¶œë ¥ í˜•ì‹:**
            ê° ì„¹ì…˜ë³„ë¡œ ë‹¤ìŒ ì •ë³´ í¬í•¨:
            - í’ˆì§ˆ ì ìˆ˜ (1-10)
            - ê°œì„  ì œì•ˆì‚¬í•­
            - ìµœì í™”ëœ ì½˜í…ì¸ 
            """,
            expected_output="ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ì„ ë° ìµœì í™” ê²°ê³¼",
            agent=self.text_analyzer_agent
        )

    def _create_enhanced_image_analysis_task(self, extracted_image_data: Dict, binding_results: List[Dict]) -> Task:
        """ê°•í™”ëœ ì´ë¯¸ì§€ ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±"""
        return Task(
            description=f"""
            ì¶”ì¶œëœ ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì í™”ëœ ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ ìƒì„±í•˜ì„¸ìš”.
            
            **ì¶”ì¶œëœ ë°ì´í„°:**
            - ì´ ì´ë¯¸ì§€ ìˆ˜: {extracted_image_data['total_images']}ê°œ
            - í…œí”Œë¦¿ ìˆ˜: {len(extracted_image_data['template_images'])}ê°œ
            - BindingAgent ê²°ê³¼: {len(binding_results)}ê°œ
            
            **í…œí”Œë¦¿ë³„ ì´ë¯¸ì§€ ë¶„ë°°:**
            {self._format_images_for_analysis(extracted_image_data['template_images'])}
            
            **ì´ë¯¸ì§€ ì†ŒìŠ¤ ì •ë³´:**
            {self._format_image_sources(extracted_image_data['image_sources'])}
            
            **ë¶„ì„ ìš”êµ¬ì‚¬í•­:**
            1. ì´ë¯¸ì§€ URL ìœ íš¨ì„± ê²€ì¦
            2. í…œí”Œë¦¿ë³„ ì´ë¯¸ì§€ ë¶„ë°° ê· í˜•ë„ í‰ê°€
            3. ì´ë¯¸ì§€ í’ˆì§ˆ ë° ì í•©ì„± í™•ì¸
            4. ì‹œê°ì  ì¼ê´€ì„± ê²€í† 
            5. ë ˆì´ì•„ì›ƒ ìµœì í™” ì œì•ˆ
            
            **ì¶œë ¥ í˜•ì‹:**
            í…œí”Œë¦¿ë³„ë¡œ ë‹¤ìŒ ì •ë³´ í¬í•¨:
            - ì´ë¯¸ì§€ ëª©ë¡ ë° ì„¤ëª…
            - ë°°ì¹˜ ê¶Œì¥ì‚¬í•­
            - ì‹œê°ì  íš¨ê³¼ ì˜ˆì¸¡
            """,
            expected_output="ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„° ê¸°ë°˜ ë°°ì¹˜ ë¶„ì„ ë° ìµœì í™” ê²°ê³¼",
            agent=self.image_analyzer_agent
        )

    def _create_enhanced_coordination_task(self, extracted_text_data: Dict, extracted_image_data: Dict) -> Task:
        """ê°•í™”ëœ í†µí•© ì¡°ìœ¨ íƒœìŠ¤í¬ ìƒì„±"""
        return Task(
            description=f"""
            ì‹¤ì œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ì™„ë²½í•œ ë§¤ê±°ì§„ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ì„¸ìš”.
            
            **í…ìŠ¤íŠ¸ ë°ì´í„° ìš”ì•½:**
            - ì„¹ì…˜ ìˆ˜: {len(extracted_text_data['sections'])}ê°œ
            - ì´ ê¸¸ì´: {extracted_text_data['total_content_length']} ë¬¸ì
            
            **ì´ë¯¸ì§€ ë°ì´í„° ìš”ì•½:**
            - ì´ ì´ë¯¸ì§€: {extracted_image_data['total_images']}ê°œ
            - í…œí”Œë¦¿ ìˆ˜: {len(extracted_image_data['template_images'])}ê°œ
            
            **í†µí•© ìš”êµ¬ì‚¬í•­:**
            1. í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ì˜ ì™„ë²½í•œ ë§¤ì¹­
            2. ê° ì„¹ì…˜ë³„ ìµœì  í…œí”Œë¦¿ ì„ íƒ
            3. ì½˜í…ì¸  í’ˆì§ˆ ë³´ì¥
            4. ì‹œê°ì  ì¼ê´€ì„± ìœ ì§€
            5. JSX êµ¬í˜„ì„ ìœ„í•œ ì™„ì „í•œ ìŠ¤í™ ìƒì„±
            
            **ìµœì¢… ì¶œë ¥ êµ¬ì¡°:**
            ```
            {
                "selected_templates": ["í…œí”Œë¦¿ ëª©ë¡"],
                "content_sections": [
                    {
                        "template": "í…œí”Œë¦¿ëª…",
                        "title": "ì‹¤ì œ ì œëª©",
                        "subtitle": "ì‹¤ì œ ë¶€ì œëª©", 
                        "body": "ì‹¤ì œ ë³¸ë¬¸ ë‚´ìš©",
                        "tagline": "íƒœê·¸ë¼ì¸",
                        "images": ["ì‹¤ì œ ì´ë¯¸ì§€ URLë“¤"],
                        "metadata": {
                            "content_quality": "í’ˆì§ˆ ì ìˆ˜",
                            "image_count": "ì´ë¯¸ì§€ ìˆ˜",
                            "source": "ë°ì´í„° ì†ŒìŠ¤"
                        }
                    }
                ],
                "integration_metadata": {
                    "total_sections": "ì„¹ì…˜ ìˆ˜",
                    "integration_quality_score": "í’ˆì§ˆ ì ìˆ˜"
                }
            }
            ```
            
            ì´ì „ íƒœìŠ¤í¬ë“¤ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ì˜ ê³ í’ˆì§ˆ ë§¤ê±°ì§„ êµ¬ì¡°ë¥¼ ì™„ì„±í•˜ì„¸ìš”.
            """,
            expected_output="ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì™„ì„±ëœ ë§¤ê±°ì§„ êµ¬ì¡° JSON",
            agent=self.crew_agent,
            context=[self._create_enhanced_text_analysis_task(extracted_text_data, []), 
                    self._create_enhanced_image_analysis_task(extracted_image_data, [])]
        )

    async def _process_enhanced_crew_result_async(self, crew_result, extracted_text_data: Dict, 
                                                extracted_image_data: Dict, org_results: List[Dict], 
                                                binding_results: List[Dict]) -> Dict:
        """ê°•í™”ëœ Crew ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬ (ë¹„ë™ê¸°)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._process_enhanced_crew_result, crew_result, extracted_text_data, 
            extracted_image_data, org_results, binding_results
        )

    def _process_enhanced_crew_result(self, crew_result, extracted_text_data: Dict, 
                                    extracted_image_data: Dict, org_results: List[Dict], 
                                    binding_results: List[Dict]) -> Dict:
        """ê°•í™”ëœ Crew ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬"""
        try:
            # Crew ê²°ê³¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            if hasattr(crew_result, 'raw') and crew_result.raw:
                result_text = crew_result.raw
            else:
                result_text = str(crew_result)
            
            # JSON íŒ¨í„´ ì°¾ê¸° ë° íŒŒì‹±
            parsed_data = self._extract_json_from_text(result_text)
            
            # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ êµ¬ì¡° ìƒì„±
            if not parsed_data.get('content_sections') or len(parsed_data.get('content_sections', [])) == 0:
                parsed_data = self._create_enhanced_structure(extracted_text_data, extracted_image_data, org_results, binding_results)
            else:
                # ê¸°ì¡´ íŒŒì‹±ëœ ë°ì´í„°ì— ì‹¤ì œ ì´ë¯¸ì§€ ì¶”ê°€
                parsed_data = self._enhance_parsed_data_with_real_images(parsed_data, extracted_image_data)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            parsed_data['integration_metadata'] = {
                "total_sections": len(parsed_data.get('content_sections', [])),
                "total_templates": len(set(section.get("template", f"Section{i+1:02d}.jsx") for i, section in enumerate(parsed_data.get('content_sections', [])))),
                "agent_enhanced": True,
                "org_results_utilized": len(org_results),
                "binding_results_utilized": len(binding_results),
                "integration_quality_score": self._calculate_enhanced_quality_score(parsed_data.get('content_sections', []), len(org_results), len(binding_results)),
                "crewai_enhanced": True,
                "async_processed": True,
                "data_source": "real_extracted_data",
                "real_content_used": True,
                "real_images_used": extracted_image_data['total_images'] > 0
            }
            
            return parsed_data
            
        except Exception as e:
            print(f"âš ï¸ ê°•í™”ëœ Crew ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_enhanced_structure(extracted_text_data, extracted_image_data, org_results, binding_results)

    def _create_enhanced_structure(self, extracted_text_data: Dict, extracted_image_data: Dict, 
                                 org_results: List[Dict], binding_results: List[Dict]) -> Dict:
        """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ê°•í™”ëœ êµ¬ì¡° ìƒì„±"""
        content_sections = []
        
        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì„¹ì…˜ í™œìš©
        for i, section in enumerate(extracted_text_data['sections']):
            template = section.get('template', f'Section{i+1:02d}.jsx')
            
            # í•´ë‹¹ í…œí”Œë¦¿ì˜ ì‹¤ì œ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
            template_images = extracted_image_data['template_images'].get(template, [])
            
            # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ í…œí”Œë¦¿ì˜ ì´ë¯¸ì§€ ì‚¬ìš©
            if not template_images:
                for temp_name, temp_images in extracted_image_data['template_images'].items():
                    if temp_images:
                        template_images = temp_images[:2]  # ìµœëŒ€ 2ê°œ
                        break
            
            enhanced_section = {
                'template': template,
                'title': section.get('title', 'ì—¬í–‰ ì´ì•¼ê¸°'),
                'subtitle': section.get('subtitle', 'íŠ¹ë³„í•œ ìˆœê°„ë“¤'),
                'body': section.get('body', 'ì—¬í–‰ì˜ íŠ¹ë³„í•œ ìˆœê°„ë“¤ì„ ë‹´ì€ ì´ì•¼ê¸°ì…ë‹ˆë‹¤.'),
                'tagline': section.get('tagline', 'TRAVEL & CULTURE'),
                'images': template_images,
                'metadata': {
                    "agent_enhanced": True,
                    "real_content": True,
                    "real_images": len(template_images) > 0,
                    "content_source": section.get('layout_source', 'extracted'),
                    "content_length": len(section.get('body', '')),
                    "image_count": len(template_images),
                    "quality_verified": True
                }
            }
            content_sections.append(enhanced_section)
        
        # ìµœì†Œ 1ê°œ ì„¹ì…˜ ë³´ì¥
        if not content_sections:
            # ì‹¤ì œ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            fallback_images = []
            for template_images in extracted_image_data['template_images'].values():
                fallback_images.extend(template_images[:2])
                if len(fallback_images) >= 2:
                    break
            
            content_sections = [{
                'template': 'Section01.jsx',
                'title': 'ì—¬í–‰ ë§¤ê±°ì§„',
                'subtitle': 'íŠ¹ë³„í•œ ì´ì•¼ê¸°',
                'body': 'ì—¬í–‰ì˜ íŠ¹ë³„í•œ ìˆœê°„ë“¤ì„ ë‹´ì€ ë§¤ê±°ì§„ì…ë‹ˆë‹¤. ì•„ë¦„ë‹¤ìš´ í’ê²½ê³¼ í•¨ê»˜í•˜ëŠ” íŠ¹ë³„í•œ ê²½í—˜ì„ ê³µìœ í•©ë‹ˆë‹¤.',
                'tagline': 'TRAVEL & CULTURE',
                'images': fallback_images,
                'metadata': {
                    "agent_enhanced": True,
                    "fallback_content": True,
                    "real_images": len(fallback_images) > 0,
                    "image_count": len(fallback_images)
                }
            }]
        
        return {
            "selected_templates": [section.get("template", f"Section{i+1:02d}.jsx") for i, section in enumerate(content_sections)],
            "content_sections": content_sections
        }

    def _enhance_parsed_data_with_real_images(self, parsed_data: Dict, extracted_image_data: Dict) -> Dict:
        """íŒŒì‹±ëœ ë°ì´í„°ì— ì‹¤ì œ ì´ë¯¸ì§€ ì¶”ê°€"""
        if 'content_sections' in parsed_data:
            for section in parsed_data['content_sections']:
                template = section.get('template', 'Section01.jsx')
                
                # ì‹¤ì œ ì´ë¯¸ì§€ë¡œ êµì²´
                real_images = extracted_image_data['template_images'].get(template, [])
                if real_images:
                    section['images'] = real_images
                elif extracted_image_data['total_images'] > 0:
                    # ë‹¤ë¥¸ í…œí”Œë¦¿ì˜ ì´ë¯¸ì§€ ì‚¬ìš©
                    for temp_images in extracted_image_data['template_images'].values():
                        if temp_images:
                            section['images'] = temp_images[:2]
                            break
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                if 'metadata' not in section:
                    section['metadata'] = {}
                section['metadata'].update({
                    "real_images_used": len(section.get('images', [])) > 0,
                    "image_count": len(section.get('images', []))
                })
        
        return parsed_data

    # ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    def _split_content_into_sections(self, content: str) -> List[str]:
        """ì½˜í…ì¸ ë¥¼ ì„¹ì…˜ë³„ë¡œ ë¶„í• """
        # í—¤ë”ë‚˜ êµ¬ë¶„ì ê¸°ë°˜ ë¶„í• 
        sections = []
        
        # === íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
        if '===' in content:
            parts = content.split('===')
            for part in parts:
                clean_part = part.strip()
                if len(clean_part) > 100:
                    sections.append(clean_part)
        
        # ë¬¸ë‹¨ ê¸°ë°˜ ë¶„í• 
        elif '\n\n' in content:
            paragraphs = content.split('\n\n')
            current_section = ""
            for paragraph in paragraphs:
                if len(current_section + paragraph) > 800:
                    if current_section:
                        sections.append(current_section.strip())
                    current_section = paragraph
                else:
                    current_section += "\n\n" + paragraph
            
            if current_section:
                sections.append(current_section.strip())
        
        # ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì„¹ì…˜ìœ¼ë¡œ
        else:
            sections = [content]
        
        return [s for s in sections if len(s) > 50]

    def _extract_title_from_content(self, content: str) -> str:
        """ì½˜í…ì¸ ì—ì„œ ì œëª© ì¶”ì¶œ"""
        lines = content.split('\n')
        for line in lines[:3]:  # ì²˜ìŒ 3ì¤„ì—ì„œ ì°¾ê¸°
            line = line.strip()
            if line and len(line) < 100:
                # ì œëª© íŒ¨í„´ ì •ë¦¬
                title = re.sub(r'^[#\*\-\s]+', '', line)
                title = re.sub(r'[#\*\-\s]+$', '', title)
                if len(title) > 5:
                    return title[:50]
        
        return "ì—¬í–‰ ì´ì•¼ê¸°"

    def _extract_subtitle_from_content(self, content: str) -> str:
        """ì½˜í…ì¸ ì—ì„œ ë¶€ì œëª© ì¶”ì¶œ"""
        lines = content.split('\n')
        for i, line in enumerate(lines[1:4]):  # 2-4ë²ˆì§¸ ì¤„ì—ì„œ ì°¾ê¸°
            line = line.strip()
            if line and len(line) < 80 and len(line) > 5:
                subtitle = re.sub(r'^[#\*\-\s]+', '', line)
                subtitle = re.sub(r'[#\*\-\s]+$', '', subtitle)
                if len(subtitle) > 3:
                    return subtitle[:40]
        
        return "íŠ¹ë³„í•œ ìˆœê°„ë“¤"

    def _clean_content(self, content: str) -> str:
        """ì½˜í…ì¸  ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
        cleaned = re.sub(r'^[#\*\-\s]+', '', content, flags=re.MULTILINE)
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        cleaned = re.sub(r'^\s*$\n', '', cleaned, flags=re.MULTILINE)
        
        return cleaned.strip()

    def _extract_structured_content(self, text: str) -> List[Dict]:
        """êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ì¶”ì¶œ"""
        sections = []
        
        # ì œëª© íŒ¨í„´ ì°¾ê¸°
        title_patterns = [
            r'ì œëª©[:\s]*([^\n]+)',
            r'title[:\s]*([^\n]+)',
            r'## ([^\n]+)',
            r'# ([^\n]+)'
        ]
        
        for pattern in title_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                title = match.group(1).strip()
                if len(title) > 3:
                    section = {
                        "template": f"Section{len(sections)+1:02d}.jsx",
                        "title": title[:50],
                        "subtitle": "ì—¬í–‰ì˜ íŠ¹ë³„í•œ ìˆœê°„",
                        "body": f"{title}ì— ëŒ€í•œ ìì„¸í•œ ì´ì•¼ê¸°ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.",
                        "tagline": "TRAVEL & CULTURE",
                        "layout_source": "org_agent"
                    }
                    sections.append(section)
                    
                    if len(sections) >= 3:  # ìµœëŒ€ 3ê°œ
                        break
            
            if sections:
                break
        
        return sections

    def _extract_template_from_binding_result(self, result: Dict) -> str:
        """BindingAgent ê²°ê³¼ì—ì„œ í…œí”Œë¦¿ëª… ì¶”ì¶œ"""
        task_desc = result.get('task_description', '')
        
        # í…œí”Œë¦¿ëª… íŒ¨í„´ ì°¾ê¸°
        template_match = re.search(r'Section\d+\.jsx', task_desc)
        if template_match:
            return template_match.group()
        
        return "Section01.jsx"

    def _extract_image_source_info(self, result: Dict, url: str) -> Dict:
        """ì´ë¯¸ì§€ ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        return {
            "url": url,
            "template": self._extract_template_from_binding_result(result),
            "source": "binding_agent",
            "timestamp": result.get('timestamp', ''),
            "quality_verified": True
        }

    def _format_sections_for_analysis(self, sections: List[Dict]) -> str:
        """ë¶„ì„ìš© ì„¹ì…˜ í¬ë§·íŒ…"""
        formatted = []
        for i, section in enumerate(sections[:3]):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
            formatted.append(f"""
ì„¹ì…˜ {i+1}:
- í…œí”Œë¦¿: {section.get('template', 'N/A')}
- ì œëª©: {section.get('title', 'N/A')}
- ë¶€ì œëª©: {section.get('subtitle', 'N/A')}
- ë³¸ë¬¸ ê¸¸ì´: {len(section.get('body', ''))} ë¬¸ì
- ì†ŒìŠ¤: {section.get('layout_source', 'N/A')}
""")
        
        return "\n".join(formatted)

    def _format_images_for_analysis(self, template_images: Dict) -> str:
        """ë¶„ì„ìš© ì´ë¯¸ì§€ í¬ë§·íŒ…"""
        formatted = []
        for template, images in template_images.items():
            formatted.append(f"""
{template}: {len(images)}ê°œ ì´ë¯¸ì§€
{chr(10).join([f"  - {img}" for img in images[:2]])}
""")
        
        return "\n".join(formatted)

    def _format_image_sources(self, image_sources: List[Dict]) -> str:
        """ì´ë¯¸ì§€ ì†ŒìŠ¤ ì •ë³´ í¬ë§·íŒ…"""
        if not image_sources:
            return "ì´ë¯¸ì§€ ì†ŒìŠ¤ ì •ë³´ ì—†ìŒ"
        
        formatted = []
        for source in image_sources[:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
            formatted.append(f"- {source.get('template', 'N/A')}: {source.get('url', 'N/A')}")
        
        return "\n".join(formatted)

    def _extract_json_from_text(self, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ"""
        # JSON íŒ¨í„´ ì°¾ê¸°
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        json_matches = re.findall(json_pattern, text, re.DOTALL)
        
        parsed_data = {}
        for match in json_matches:
            try:
                if len(match) < 10000:  # í¬ê¸° ì œí•œ
                    data = json.loads(match)
                    if isinstance(data, dict):
                        parsed_data.update(data)
            except json.JSONDecodeError:
                continue
        
        return parsed_data

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (ë³€ê²½ ì—†ìŒ)
    async def _log_coordination_result_async(self, final_result: Dict, text_mapping: Dict, 
                                           image_distribution: Dict, org_results: List[Dict], 
                                           binding_results: List[Dict]) -> None:
        """ì¡°ìœ¨ ê²°ê³¼ ë¡œê¹… (ë¹„ë™ê¸°)"""
        await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: self.logger.log_agent_real_output(
                agent_name="CoordinatorAgent",
                agent_role="ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ì",
                task_description=f"CrewAI ê¸°ë°˜ ë¹„ë™ê¸° ì‹¤ì œ ë°ì´í„° í™œìš©ìœ¼ë¡œ {len(final_result.get('content_sections', []))}ê°œ ì„¹ì…˜ ìƒì„±",
                final_answer=f"ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì™„ë£Œ: {len(final_result.get('content_sections', []))}ê°œ ì„¹ì…˜, í’ˆì§ˆ ì ìˆ˜: {final_result.get('integration_metadata', {}).get('integration_quality_score', 0):.2f}",
                reasoning_process=f"CrewAI ê¸°ë°˜ ë¹„ë™ê¸° ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ ë° í™œìš©ìœ¼ë¡œ OrgAgent {len(org_results)}ê°œ, BindingAgent {len(binding_results)}ê°œ ê²°ê³¼ í†µí•©",
                execution_steps=[
                    "CrewAI ì—ì´ì „íŠ¸ ìƒì„±",
                    "ë¹„ë™ê¸° ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ",
                    "ê°•í™”ëœ í…ìŠ¤íŠ¸ ë¶„ì„ íƒœìŠ¤í¬ ì‹¤í–‰",
                    "ê°•í™”ëœ ì´ë¯¸ì§€ ë¶„ì„ íƒœìŠ¤í¬ ì‹¤í–‰", 
                    "ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í†µí•© ì¡°ìœ¨",
                    "í’ˆì§ˆ ê²€ì¦ ë° ìµœì í™”"
                ],
                raw_input={"text_mapping": text_mapping, "image_distribution": image_distribution},
                raw_output=final_result,
                performance_metrics={
                    "async_processing": True,
                    "real_data_used": True,
                    "crew_execution_time": "optimized",
                    "total_sections": len(final_result.get('content_sections', [])),
                    "quality_score": final_result.get('integration_metadata', {}).get('integration_quality_score', 0),
                    "org_results_utilized": len(org_results),
                    "binding_results_utilized": len(binding_results),
                    "real_images_count": sum(len(section.get('images', [])) for section in final_result.get('content_sections', [])),
                    "content_enhancement": True
                }
            )
        )

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€
    async def _get_enhanced_previous_results_async(self) -> List[Dict]:
        """ê°•í™”ëœ ì´ì „ ê²°ê³¼ ìˆ˜ì§‘ (ë¹„ë™ê¸°)"""
        try:
            # ë³‘ë ¬ë¡œ ê²°ê³¼ ìˆ˜ì§‘
            basic_results_task = asyncio.get_event_loop().run_in_executor(
                None, lambda: self.logger.get_all_previous_results("CoordinatorAgent")
            )
            file_results_task = self._load_results_from_file_async()
            
            basic_results, file_results = await asyncio.gather(
                basic_results_task, file_results_task, return_exceptions=True
            )
            
            # ì˜ˆì™¸ ì²˜ë¦¬
            if isinstance(basic_results, Exception):
                print(f"âš ï¸ ê¸°ë³¸ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {basic_results}")
                basic_results = []
            
            if isinstance(file_results, Exception):
                print(f"âš ï¸ íŒŒì¼ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {file_results}")
                file_results = []
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            results = []
            results.extend(basic_results)
            results.extend(file_results)
            
            # ì¤‘ë³µ ì œê±° (ë¹„ë™ê¸°)
            unique_results = await asyncio.get_event_loop().run_in_executor(
                None, self._deduplicate_results, results
            )
            
            return unique_results
            
        except Exception as e:
            print(f"âš ï¸ ë¹„ë™ê¸° ì´ì „ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    async def _load_results_from_file_async(self) -> List[Dict]:
        """íŒŒì¼ì—ì„œ ì§ì ‘ ê²°ê³¼ ë¡œë“œ (ë¹„ë™ê¸°)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._load_results_from_file
        )

    def _load_results_from_file(self) -> List[Dict]:
        """íŒŒì¼ì—ì„œ ì§ì ‘ ê²°ê³¼ ë¡œë“œ (ë™ê¸° ë²„ì „ - í˜¸í™˜ì„± ìœ ì§€)"""
        results = []
        
        try:
            # latest_outputs.jsonì—ì„œ ë¡œë“œ
            if os.path.exists('./agent_outputs/latest_outputs.json'):
                with open('./agent_outputs/latest_outputs.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    latest_outputs = data.get('latest_outputs', [])
                    results.extend(latest_outputs)
            
            # ì„¸ì…˜ íŒŒì¼ì—ì„œ ë¡œë“œ
            session_files = []
            if os.path.exists('./agent_outputs/outputs'):
                for session_dir in os.listdir('./agent_outputs/outputs'):
                    session_path = os.path.join('./agent_outputs/outputs', session_dir, 'agent_outputs.json')
                    if os.path.exists(session_path):
                        session_files.append(session_path)
            
            # ìµœì‹  ì„¸ì…˜ íŒŒì¼ ìš°ì„  ì²˜ë¦¬
            session_files.sort(reverse=True)
            for session_file in session_files[:3]:  # ìµœê·¼ 3ê°œ ì„¸ì…˜ë§Œ
                try:
                    with open(session_file, 'r', encoding='utf-8') as f:
                        session_data = json.load(f)
                        if 'outputs' in session_data:
                            results.extend(session_data['outputs'])
                except Exception as e:
                    print(f"âš ï¸ ì„¸ì…˜ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {session_file}: {e}")
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return results

    def _filter_agent_results(self, results: List[Dict], agent_type: str) -> List[Dict]:
        """íŠ¹ì • ì—ì´ì „íŠ¸ ê²°ê³¼ í•„í„°ë§"""
        filtered = []
        for result in results:
            agent_name = result.get('agent_name', '')
            if agent_type in agent_name:
                filtered.append(result)
        return filtered

    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """ê²°ê³¼ ì¤‘ë³µ ì œê±°"""
        seen_ids = set()
        unique_results = []
        
        for result in results:
            result_id = result.get('output_id') or result.get('timestamp', '')
            if result_id and result_id not in seen_ids:
                seen_ids.add(result_id)
                unique_results.append(result)
        
        return unique_results

    def _calculate_enhanced_quality_score(self, content_sections: List[Dict], 
                                        org_count: int, binding_count: int) -> float:
        """ê°•í™”ëœ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not content_sections:
            return 0.0
        
        quality_score = 0.0
        
        # 1. ì„¹ì…˜ í’ˆì§ˆ (60%)
        section_quality = 0.0
        for section in content_sections:
            section_score = 0.0
            
            if section.get("title") and len(section.get("title", "")) > 3:
                section_score += 0.25
            if section.get("subtitle") and len(section.get("subtitle", "")) > 3:
                section_score += 0.15
            if section.get("body") and len(section.get("body", "")) > 50:
                section_score += 0.35
            if section.get("images") and len(section.get("images", [])) > 0:
                section_score += 0.25
            
            section_quality += min(section_score, 1.0)
        
        quality_score += (section_quality / len(content_sections)) * 0.6
        
        # 2. ì—ì´ì „íŠ¸ í™œìš©ë„ (25%)
        agent_score = 0.0
        if org_count > 0:
            agent_score += 0.5
        if binding_count > 0:
            agent_score += 0.5
        
        quality_score += agent_score * 0.25
        
        # 3. ì‹¤ì œ ë°ì´í„° í™œìš©ë„ (15%)
        real_data_score = 0.0
        real_content_sections = sum(1 for section in content_sections 
                                  if section.get('metadata', {}).get('real_content', False))
        real_image_sections = sum(1 for section in content_sections 
                                if section.get('metadata', {}).get('real_images', False))
        
        if real_content_sections > 0:
            real_data_score += 0.5
        if real_image_sections > 0:
            real_data_score += 0.5
        
        quality_score += real_data_score * 0.15
        
        return min(quality_score, 1.0)

    # ë™ê¸° ë²„ì „ ë©”ì„œë“œ ìœ ì§€ (í˜¸í™˜ì„± ë³´ì¥)
    def coordinate_magazine_creation_sync(self, text_mapping: Dict, image_distribution: Dict) -> Dict:
        """ë§¤ê±°ì§„ êµ¬ì¡° í†µí•© ì¡°ìœ¨ (ë™ê¸° ë²„ì „ - í˜¸í™˜ì„± ìœ ì§€)"""
        return asyncio.run(self.coordinate_magazine_creation(text_mapping, image_distribution))

    def _get_enhanced_previous_results(self) -> List[Dict]:
        """ê°•í™”ëœ ì´ì „ ê²°ê³¼ ìˆ˜ì§‘ (ë™ê¸° ë²„ì „ - í˜¸í™˜ì„± ìœ ì§€)"""
        return asyncio.run(self._get_enhanced_previous_results_async())
