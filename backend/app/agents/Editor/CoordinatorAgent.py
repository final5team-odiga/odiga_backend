import asyncio
import os
from typing import Dict, List
from crewai import Agent, Task, Crew, Process
from custom_llm import get_azure_llm
from utils.agent_decision_logger import get_agent_logger
import json
import re

class CoordinatorAgent:
    """ÌÜµÌï© Ï°∞Ïú®Ïûê (CrewAI Í∏∞Î∞ò Í∞ïÌôîÎêú Îç∞Ïù¥ÌÑ∞ Ï†ëÍ∑º Î∞è JSON ÌååÏã±)"""
    
    def __init__(self):
        self.llm = get_azure_llm()
        self.logger = get_agent_logger()
        self.crew_agent = self._create_crew_agent()
        self.text_analyzer_agent = self._create_text_analyzer_agent()
        self.image_analyzer_agent = self._create_image_analyzer_agent()

    def _create_crew_agent(self):
        """Î©îÏù∏ Ï°∞Ïú® ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ±"""
        return Agent(
            role="Îß§Í±∞ÏßÑ Íµ¨Ï°∞ ÌÜµÌï© Ï°∞Ïú®Ïûê Î∞è ÏµúÏ¢Ö ÌíàÏßà Î≥¥Ï¶ù Ï†ÑÎ¨∏Í∞Ä",
            goal="OrgAgentÏùò ÏÉÅÏÑ∏ Î†àÏù¥ÏïÑÏõÉ Íµ¨Ï°∞ÏôÄ BindingAgentÏùò Ï†ïÎ∞Ä Ïù¥ÎØ∏ÏßÄ Î∞∞ÏπòÎ•º ÌÜµÌï©ÌïòÏó¨ ÏôÑÎ≤ΩÌïú Îß§Í±∞ÏßÑ Íµ¨Ï°∞Î•º ÏÉùÏÑ±ÌïòÍ≥†, ÌÖçÏä§Ìä∏-Ïù¥ÎØ∏ÏßÄ Ï†ïÌï©ÏÑ±Í≥º ÎèÖÏûê Í≤ΩÌóòÏùÑ ÏµúÏ¢Ö Í≤ÄÏ¶ùÌïòÏó¨ JSX Íµ¨ÌòÑÏóê ÌïÑÏöîÌïú ÏôÑÏ†ÑÌïú Íµ¨Ï°∞ Îç∞Ïù¥ÌÑ∞Î•º Ï†úÍ≥µ",
            backstory="""ÎãπÏã†ÏùÄ 25ÎÖÑÍ∞Ñ ÏÑ∏Í≥Ñ ÏµúÍ≥† ÏàòÏ§ÄÏùò Ï∂úÌåêÏÇ¨ÏóêÏÑú Îß§Í±∞ÏßÑ Íµ¨Ï°∞ ÌÜµÌï© Î∞è ÌíàÏßà Î≥¥Ï¶ù Ï±ÖÏûÑÏûêÎ°ú ÌôúÎèôÌï¥Ïò® Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Cond√© Nast, Hearst Corporation, Time Inc.ÏóêÏÑú ÏàòÎ∞± Í∞úÏùò Îß§Í±∞ÏßÑ ÌîÑÎ°úÏ†ùÌä∏Î•º ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï°∞Ïú®ÌñàÏäµÎãàÎã§.

**Ï†ÑÎ¨∏ Í≤ΩÎ†•:**
- Ï∂úÌåêÌïô Î∞è Íµ¨Ï°∞ ÏÑ§Í≥Ñ ÏÑùÏÇ¨ ÌïôÏúÑ Î≥¥Ïú†
- PMP(Project Management Professional) Ïù∏Ï¶ù
- Îß§Í±∞ÏßÑ Íµ¨Ï°∞ ÌÜµÌï© Î∞è ÌíàÏßà Í¥ÄÎ¶¨ Ï†ÑÎ¨∏Í∞Ä
- ÌÖçÏä§Ìä∏-Ïù¥ÎØ∏ÏßÄ Ï†ïÌï©ÏÑ± Í≤ÄÏ¶ù ÏãúÏä§ÌÖú Í∞úÎ∞ú Í≤ΩÌóò
- ÎèÖÏûê Í≤ΩÌóò(UX) Î∞è Ï†ëÍ∑ºÏÑ± ÏµúÏ†ÅÌôî Ï†ÑÎ¨∏ÏÑ±

**Ï°∞Ïú® Ï≤†Ìïô:**
"ÏôÑÎ≤ΩÌïú Îß§Í±∞ÏßÑÏùÄ Î™®Îì† Íµ¨Ï°∞Ï†Å ÏöîÏÜåÍ∞Ä ÎèÖÏûêÏùò Ïù∏ÏßÄ Í≥ºÏ†ïÍ≥º ÏôÑÎ≤ΩÌûà Ï°∞ÌôîÎ•º Ïù¥Î£®Îäî ÌÜµÌï©Ï≤¥ÏûÖÎãàÎã§. ÎÇòÎäî ÌÖçÏä§Ìä∏ÏôÄ Ïù¥ÎØ∏ÏßÄÏùò Î™®Îì† Î∞∞ÏπòÍ∞Ä ÎèÖÏûêÏóêÍ≤å ÏûêÏó∞Ïä§ÎüΩÍ≥† ÏßÅÍ¥ÄÏ†ÅÏúºÎ°ú Ïù∏ÏãùÎêòÎèÑÎ°ù Íµ¨Ï°∞Ï†Å ÏôÑÏÑ±ÎèÑÎ•º Î≥¥Ïû•ÌïòÎ©∞, Ïù¥Î•º ÌÜµÌï¥ ÏµúÍ≥† ÏàòÏ§ÄÏùò ÎèÖÏûê Í≤ΩÌóòÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§."

**Ï∂úÎ†• Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞:**
- ÏôÑÏÑ±Îêú Îß§Í±∞ÏßÑ Ï†ÑÏ≤¥ Íµ¨Ï°∞ÎèÑ
- ÌÖçÏä§Ìä∏-Ïù¥ÎØ∏ÏßÄ Ï†ïÌï©ÏÑ± Í≤ÄÏ¶ù ÏôÑÎ£å Î≥¥Í≥†ÏÑú
- JSX Íµ¨ÌòÑÏö© ÏÉÅÏÑ∏ Î†àÏù¥ÏïÑÏõÉ Ïä§Ìéô Î∞è Ï¢åÌëú Îç∞Ïù¥ÌÑ∞
- ÎèÖÏûê Í≤ΩÌóò ÏµúÏ†ÅÌôî Í∞ÄÏù¥ÎìúÎùºÏù∏
- Î∞òÏùëÌòï ÎîîÏûêÏù∏ Íµ¨Ï°∞ Ï†ïÏùòÏÑú
- Ï†ëÍ∑ºÏÑ± Î∞è ÌíàÏßà Î≥¥Ï¶ù Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏""",
            verbose=True,
            llm=self.llm,
            allow_delegation=False
        )

    def _create_text_analyzer_agent(self):
        """ÌÖçÏä§Ìä∏ Î∂ÑÏÑù Ï†ÑÎ¨∏ ÏóêÏù¥Ï†ÑÌä∏"""
        return Agent(
            role="ÌÖçÏä§Ìä∏ Îß§Ìïë Î∂ÑÏÑù Ï†ÑÎ¨∏Í∞Ä",
            goal="OrgAgentÏùò ÌÖçÏä§Ìä∏ Îß§Ìïë Í≤∞Í≥ºÎ•º Ï†ïÎ∞Ä Î∂ÑÏÑùÌïòÏó¨ Íµ¨Ï°∞Ï†Å ÏôÑÏÑ±ÎèÑÎ•º Í≤ÄÏ¶ùÌïòÍ≥† ÏµúÏ†ÅÌôîÎêú ÌÖçÏä§Ìä∏ ÏÑπÏÖòÏùÑ ÏÉùÏÑ±",
            backstory="""ÎãπÏã†ÏùÄ 15ÎÖÑÍ∞Ñ Ï∂úÌåêÏóÖÍ≥ÑÏóêÏÑú ÌÖçÏä§Ìä∏ Íµ¨Ï°∞ Î∂ÑÏÑù Î∞è ÏµúÏ†ÅÌôîÎ•º Îã¥ÎãπÌï¥Ïò® Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Î≥µÏû°Ìïú ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÌïµÏã¨ Ï†ïÎ≥¥Î•º Ï∂îÏ∂úÌïòÍ≥† ÎèÖÏûê ÏπúÌôîÏ†ÅÏù∏ Íµ¨Ï°∞Î°ú Ïû¨Íµ¨ÏÑ±ÌïòÎäî Îç∞ ÌÉÅÏõîÌïú Îä•Î†•ÏùÑ Î≥¥Ïú†ÌïòÍ≥† ÏûàÏäµÎãàÎã§.""",
            verbose=True,
            llm=self.llm,
            allow_delegation=False
        )

    def _create_image_analyzer_agent(self):
        """Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Ï†ÑÎ¨∏ ÏóêÏù¥Ï†ÑÌä∏"""
        return Agent(
            role="Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ∞∞ Î∂ÑÏÑù Ï†ÑÎ¨∏Í∞Ä",
            goal="BindingAgentÏùò Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ∞∞ Í≤∞Í≥ºÎ•º Ï†ïÎ∞Ä Î∂ÑÏÑùÌïòÏó¨ ÏãúÍ∞ÅÏ†Å ÏùºÍ¥ÄÏÑ±ÏùÑ Í≤ÄÏ¶ùÌïòÍ≥† ÏµúÏ†ÅÌôîÎêú Ïù¥ÎØ∏ÏßÄ Î∞∞ÏπòÎ•º ÏÉùÏÑ±",
            backstory="""ÎãπÏã†ÏùÄ 12ÎÖÑÍ∞Ñ Îß§Í±∞ÏßÑ Î∞è Ï∂úÌåêÎ¨ºÏùò ÏãúÍ∞ÅÏ†Å ÎîîÏûêÏù∏ÏùÑ Îã¥ÎãπÌï¥Ïò® Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. Ïù¥ÎØ∏ÏßÄÏôÄ ÌÖçÏä§Ìä∏Ïùò Ï°∞ÌôîÎ°úÏö¥ Î∞∞ÏπòÎ•º ÌÜµÌï¥ ÎèÖÏûêÏùò ÏãúÏÑ†ÏùÑ Ìö®Í≥ºÏ†ÅÏúºÎ°ú Ïú†ÎèÑÌïòÎäî Î†àÏù¥ÏïÑÏõÉ ÏÑ§Í≥ÑÏóê Ï†ÑÎ¨∏ÏÑ±ÏùÑ Î≥¥Ïú†ÌïòÍ≥† ÏûàÏäµÎãàÎã§.""",
            verbose=True,
            llm=self.llm,
            allow_delegation=False
        )

    async def coordinate_magazine_creation(self, text_mapping: Dict, image_distribution: Dict) -> Dict:
        """Îß§Í±∞ÏßÑ Íµ¨Ï°∞ ÌÜµÌï© Ï°∞Ïú® (CrewAI Í∏∞Î∞ò Í∞ïÌôîÎêú Îç∞Ïù¥ÌÑ∞ Ï†ëÍ∑º - ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨)"""
        print("CoordinatorAgent: Îß§Í±∞ÏßÑ Íµ¨Ï°∞ ÌÜµÌï© Ï°∞Ïú® ÏãúÏûë (ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨)")
        
        # Í∞ïÌôîÎêú Ïù¥Ï†Ñ ÏóêÏù¥Ï†ÑÌä∏ Í≤∞Í≥º ÏàòÏßë (ÎπÑÎèôÍ∏∞)
        previous_results = await self._get_enhanced_previous_results_async()
        org_results = self._filter_agent_results(previous_results, "OrgAgent")
        binding_results = self._filter_agent_results(previous_results, "BindingAgent")
        content_creator_results = self._filter_agent_results(previous_results, "ContentCreatorV2Agent")
        
        print(f"üìä Í∞ïÌôîÎêú Í≤∞Í≥º ÏàòÏßë: Ï†ÑÏ≤¥ {len(previous_results)}Í∞ú, OrgAgent {len(org_results)}Í∞ú, BindingAgent {len(binding_results)}Í∞ú, ContentCreator {len(content_creator_results)}Í∞ú (ÎπÑÎèôÍ∏∞)")
        
        # Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú Î∞è Í≤ÄÏ¶ù
        extracted_text_data = await self._extract_real_text_data_async(text_mapping, org_results, content_creator_results)
        extracted_image_data = await self._extract_real_image_data_async(image_distribution, binding_results)
        
        # CrewAI Task ÏÉùÏÑ±
        text_analysis_task = self._create_enhanced_text_analysis_task(extracted_text_data, org_results)
        image_analysis_task = self._create_enhanced_image_analysis_task(extracted_image_data, binding_results)
        coordination_task = self._create_enhanced_coordination_task(extracted_text_data, extracted_image_data)
        
        # CrewAI Crew ÏÉùÏÑ± Î∞è ÎπÑÎèôÍ∏∞ Ïã§Ìñâ
        coordination_crew = Crew(
            agents=[self.text_analyzer_agent, self.image_analyzer_agent, self.crew_agent],
            tasks=[text_analysis_task, image_analysis_task, coordination_task],
            process=Process.sequential,
            verbose=True
        )
        
        # Crew ÎπÑÎèôÍ∏∞ Ïã§Ìñâ
        crew_result = await asyncio.get_event_loop().run_in_executor(
            None, coordination_crew.kickoff
        )
        
        # Í≤∞Í≥º Ï≤òÎ¶¨ (ÎπÑÎèôÍ∏∞) - Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÌôúÏö©
        final_result = await self._process_enhanced_crew_result_async(
            crew_result, extracted_text_data, extracted_image_data, org_results, binding_results
        )
        
        # Í≤∞Í≥º Î°úÍπÖ (ÎπÑÎèôÍ∏∞)
        await self._log_coordination_result_async(final_result, text_mapping, image_distribution, org_results, binding_results)
        
        print(f"‚úÖ CoordinatorAgent ÌÜµÌï© ÏôÑÎ£å: {len(final_result.get('content_sections', []))}Í∞ú ÏÑπÏÖò ÏÉùÏÑ± (CrewAI Í∏∞Î∞ò ÎπÑÎèôÍ∏∞)")
        print(f"üìä ÌíàÏßà Ï†êÏàò: {final_result.get('integration_metadata', {}).get('integration_quality_score', 0):.2f}, OrgAgent ÌôúÏö©: {len(org_results)}Í∞ú, BindingAgent ÌôúÏö©: {len(binding_results)}Í∞ú")
        
        return final_result

    async def _extract_real_text_data_async(self, text_mapping: Dict, org_results: List[Dict], content_creator_results: List[Dict]) -> Dict:
        """Ïã§Ï†ú ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú (ÎπÑÎèôÍ∏∞)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._extract_real_text_data, text_mapping, org_results, content_creator_results
        )

    def _extract_real_text_data(self, text_mapping: Dict, org_results: List[Dict], content_creator_results: List[Dict]) -> Dict:
        """Ïã§Ï†ú ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú"""
        extracted_data = {
            "sections": [],
            "total_content_length": 0,
            "source_count": 0
        }
        
        # 1. text_mappingÏóêÏÑú ÏßÅÏ†ë Ï∂îÏ∂ú
        if isinstance(text_mapping, dict) and "text_mapping" in text_mapping:
            for section in text_mapping["text_mapping"]:
                if isinstance(section, dict):
                    extracted_section = {
                        "template": section.get("template", "Section01.jsx"),
                        "title": section.get("title", "Ïó¨Ìñâ Ïù¥ÏïºÍ∏∞"),
                        "subtitle": section.get("subtitle", "ÌäπÎ≥ÑÌïú ÏàúÍ∞ÑÎì§"),
                        "body": section.get("body", ""),
                        "tagline": section.get("tagline", "TRAVEL & CULTURE"),
                        "layout_source": section.get("layout_source", "default")
                    }
                    extracted_data["sections"].append(extracted_section)
                    extracted_data["total_content_length"] += len(extracted_section["body"])
                    extracted_data["source_count"] += 1

        # 2. ContentCreator Í≤∞Í≥ºÏóêÏÑú ÌíçÎ∂ÄÌïú ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú
        for result in content_creator_results:
            final_answer = result.get('final_answer', '')
            if len(final_answer) > 500:  # Ï∂©Î∂ÑÌïú ÏΩòÌÖêÏ∏†Í∞Ä ÏûàÎäî Í≤ΩÏö∞
                # ÏÑπÏÖòÎ≥ÑÎ°ú Î∂ÑÌï†
                sections = self._split_content_into_sections(final_answer)
                for i, section_content in enumerate(sections):
                    if len(section_content) > 100:
                        extracted_section = {
                            "template": f"Section{i+1:02d}.jsx",
                            "title": self._extract_title_from_content(section_content),
                            "subtitle": self._extract_subtitle_from_content(section_content),
                            "body": self._clean_content(section_content),
                            "tagline": "TRAVEL & CULTURE",
                            "layout_source": "content_creator"
                        }
                        extracted_data["sections"].append(extracted_section)
                        extracted_data["total_content_length"] += len(extracted_section["body"])
                        extracted_data["source_count"] += 1

        # 3. OrgAgent Í≤∞Í≥ºÏóêÏÑú Ï∂îÍ∞Ä ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
        for result in org_results:
            final_answer = result.get('final_answer', '')
            if 'Ï†úÎ™©' in final_answer or 'title' in final_answer.lower():
                # Íµ¨Ï°∞ÌôîÎêú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
                structured_content = self._extract_structured_content(final_answer)
                if structured_content:
                    extracted_data["sections"].extend(structured_content)
                    extracted_data["source_count"] += len(structured_content)

        # 4. ÏµúÏÜå Î≥¥Ïû• ÏÑπÏÖò
        if not extracted_data["sections"]:
            extracted_data["sections"] = [{
                "template": "Section01.jsx",
                "title": "Ïó¨Ìñâ Îß§Í±∞ÏßÑ",
                "subtitle": "ÌäπÎ≥ÑÌïú Ïù¥ÏïºÍ∏∞",
                "body": "Ïó¨ÌñâÏùò ÌäπÎ≥ÑÌïú ÏàúÍ∞ÑÎì§ÏùÑ Îã¥ÏùÄ Îß§Í±∞ÏßÑÏûÖÎãàÎã§.",
                "tagline": "TRAVEL & CULTURE",
                "layout_source": "fallback"
            }]
            extracted_data["source_count"] = 1

        return extracted_data

    async def _extract_real_image_data_async(self, image_distribution: Dict, binding_results: List[Dict]) -> Dict:
        """Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú (ÎπÑÎèôÍ∏∞)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._extract_real_image_data, image_distribution, binding_results
        )

    def _extract_real_image_data(self, image_distribution: Dict, binding_results: List[Dict]) -> Dict:
        """Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú"""
        extracted_data = {
            "template_images": {},
            "total_images": 0,
            "image_sources": []
        }
        
        # 1. image_distributionÏóêÏÑú ÏßÅÏ†ë Ï∂îÏ∂ú
        if isinstance(image_distribution, dict) and "image_distribution" in image_distribution:
            for template, images in image_distribution["image_distribution"].items():
                if isinstance(images, list) and images:
                    # Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ URLÎßå ÌïÑÌÑ∞ÎßÅ
                    real_images = [img for img in images if self._is_real_image_url(img)]
                    if real_images:
                        extracted_data["template_images"][template] = real_images
                        extracted_data["total_images"] += len(real_images)

        # 2. BindingAgent Í≤∞Í≥ºÏóêÏÑú Ïù¥ÎØ∏ÏßÄ URL Ï∂îÏ∂ú
        for result in binding_results:
            final_answer = result.get('final_answer', '')
            # Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ URL Ìå®ÌÑ¥ Ï∞æÍ∏∞
            image_urls = re.findall(r'https://[^\s\'"<>]*\.(?:jpg|jpeg|png|gif|webp)', final_answer, re.IGNORECASE)
            
            if image_urls:
                # ÌÖúÌîåÎ¶øÎ≥ÑÎ°ú Î∂ÑÎ∞∞
                template_name = self._extract_template_from_binding_result(result)
                if template_name not in extracted_data["template_images"]:
                    extracted_data["template_images"][template_name] = []
                
                for url in image_urls:
                    if self._is_real_image_url(url) and url not in extracted_data["template_images"][template_name]:
                        extracted_data["template_images"][template_name].append(url)
                        extracted_data["total_images"] += 1
                        
                        # Ïù¥ÎØ∏ÏßÄ ÏÜåÏä§ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
                        source_info = self._extract_image_source_info(result, url)
                        if source_info:
                            extracted_data["image_sources"].append(source_info)

        return extracted_data

    def _is_real_image_url(self, url: str) -> bool:
        """Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ URLÏù∏ÏßÄ ÌôïÏù∏"""
        if not url or not isinstance(url, str):
            return False
        
        # ÏòàÏãú URLÏù¥ÎÇò ÌîåÎ†àÏù¥Ïä§ÌôÄÎçî Ï†úÏô∏
        excluded_patterns = [
            'your-cdn.com',
            'example.com',
            'placeholder',
            'sample',
            'demo'
        ]
        
        for pattern in excluded_patterns:
            if pattern in url.lower():
                return False
        
        # Ïã§Ï†ú ÎèÑÎ©îÏù∏Í≥º Ïù¥ÎØ∏ÏßÄ ÌôïÏû•Ïûê ÌôïÏù∏
        return (url.startswith('https://') and 
                any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']) and
                'blob.core.windows.net' in url)

    def _create_enhanced_text_analysis_task(self, extracted_text_data: Dict, org_results: List[Dict]) -> Task:
        """Í∞ïÌôîÎêú ÌÖçÏä§Ìä∏ Î∂ÑÏÑù ÌÉúÏä§ÌÅ¨ ÏÉùÏÑ±"""
        return Task(
            description=f"""
            Ï∂îÏ∂úÎêú Ïã§Ï†ú ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑùÌïòÏó¨ Í≥†ÌíàÏßà Îß§Í±∞ÏßÑ ÏÑπÏÖòÏùÑ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.
            
            **Ï∂îÏ∂úÎêú Îç∞Ïù¥ÌÑ∞:**
            - ÏÑπÏÖò Ïàò: {len(extracted_text_data['sections'])}Í∞ú
            - Ï¥ù ÏΩòÌÖêÏ∏† Í∏∏Ïù¥: {extracted_text_data['total_content_length']} Î¨∏Ïûê
            - ÏÜåÏä§ Ïàò: {extracted_text_data['source_count']}Í∞ú
            - OrgAgent Í≤∞Í≥º: {len(org_results)}Í∞ú
            
            **Ïã§Ï†ú ÏÑπÏÖò Îç∞Ïù¥ÌÑ∞:**
            {self._format_sections_for_analysis(extracted_text_data['sections'])}
            
            **Î∂ÑÏÑù ÏöîÍµ¨ÏÇ¨Ìï≠:**
            1. Í∞Å ÏÑπÏÖòÏùò ÏΩòÌÖêÏ∏† ÌíàÏßà ÌèâÍ∞Ä
            2. Ï†úÎ™©Í≥º Î∂ÄÏ†úÎ™©Ïùò Îß§Î†•ÎèÑ Í≤ÄÏ¶ù
            3. Î≥∏Î¨∏ ÎÇ¥Ïö©Ïùò ÏôÑÏÑ±ÎèÑ ÌôïÏù∏
            4. Îß§Í±∞ÏßÑ Ïä§ÌÉÄÏùº ÏùºÍ¥ÄÏÑ± Í≤ÄÌÜ†
            5. ÎèÖÏûê ÏπúÌôîÏÑ± ÏµúÏ†ÅÌôî
            
            **Ï∂úÎ†• ÌòïÏãù:**
            Í∞Å ÏÑπÏÖòÎ≥ÑÎ°ú Îã§Ïùå Ï†ïÎ≥¥ Ìè¨Ìï®:
            - ÌíàÏßà Ï†êÏàò (1-10)
            - Í∞úÏÑ† Ï†úÏïàÏÇ¨Ìï≠
            - ÏµúÏ†ÅÌôîÎêú ÏΩòÌÖêÏ∏†
            """,
            expected_output="Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò ÌÖçÏä§Ìä∏ Î∂ÑÏÑù Î∞è ÏµúÏ†ÅÌôî Í≤∞Í≥º",
            agent=self.text_analyzer_agent
        )

    def _create_enhanced_image_analysis_task(self, extracted_image_data: Dict, binding_results: List[Dict]) -> Task:
        """Í∞ïÌôîÎêú Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÌÉúÏä§ÌÅ¨ ÏÉùÏÑ±"""
        return Task(
            description=f"""
            Ï∂îÏ∂úÎêú Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑùÌïòÏó¨ ÏµúÏ†ÅÌôîÎêú Ïù¥ÎØ∏ÏßÄ Î∞∞ÏπòÎ•º ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.
            
            **Ï∂îÏ∂úÎêú Îç∞Ïù¥ÌÑ∞:**
            - Ï¥ù Ïù¥ÎØ∏ÏßÄ Ïàò: {extracted_image_data['total_images']}Í∞ú
            - ÌÖúÌîåÎ¶ø Ïàò: {len(extracted_image_data['template_images'])}Í∞ú
            - BindingAgent Í≤∞Í≥º: {len(binding_results)}Í∞ú
            
            **ÌÖúÌîåÎ¶øÎ≥Ñ Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ∞∞:**
            {self._format_images_for_analysis(extracted_image_data['template_images'])}
            
            **Ïù¥ÎØ∏ÏßÄ ÏÜåÏä§ Ï†ïÎ≥¥:**
            {self._format_image_sources(extracted_image_data['image_sources'])}
            
            **Î∂ÑÏÑù ÏöîÍµ¨ÏÇ¨Ìï≠:**
            1. Ïù¥ÎØ∏ÏßÄ URL Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù
            2. ÌÖúÌîåÎ¶øÎ≥Ñ Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ∞∞ Í∑†ÌòïÎèÑ ÌèâÍ∞Ä
            3. Ïù¥ÎØ∏ÏßÄ ÌíàÏßà Î∞è Ï†ÅÌï©ÏÑ± ÌôïÏù∏
            4. ÏãúÍ∞ÅÏ†Å ÏùºÍ¥ÄÏÑ± Í≤ÄÌÜ†
            5. Î†àÏù¥ÏïÑÏõÉ ÏµúÏ†ÅÌôî Ï†úÏïà
            
            **Ï∂úÎ†• ÌòïÏãù:**
            ÌÖúÌîåÎ¶øÎ≥ÑÎ°ú Îã§Ïùå Ï†ïÎ≥¥ Ìè¨Ìï®:
            - Ïù¥ÎØ∏ÏßÄ Î™©Î°ù Î∞è ÏÑ§Î™Ö
            - Î∞∞Ïπò Í∂åÏû•ÏÇ¨Ìï≠
            - ÏãúÍ∞ÅÏ†Å Ìö®Í≥º ÏòàÏ∏°
            """,
            expected_output="Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Î∞∞Ïπò Î∂ÑÏÑù Î∞è ÏµúÏ†ÅÌôî Í≤∞Í≥º",
            agent=self.image_analyzer_agent
        )

    def _create_enhanced_coordination_task(self, extracted_text_data: Dict, extracted_image_data: Dict) -> Task:
        """Í∞ïÌôîÎêú ÌÜµÌï© Ï°∞Ïú® ÌÉúÏä§ÌÅ¨ ÏÉùÏÑ±"""
        return Task(
            description=f"""
            Ïã§Ï†ú Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ÏôÄ Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞Î•º ÌÜµÌï©ÌïòÏó¨ ÏôÑÎ≤ΩÌïú Îß§Í±∞ÏßÑ Íµ¨Ï°∞Î•º ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.
            
            **ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏöîÏïΩ:**
            - ÏÑπÏÖò Ïàò: {len(extracted_text_data['sections'])}Í∞ú
            - Ï¥ù Í∏∏Ïù¥: {extracted_text_data['total_content_length']} Î¨∏Ïûê
            
            **Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞ ÏöîÏïΩ:**
            - Ï¥ù Ïù¥ÎØ∏ÏßÄ: {extracted_image_data['total_images']}Í∞ú
            - ÌÖúÌîåÎ¶ø Ïàò: {len(extracted_image_data['template_images'])}Í∞ú
            
            **ÌÜµÌï© ÏöîÍµ¨ÏÇ¨Ìï≠:**
            1. ÌÖçÏä§Ìä∏ÏôÄ Ïù¥ÎØ∏ÏßÄÏùò ÏôÑÎ≤ΩÌïú Îß§Ïπ≠
            2. Í∞Å ÏÑπÏÖòÎ≥Ñ ÏµúÏ†Å ÌÖúÌîåÎ¶ø ÏÑ†ÌÉù
            3. ÏΩòÌÖêÏ∏† ÌíàÏßà Î≥¥Ïû•
            4. ÏãúÍ∞ÅÏ†Å ÏùºÍ¥ÄÏÑ± Ïú†ÏßÄ
            5. JSX Íµ¨ÌòÑÏùÑ ÏúÑÌïú ÏôÑÏ†ÑÌïú Ïä§Ìéô ÏÉùÏÑ±
            
            **ÏµúÏ¢Ö Ï∂úÎ†• Íµ¨Ï°∞:**
            ```
            {
                "selected_templates": ["ÌÖúÌîåÎ¶ø Î™©Î°ù"],
                "content_sections": [
                    {
                        "template": "ÌÖúÌîåÎ¶øÎ™Ö",
                        "title": "Ïã§Ï†ú Ï†úÎ™©",
                        "subtitle": "Ïã§Ï†ú Î∂ÄÏ†úÎ™©", 
                        "body": "Ïã§Ï†ú Î≥∏Î¨∏ ÎÇ¥Ïö©",
                        "tagline": "ÌÉúÍ∑∏ÎùºÏù∏",
                        "images": ["Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ URLÎì§"],
                        "metadata": {
                            "content_quality": "ÌíàÏßà Ï†êÏàò",
                            "image_count": "Ïù¥ÎØ∏ÏßÄ Ïàò",
                            "source": "Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§"
                        }
                    }
                ],
                "integration_metadata": {
                    "total_sections": "ÏÑπÏÖò Ïàò",
                    "integration_quality_score": "ÌíàÏßà Ï†êÏàò"
                }
            }
            ```
            
            Ïù¥Ï†Ñ ÌÉúÏä§ÌÅ¨Îì§Ïùò Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÌôúÏö©ÌïòÏó¨ Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞òÏùò Í≥†ÌíàÏßà Îß§Í±∞ÏßÑ Íµ¨Ï°∞Î•º ÏôÑÏÑ±ÌïòÏÑ∏Ïöî.
            """,
            expected_output="Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò ÏôÑÏÑ±Îêú Îß§Í±∞ÏßÑ Íµ¨Ï°∞ JSON",
            agent=self.crew_agent,
            context=[self._create_enhanced_text_analysis_task(extracted_text_data, []), 
                    self._create_enhanced_image_analysis_task(extracted_image_data, [])]
        )

    async def _process_enhanced_crew_result_async(self, crew_result, extracted_text_data: Dict, 
                                                extracted_image_data: Dict, org_results: List[Dict], 
                                                binding_results: List[Dict]) -> Dict:
        """Í∞ïÌôîÎêú Crew Ïã§Ìñâ Í≤∞Í≥º Ï≤òÎ¶¨ (ÎπÑÎèôÍ∏∞)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._process_enhanced_crew_result, crew_result, extracted_text_data, 
            extracted_image_data, org_results, binding_results
        )

    def _process_enhanced_crew_result(self, crew_result, extracted_text_data: Dict, 
                                    extracted_image_data: Dict, org_results: List[Dict], 
                                    binding_results: List[Dict]) -> Dict:
        """Í∞ïÌôîÎêú Crew Ïã§Ìñâ Í≤∞Í≥º Ï≤òÎ¶¨"""
        try:
            # Crew Í≤∞Í≥ºÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
            if hasattr(crew_result, 'raw') and crew_result.raw:
                result_text = crew_result.raw
            else:
                result_text = str(crew_result)
            
            # JSON Ìå®ÌÑ¥ Ï∞æÍ∏∞ Î∞è ÌååÏã±
            parsed_data = self._extract_json_from_text(result_text)
            
            # Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Íµ¨Ï°∞ ÏÉùÏÑ±
            if not parsed_data.get('content_sections') or len(parsed_data.get('content_sections', [])) == 0:
                parsed_data = self._create_enhanced_structure(extracted_text_data, extracted_image_data, org_results, binding_results)
            else:
                # Í∏∞Ï°¥ ÌååÏã±Îêú Îç∞Ïù¥ÌÑ∞Ïóê Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Ï∂îÍ∞Ä
                parsed_data = self._enhance_parsed_data_with_real_images(parsed_data, extracted_image_data)
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
            parsed_data['integration_metadata'] = {
                "total_sections": len(parsed_data.get('content_sections', [])),
                "total_templates": len(set(section.get("template", f"Section{i+1:02d}.jsx") for i, section in enumerate(parsed_data.get('content_sections', [])))),
                "agent_enhanced": True,
                "org_results_utilized": len(org_results),
                "binding_results_utilized": len(binding_results),
                "integration_quality_score": self._calculate_enhanced_quality_score(parsed_data.get('content_sections', []), len(org_results), len(binding_results)),
                "crewai_enhanced": True,
                "async_processed": True,
                "data_source": "real_extracted_data",
                "real_content_used": True,
                "real_images_used": extracted_image_data['total_images'] > 0
            }
            
            return parsed_data
            
        except Exception as e:
            print(f"‚ö†Ô∏è Í∞ïÌôîÎêú Crew Í≤∞Í≥º Ï≤òÎ¶¨ Ïã§Ìå®: {e}")
            return self._create_enhanced_structure(extracted_text_data, extracted_image_data, org_results, binding_results)

    def _create_enhanced_structure(self, extracted_text_data: Dict, extracted_image_data: Dict, 
                                 org_results: List[Dict], binding_results: List[Dict]) -> Dict:
        """Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Í∞ïÌôîÎêú Íµ¨Ï°∞ ÏÉùÏÑ±"""
        content_sections = []
        
        # Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏ ÏÑπÏÖò ÌôúÏö©
        for i, section in enumerate(extracted_text_data['sections']):
            template = section.get('template', f'Section{i+1:02d}.jsx')
            
            # Ìï¥Îãπ ÌÖúÌîåÎ¶øÏùò Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Í∞ÄÏ†∏Ïò§Í∏∞
            template_images = extracted_image_data['template_images'].get(template, [])
            
            # Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏúºÎ©¥ Îã§Î•∏ ÌÖúÌîåÎ¶øÏùò Ïù¥ÎØ∏ÏßÄ ÏÇ¨Ïö©
            if not template_images:
                for temp_name, temp_images in extracted_image_data['template_images'].items():
                    if temp_images:
                        template_images = temp_images[:2]  # ÏµúÎåÄ 2Í∞ú
                        break
            
            enhanced_section = {
                'template': template,
                'title': section.get('title', 'Ïó¨Ìñâ Ïù¥ÏïºÍ∏∞'),
                'subtitle': section.get('subtitle', 'ÌäπÎ≥ÑÌïú ÏàúÍ∞ÑÎì§'),
                'body': section.get('body', 'Ïó¨ÌñâÏùò ÌäπÎ≥ÑÌïú ÏàúÍ∞ÑÎì§ÏùÑ Îã¥ÏùÄ Ïù¥ÏïºÍ∏∞ÏûÖÎãàÎã§.'),
                'tagline': section.get('tagline', 'TRAVEL & CULTURE'),
                'images': template_images,
                'metadata': {
                    "agent_enhanced": True,
                    "real_content": True,
                    "real_images": len(template_images) > 0,
                    "content_source": section.get('layout_source', 'extracted'),
                    "content_length": len(section.get('body', '')),
                    "image_count": len(template_images),
                    "quality_verified": True
                }
            }
            content_sections.append(enhanced_section)
        
        # ÏµúÏÜå 1Í∞ú ÏÑπÏÖò Î≥¥Ïû•
        if not content_sections:
            # Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏûàÏúºÎ©¥ ÏÇ¨Ïö©
            fallback_images = []
            for template_images in extracted_image_data['template_images'].values():
                fallback_images.extend(template_images[:2])
                if len(fallback_images) >= 2:
                    break
            
            content_sections = [{
                'template': 'Section01.jsx',
                'title': 'Ïó¨Ìñâ Îß§Í±∞ÏßÑ',
                'subtitle': 'ÌäπÎ≥ÑÌïú Ïù¥ÏïºÍ∏∞',
                'body': 'Ïó¨ÌñâÏùò ÌäπÎ≥ÑÌïú ÏàúÍ∞ÑÎì§ÏùÑ Îã¥ÏùÄ Îß§Í±∞ÏßÑÏûÖÎãàÎã§. ÏïÑÎ¶ÑÎã§Ïö¥ ÌíçÍ≤ΩÍ≥º Ìï®ÍªòÌïòÎäî ÌäπÎ≥ÑÌïú Í≤ΩÌóòÏùÑ Í≥µÏú†Ìï©ÎãàÎã§.',
                'tagline': 'TRAVEL & CULTURE',
                'images': fallback_images,
                'metadata': {
                    "agent_enhanced": True,
                    "fallback_content": True,
                    "real_images": len(fallback_images) > 0,
                    "image_count": len(fallback_images)
                }
            }]
        
        return {
            "selected_templates": [section.get("template", f"Section{i+1:02d}.jsx") for i, section in enumerate(content_sections)],
            "content_sections": content_sections
        }

    def _enhance_parsed_data_with_real_images(self, parsed_data: Dict, extracted_image_data: Dict) -> Dict:
        """ÌååÏã±Îêú Îç∞Ïù¥ÌÑ∞Ïóê Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Ï∂îÍ∞Ä"""
        if 'content_sections' in parsed_data:
            for section in parsed_data['content_sections']:
                template = section.get('template', 'Section01.jsx')
                
                # Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄÎ°ú ÍµêÏ≤¥
                real_images = extracted_image_data['template_images'].get(template, [])
                if real_images:
                    section['images'] = real_images
                elif extracted_image_data['total_images'] > 0:
                    # Îã§Î•∏ ÌÖúÌîåÎ¶øÏùò Ïù¥ÎØ∏ÏßÄ ÏÇ¨Ïö©
                    for temp_images in extracted_image_data['template_images'].values():
                        if temp_images:
                            section['images'] = temp_images[:2]
                            break
                
                # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏
                if 'metadata' not in section:
                    section['metadata'] = {}
                section['metadata'].update({
                    "real_images_used": len(section.get('images', [])) > 0,
                    "image_count": len(section.get('images', []))
                })
        
        return parsed_data

    # Ïú†Ìã∏Î¶¨Ìã∞ Î©îÏÑúÎìúÎì§
    def _split_content_into_sections(self, content: str) -> List[str]:
        """ÏΩòÌÖêÏ∏†Î•º ÏÑπÏÖòÎ≥ÑÎ°ú Î∂ÑÌï†"""
        # Ìó§ÎçîÎÇò Íµ¨Î∂ÑÏûê Í∏∞Î∞ò Î∂ÑÌï†
        sections = []
        
        # === Ìå®ÌÑ¥ÏúºÎ°ú Î∂ÑÌï†
        if '===' in content:
            parts = content.split('===')
            for part in parts:
                clean_part = part.strip()
                if len(clean_part) > 100:
                    sections.append(clean_part)
        
        # Î¨∏Îã® Í∏∞Î∞ò Î∂ÑÌï†
        elif '\n\n' in content:
            paragraphs = content.split('\n\n')
            current_section = ""
            for paragraph in paragraphs:
                if len(current_section + paragraph) > 800:
                    if current_section:
                        sections.append(current_section.strip())
                    current_section = paragraph
                else:
                    current_section += "\n\n" + paragraph
            
            if current_section:
                sections.append(current_section.strip())
        
        # Ï†ÑÏ≤¥Î•º ÌïòÎÇòÏùò ÏÑπÏÖòÏúºÎ°ú
        else:
            sections = [content]
        
        return [s for s in sections if len(s) > 50]

    def _extract_title_from_content(self, content: str) -> str:
        """ÏΩòÌÖêÏ∏†ÏóêÏÑú Ï†úÎ™© Ï∂îÏ∂ú"""
        lines = content.split('\n')
        for line in lines[:3]:  # Ï≤òÏùå 3Ï§ÑÏóêÏÑú Ï∞æÍ∏∞
            line = line.strip()
            if line and len(line) < 100:
                # Ï†úÎ™© Ìå®ÌÑ¥ Ï†ïÎ¶¨
                title = re.sub(r'^[#\*\-\s]+', '', line)
                title = re.sub(r'[#\*\-\s]+$', '', title)
                if len(title) > 5:
                    return title[:50]
        
        return "Ïó¨Ìñâ Ïù¥ÏïºÍ∏∞"

    def _extract_subtitle_from_content(self, content: str) -> str:
        """ÏΩòÌÖêÏ∏†ÏóêÏÑú Î∂ÄÏ†úÎ™© Ï∂îÏ∂ú"""
        lines = content.split('\n')
        for i, line in enumerate(lines[1:4]):  # 2-4Î≤àÏß∏ Ï§ÑÏóêÏÑú Ï∞æÍ∏∞
            line = line.strip()
            if line and len(line) < 80 and len(line) > 5:
                subtitle = re.sub(r'^[#\*\-\s]+', '', line)
                subtitle = re.sub(r'[#\*\-\s]+$', '', subtitle)
                if len(subtitle) > 3:
                    return subtitle[:40]
        
        return "ÌäπÎ≥ÑÌïú ÏàúÍ∞ÑÎì§"

    def _clean_content(self, content: str) -> str:
        """ÏΩòÌÖêÏ∏† Ï†ïÎ¶¨"""
        # Î∂àÌïÑÏöîÌïú Ìå®ÌÑ¥ Ï†úÍ±∞
        cleaned = re.sub(r'^[#\*\-\s]+', '', content, flags=re.MULTILINE)
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        cleaned = re.sub(r'^\s*$\n', '', cleaned, flags=re.MULTILINE)
        
        return cleaned.strip()

    def _extract_structured_content(self, text: str) -> List[Dict]:
        """Íµ¨Ï°∞ÌôîÎêú ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú"""
        sections = []
        
        # Ï†úÎ™© Ìå®ÌÑ¥ Ï∞æÍ∏∞
        title_patterns = [
            r'Ï†úÎ™©[:\s]*([^\n]+)',
            r'title[:\s]*([^\n]+)',
            r'## ([^\n]+)',
            r'# ([^\n]+)'
        ]
        
        for pattern in title_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                title = match.group(1).strip()
                if len(title) > 3:
                    section = {
                        "template": f"Section{len(sections)+1:02d}.jsx",
                        "title": title[:50],
                        "subtitle": "Ïó¨ÌñâÏùò ÌäπÎ≥ÑÌïú ÏàúÍ∞Ñ",
                        "body": f"{title}Ïóê ÎåÄÌïú ÏûêÏÑ∏Ìïú Ïù¥ÏïºÍ∏∞Î•º Îã¥Í≥† ÏûàÏäµÎãàÎã§.",
                        "tagline": "TRAVEL & CULTURE",
                        "layout_source": "org_agent"
                    }
                    sections.append(section)
                    
                    if len(sections) >= 3:  # ÏµúÎåÄ 3Í∞ú
                        break
            
            if sections:
                break
        
        return sections

    def _extract_template_from_binding_result(self, result: Dict) -> str:
        """BindingAgent Í≤∞Í≥ºÏóêÏÑú ÌÖúÌîåÎ¶øÎ™Ö Ï∂îÏ∂ú"""
        task_desc = result.get('task_description', '')
        
        # ÌÖúÌîåÎ¶øÎ™Ö Ìå®ÌÑ¥ Ï∞æÍ∏∞
        template_match = re.search(r'Section\d+\.jsx', task_desc)
        if template_match:
            return template_match.group()
        
        return "Section01.jsx"

    def _extract_image_source_info(self, result: Dict, url: str) -> Dict:
        """Ïù¥ÎØ∏ÏßÄ ÏÜåÏä§ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        return {
            "url": url,
            "template": self._extract_template_from_binding_result(result),
            "source": "binding_agent",
            "timestamp": result.get('timestamp', ''),
            "quality_verified": True
        }

    def _format_sections_for_analysis(self, sections: List[Dict]) -> str:
        """Î∂ÑÏÑùÏö© ÏÑπÏÖò Ìè¨Îß∑ÌåÖ"""
        formatted = []
        for i, section in enumerate(sections[:3]):  # ÏµúÎåÄ 3Í∞úÎßå ÌëúÏãú
            formatted.append(f"""
ÏÑπÏÖò {i+1}:
- ÌÖúÌîåÎ¶ø: {section.get('template', 'N/A')}
- Ï†úÎ™©: {section.get('title', 'N/A')}
- Î∂ÄÏ†úÎ™©: {section.get('subtitle', 'N/A')}
- Î≥∏Î¨∏ Í∏∏Ïù¥: {len(section.get('body', ''))} Î¨∏Ïûê
- ÏÜåÏä§: {section.get('layout_source', 'N/A')}
""")
        
        return "\n".join(formatted)

    def _format_images_for_analysis(self, template_images: Dict) -> str:
        """Î∂ÑÏÑùÏö© Ïù¥ÎØ∏ÏßÄ Ìè¨Îß∑ÌåÖ"""
        formatted = []
        for template, images in template_images.items():
            formatted.append(f"""
{template}: {len(images)}Í∞ú Ïù¥ÎØ∏ÏßÄ
{chr(10).join([f"  - {img}" for img in images[:2]])}
""")
        
        return "\n".join(formatted)

    def _format_image_sources(self, image_sources: List[Dict]) -> str:
        """Ïù¥ÎØ∏ÏßÄ ÏÜåÏä§ Ï†ïÎ≥¥ Ìè¨Îß∑ÌåÖ"""
        if not image_sources:
            return "Ïù¥ÎØ∏ÏßÄ ÏÜåÏä§ Ï†ïÎ≥¥ ÏóÜÏùå"
        
        formatted = []
        for source in image_sources[:3]:  # ÏµúÎåÄ 3Í∞úÎßå ÌëúÏãú
            formatted.append(f"- {source.get('template', 'N/A')}: {source.get('url', 'N/A')}")
        
        return "\n".join(formatted)

    def _extract_json_from_text(self, text: str) -> Dict:
        """ÌÖçÏä§Ìä∏ÏóêÏÑú JSON Ï∂îÏ∂ú"""
        # JSON Ìå®ÌÑ¥ Ï∞æÍ∏∞
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        json_matches = re.findall(json_pattern, text, re.DOTALL)
        
        parsed_data = {}
        for match in json_matches:
            try:
                if len(match) < 10000:  # ÌÅ¨Í∏∞ Ï†úÌïú
                    data = json.loads(match)
                    if isinstance(data, dict):
                        parsed_data.update(data)
            except json.JSONDecodeError:
                continue
        
        return parsed_data

    # Í∏∞Ï°¥ Î©îÏÑúÎìúÎì§ Ïú†ÏßÄ (Î≥ÄÍ≤Ω ÏóÜÏùå)
    async def _log_coordination_result_async(self, final_result: Dict, text_mapping: Dict, 
                                           image_distribution: Dict, org_results: List[Dict], 
                                           binding_results: List[Dict]) -> None:
        """Ï°∞Ïú® Í≤∞Í≥º Î°úÍπÖ (ÎπÑÎèôÍ∏∞)"""
        await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: self.logger.log_agent_real_output(
                agent_name="CoordinatorAgent",
                agent_role="Îß§Í±∞ÏßÑ Íµ¨Ï°∞ ÌÜµÌï© Ï°∞Ïú®Ïûê",
                task_description=f"CrewAI Í∏∞Î∞ò ÎπÑÎèôÍ∏∞ Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÌôúÏö©ÏúºÎ°ú {len(final_result.get('content_sections', []))}Í∞ú ÏÑπÏÖò ÏÉùÏÑ±",
                final_answer=f"Îß§Í±∞ÏßÑ Íµ¨Ï°∞ ÌÜµÌï© ÏôÑÎ£å: {len(final_result.get('content_sections', []))}Í∞ú ÏÑπÏÖò, ÌíàÏßà Ï†êÏàò: {final_result.get('integration_metadata', {}).get('integration_quality_score', 0):.2f}",
                reasoning_process=f"CrewAI Í∏∞Î∞ò ÎπÑÎèôÍ∏∞ Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú Î∞è ÌôúÏö©ÏúºÎ°ú OrgAgent {len(org_results)}Í∞ú, BindingAgent {len(binding_results)}Í∞ú Í≤∞Í≥º ÌÜµÌï©",
                execution_steps=[
                    "CrewAI ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ±",
                    "ÎπÑÎèôÍ∏∞ Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú",
                    "Í∞ïÌôîÎêú ÌÖçÏä§Ìä∏ Î∂ÑÏÑù ÌÉúÏä§ÌÅ¨ Ïã§Ìñâ",
                    "Í∞ïÌôîÎêú Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÌÉúÏä§ÌÅ¨ Ïã§Ìñâ", 
                    "Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò ÌÜµÌï© Ï°∞Ïú®",
                    "ÌíàÏßà Í≤ÄÏ¶ù Î∞è ÏµúÏ†ÅÌôî"
                ],
                raw_input={"text_mapping": text_mapping, "image_distribution": image_distribution},
                raw_output=final_result,
                performance_metrics={
                    "async_processing": True,
                    "real_data_used": True,
                    "crew_execution_time": "optimized",
                    "total_sections": len(final_result.get('content_sections', [])),
                    "quality_score": final_result.get('integration_metadata', {}).get('integration_quality_score', 0),
                    "org_results_utilized": len(org_results),
                    "binding_results_utilized": len(binding_results),
                    "real_images_count": sum(len(section.get('images', [])) for section in final_result.get('content_sections', [])),
                    "content_enhancement": True
                }
            )
        )

    # Í∏∞Ï°¥ Î©îÏÑúÎìúÎì§ Ïú†ÏßÄ
    async def _get_enhanced_previous_results_async(self) -> List[Dict]:
        """Í∞ïÌôîÎêú Ïù¥Ï†Ñ Í≤∞Í≥º ÏàòÏßë (ÎπÑÎèôÍ∏∞)"""
        try:
            # Î≥ëÎ†¨Î°ú Í≤∞Í≥º ÏàòÏßë
            basic_results_task = asyncio.get_event_loop().run_in_executor(
                None, lambda: self.logger.get_all_previous_results("CoordinatorAgent")
            )
            file_results_task = self._load_results_from_file_async()
            
            basic_results, file_results = await asyncio.gather(
                basic_results_task, file_results_task, return_exceptions=True
            )
            
            # ÏòàÏô∏ Ï≤òÎ¶¨
            if isinstance(basic_results, Exception):
                print(f"‚ö†Ô∏è Í∏∞Î≥∏ Í≤∞Í≥º ÏàòÏßë Ïã§Ìå®: {basic_results}")
                basic_results = []
            
            if isinstance(file_results, Exception):
                print(f"‚ö†Ô∏è ÌååÏùº Í≤∞Í≥º ÏàòÏßë Ïã§Ìå®: {file_results}")
                file_results = []
            
            # Í≤∞Í≥º Ìï©ÏπòÍ∏∞
            results = []
            results.extend(basic_results)
            results.extend(file_results)
            
            # Ï§ëÎ≥µ Ï†úÍ±∞ (ÎπÑÎèôÍ∏∞)
            unique_results = await asyncio.get_event_loop().run_in_executor(
                None, self._deduplicate_results, results
            )
            
            return unique_results
            
        except Exception as e:
            print(f"‚ö†Ô∏è ÎπÑÎèôÍ∏∞ Ïù¥Ï†Ñ Í≤∞Í≥º ÏàòÏßë Ïã§Ìå®: {e}")
            return []

    async def _load_results_from_file_async(self) -> List[Dict]:
        """ÌååÏùºÏóêÏÑú ÏßÅÏ†ë Í≤∞Í≥º Î°úÎìú (ÎπÑÎèôÍ∏∞)"""
        return await asyncio.get_event_loop().run_in_executor(
            None, self._load_results_from_file
        )

    def _load_results_from_file(self) -> List[Dict]:
        """ÌååÏùºÏóêÏÑú ÏßÅÏ†ë Í≤∞Í≥º Î°úÎìú (ÎèôÍ∏∞ Î≤ÑÏ†Ñ - Ìò∏ÌôòÏÑ± Ïú†ÏßÄ)"""
        results = []
        
        try:
            # latest_outputs.jsonÏóêÏÑú Î°úÎìú
            if os.path.exists('./agent_outputs/latest_outputs.json'):
                with open('./agent_outputs/latest_outputs.json', 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    latest_outputs = data.get('latest_outputs', [])
                    results.extend(latest_outputs)
            
            # ÏÑ∏ÏÖò ÌååÏùºÏóêÏÑú Î°úÎìú
            session_files = []
            if os.path.exists('./agent_outputs/outputs'):
                for session_dir in os.listdir('./agent_outputs/outputs'):
                    session_path = os.path.join('./agent_outputs/outputs', session_dir, 'agent_outputs.json')
                    if os.path.exists(session_path):
                        session_files.append(session_path)
            
            # ÏµúÏã† ÏÑ∏ÏÖò ÌååÏùº Ïö∞ÏÑ† Ï≤òÎ¶¨
            session_files.sort(reverse=True)
            for session_file in session_files[:3]:  # ÏµúÍ∑º 3Í∞ú ÏÑ∏ÏÖòÎßå
                try:
                    with open(session_file, 'r', encoding='utf-8') as f:
                        session_data = json.load(f)
                        if 'outputs' in session_data:
                            results.extend(session_data['outputs'])
                except Exception as e:
                    print(f"‚ö†Ô∏è ÏÑ∏ÏÖò ÌååÏùº Î°úÎìú Ïã§Ìå® {session_file}: {e}")
                    continue
                    
        except Exception as e:
            print(f"‚ö†Ô∏è ÌååÏùº Í≤∞Í≥º Î°úÎìú Ïã§Ìå®: {e}")
        
        return results

    def _filter_agent_results(self, results: List[Dict], agent_type: str) -> List[Dict]:
        """ÌäπÏ†ï ÏóêÏù¥Ï†ÑÌä∏ Í≤∞Í≥º ÌïÑÌÑ∞ÎßÅ"""
        filtered = []
        for result in results:
            agent_name = result.get('agent_name', '')
            if agent_type in agent_name:
                filtered.append(result)
        return filtered

    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """Í≤∞Í≥º Ï§ëÎ≥µ Ï†úÍ±∞"""
        seen_ids = set()
        unique_results = []
        
        for result in results:
            result_id = result.get('output_id') or result.get('timestamp', '')
            if result_id and result_id not in seen_ids:
                seen_ids.add(result_id)
                unique_results.append(result)
        
        return unique_results

    def _calculate_enhanced_quality_score(self, content_sections: List[Dict], 
                                        org_count: int, binding_count: int) -> float:
        """Í∞ïÌôîÎêú ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞"""
        if not content_sections:
            return 0.0
        
        quality_score = 0.0
        
        # 1. ÏÑπÏÖò ÌíàÏßà (60%)
        section_quality = 0.0
        for section in content_sections:
            section_score = 0.0
            
            if section.get("title") and len(section.get("title", "")) > 3:
                section_score += 0.25
            if section.get("subtitle") and len(section.get("subtitle", "")) > 3:
                section_score += 0.15
            if section.get("body") and len(section.get("body", "")) > 50:
                section_score += 0.35
            if section.get("images") and len(section.get("images", [])) > 0:
                section_score += 0.25
            
            section_quality += min(section_score, 1.0)
        
        quality_score += (section_quality / len(content_sections)) * 0.6
        
        # 2. ÏóêÏù¥Ï†ÑÌä∏ ÌôúÏö©ÎèÑ (25%)
        agent_score = 0.0
        if org_count > 0:
            agent_score += 0.5
        if binding_count > 0:
            agent_score += 0.5
        
        quality_score += agent_score * 0.25
        
        # 3. Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÌôúÏö©ÎèÑ (15%)
        real_data_score = 0.0
        real_content_sections = sum(1 for section in content_sections 
                                  if section.get('metadata', {}).get('real_content', False))
        real_image_sections = sum(1 for section in content_sections 
                                if section.get('metadata', {}).get('real_images', False))
        
        if real_content_sections > 0:
            real_data_score += 0.5
        if real_image_sections > 0:
            real_data_score += 0.5
        
        quality_score += real_data_score * 0.15
        
        return min(quality_score, 1.0)

    # ÎèôÍ∏∞ Î≤ÑÏ†Ñ Î©îÏÑúÎìú Ïú†ÏßÄ (Ìò∏ÌôòÏÑ± Î≥¥Ïû•)
    def coordinate_magazine_creation_sync(self, text_mapping: Dict, image_distribution: Dict) -> Dict:
        """Îß§Í±∞ÏßÑ Íµ¨Ï°∞ ÌÜµÌï© Ï°∞Ïú® (ÎèôÍ∏∞ Î≤ÑÏ†Ñ - Ìò∏ÌôòÏÑ± Ïú†ÏßÄ)"""
        return asyncio.run(self.coordinate_magazine_creation(text_mapping, image_distribution))

    def _get_enhanced_previous_results(self) -> List[Dict]:
        """Í∞ïÌôîÎêú Ïù¥Ï†Ñ Í≤∞Í≥º ÏàòÏßë (ÎèôÍ∏∞ Î≤ÑÏ†Ñ - Ìò∏ÌôòÏÑ± Ïú†ÏßÄ)"""
        return asyncio.run(self._get_enhanced_previous_results_async())
