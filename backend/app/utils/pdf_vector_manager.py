import os
import json
from typing import List, Dict, Optional
from openai import AzureOpenAI
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.models import VectorizedQuery
from azure.search.documents.indexes.models import (
    SearchIndex, SimpleField, SearchFieldDataType, VectorSearch,
    VectorSearchProfile, HnswAlgorithmConfiguration, SearchField
)
from utils.pdf_splitter import PDFSplitter
from dotenv import load_dotenv
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
dotenv_path = Path(r'C:\Users\EL0021\Desktop\odiga_agent\.env')
load_dotenv(dotenv_path=dotenv_path, override=True)

class PDFVectorManager:
    """PDF ë²¡í„° ë°ì´í„° ê´€ë¦¬ì - Azure ì„œë¹„ìŠ¤ í™œìš©"""
    
    def __init__(self):
        
        # Azure Form Recognizer ì´ˆê¸°í™”
        self.form_recognizer_endpoint = os.getenv("AZURE_FORM_RECOGNIZER_ENDPOINT")
        self.form_recognizer_key = os.getenv("AZURE_FORM_RECOGNIZER_KEY")
        self.form_recognizer_client = DocumentAnalysisClient(
            endpoint=self.form_recognizer_endpoint,
            credential=AzureKeyCredential(self.form_recognizer_key)
        )
        
        # Azure Cognitive Search ì´ˆê¸°í™”
        self.search_endpoint = os.getenv("AZURE_SEARCH_ENDPOINT")
        self.search_key = os.getenv("AZURE_SEARCH_KEY")
        self.search_index_name = "magazine-templates-index"
        
        self.search_client = SearchClient(
            endpoint=self.search_endpoint,
            index_name=self.search_index_name,
            credential=AzureKeyCredential(self.search_key)
        )
        
        self.search_index_client = SearchIndexClient(
            endpoint=self.search_endpoint,
            credential=AzureKeyCredential(self.search_key)
        )
        
        # Azure OpenAI ì´ˆê¸°í™” (v1.x ë°©ì‹)
        self.openai_client = AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_KEY"),         
            api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
        )
        
        self.embedding_model = "text-embedding-ada-002"
        self.pdf_splitter = PDFSplitter(max_size_mb=20.0)
    
        
    def initialize_search_index(self):
        """Azure Cognitive Search ì¸ë±ìŠ¤ ì´ˆê¸°í™” - ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ê¹Œì§€ í™•ì¸"""
        try:
            # 1. ê¸°ì¡´ ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            try:
                existing_index = self.search_index_client.get_index(self.search_index_name)
                print(f"âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ '{self.search_index_name}' ë°œê²¬")
                
                # ì¸ë±ìŠ¤ì— ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                if self._check_index_has_data():
                    print(f"âœ… ì¸ë±ìŠ¤ì— ë°ì´í„° ì¡´ì¬ ({self._get_document_count()}ê°œ ë¬¸ì„œ) - ë²¡í„° ì²˜ë¦¬ ìƒëµ")
                    return True  # ì´ˆê¸°í™” ì™„ë£Œ, PDF ì²˜ë¦¬ ë¶ˆí•„ìš”
                else:
                    print(f"âš ï¸ ì¸ë±ìŠ¤ëŠ” ìˆì§€ë§Œ ë°ì´í„° ì—†ìŒ - PDF ì²˜ë¦¬ í•„ìš”")
                    return False  # ë°ì´í„° ì²˜ë¦¬ í•„ìš”
                    
            except Exception as e:
                print(f"ğŸ“„ ê¸°ì¡´ ì¸ë±ìŠ¤ ì—†ìŒ - ìƒˆë¡œ ìƒì„±: {e}")
            
            # 2. ìƒˆ ì¸ë±ìŠ¤ ìƒì„± (ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
            vector_search = VectorSearch(
                profiles=[
                    VectorSearchProfile(
                        name="magazine-profile",
                        algorithm_configuration_name="magazine-algorithm"
                    )
                ],
                algorithms=[
                    HnswAlgorithmConfiguration(
                        name="magazine-algorithm",
                        parameters={
                            "m": 4,
                            "efConstruction": 400,
                            "efSearch": 500,
                            "metric": "cosine"
                        }
                    )
                ]
            )
            
            fields = [
                SimpleField(name="id", type=SearchFieldDataType.String, key=True),
                SimpleField(name="pdf_name", type=SearchFieldDataType.String, filterable=True),
                SimpleField(name="page_number", type=SearchFieldDataType.Int32, filterable=True),
                SimpleField(name="content_type", type=SearchFieldDataType.String, filterable=True),
                SimpleField(name="text_content", type=SearchFieldDataType.String, searchable=True),
                SimpleField(name="layout_info", type=SearchFieldDataType.String),
                SimpleField(name="image_info", type=SearchFieldDataType.String),
                SearchField(
                    name="content_vector",
                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                    searchable=True,
                    vector_search_dimensions=1536,
                    vector_search_profile_name="magazine-profile"
                )
            ]
            
            index = SearchIndex(
                name=self.search_index_name,
                fields=fields,
                vector_search=vector_search
            )
            
            self.search_index_client.create_index(index)
            print(f"âœ… ìƒˆ ì¸ë±ìŠ¤ '{self.search_index_name}' ìƒì„± ì™„ë£Œ")
            return False  # ë°ì´í„° ì²˜ë¦¬ í•„ìš”
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def _check_index_has_data(self) -> bool:
        """ì¸ë±ìŠ¤ì— ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ - ê°•í™”ëœ ë²„ì „"""
        try:
            print("ğŸ” ì¸ë±ìŠ¤ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ì¤‘...")
            
            # ì™€ì¼ë“œì¹´ë“œ ê²€ìƒ‰ìœ¼ë¡œ ëª¨ë“  ë¬¸ì„œ í™•ì¸ (ê²€ìƒ‰ ê²°ê³¼ [8] ì°¸ì¡°)
            results = self.search_client.search(
                search_text="*",  # ì™€ì¼ë“œì¹´ë“œë¡œ ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰
                top=1,            # 1ê°œë§Œ í™•ì¸í•˜ë©´ ì¶©ë¶„
                include_total_count=True  # ì „ì²´ ê°œìˆ˜ í¬í•¨
            )
            
            # ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            document_count = 0
            for result in results:
                document_count += 1
                break  # ì²« ë²ˆì§¸ ê²°ê³¼ë§Œ í™•ì¸í•˜ë©´ ì¶©ë¶„
            
            # ì „ì²´ ê°œìˆ˜ë„ í™•ì¸ (ê°€ëŠ¥í•œ ê²½ìš°)
            total_count = getattr(results, 'get_count', lambda: None)()
            
            if document_count > 0:
                print(f"âœ… ë°ì´í„° í™•ì¸ë¨: ìµœì†Œ {document_count}ê°œ ë¬¸ì„œ ì¡´ì¬")
                if total_count:
                    print(f"   ì´ ë¬¸ì„œ ìˆ˜: {total_count}ê°œ")
                return True
            else:
                print(f"âŒ ë°ì´í„° ì—†ìŒ: ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìŒ")
                return False
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _get_document_count(self) -> int:
        """ì¸ë±ìŠ¤ì˜ ì´ ë¬¸ì„œ ìˆ˜ ë°˜í™˜"""
        try:
            results = self.search_client.search(
                search_text="*",
                top=0,  # ë¬¸ì„œëŠ” ê°€ì ¸ì˜¤ì§€ ì•Šê³  ê°œìˆ˜ë§Œ
                include_total_count=True
            )
            
            total_count = getattr(results, 'get_count', lambda: 0)()
            return total_count if total_count else 0
            
        except Exception as e:
            print(f"ë¬¸ì„œ ìˆ˜ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0

    def process_pdf_templates(self, template_folder: str = "templates"):
        """í…œí”Œë¦¿ í´ë”ì˜ PDF íŒŒì¼ë“¤ì„ ë²¡í„°í™”í•˜ì—¬ ì¸ë±ìŠ¤ì— ì €ì¥ - ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        
        if not os.path.exists(template_folder):
            print(f"âŒ í…œí”Œë¦¿ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {template_folder}")
            return
        
        # ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë° ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        has_data = self.initialize_search_index()
        
        if has_data:
            print("ğŸ‰ ê¸°ì¡´ ë²¡í„° ë°ì´í„° ì‚¬ìš© - PDF ì²˜ë¦¬ ì™„ì „ ìƒëµ")
            return
        
        print("ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘ - ì¸ë±ìŠ¤ì— ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ìƒˆë¡œìš´ ì¸ë±ìŠ¤")
        
        # PDF ë¶„í•  ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        self.pdf_splitter.test_split_functionality(template_folder)
        
        # 1ë‹¨ê³„: í° PDF íŒŒì¼ë“¤ ë¶„í• 
        print("\nğŸ”ª PDF ë¶„í•  ë‹¨ê³„")
        available_pdf_files = self.pdf_splitter.split_large_pdfs(template_folder)
        
        if not available_pdf_files:
            print(f"âŒ ì²˜ë¦¬í•  PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {template_folder}")
            return
        
        # ê¸°ì¡´ì— ì²˜ë¦¬ëœ PDF í™•ì¸ (ì¸ë±ìŠ¤ì— ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        processed_pdfs = self._get_processed_pdfs()
        new_pdfs = [pdf for pdf in available_pdf_files if pdf not in processed_pdfs]
        
        if not new_pdfs and processed_pdfs:
            print("âœ… ëª¨ë“  PDFê°€ ì´ë¯¸ ì²˜ë¦¬ë¨ - ì¶”ê°€ ì²˜ë¦¬ ë¶ˆí•„ìš”")
            return
        
        # ì²˜ë¦¬í•  PDFê°€ ì—†ìœ¼ë©´ ëª¨ë“  PDF ì²˜ë¦¬
        pdfs_to_process = new_pdfs if new_pdfs else available_pdf_files
        
        print(f"\nğŸ“ {len(pdfs_to_process)}ê°œ PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
        
        all_documents = []
        
        for pdf_file in pdfs_to_process:
            pdf_path = os.path.join(template_folder, pdf_file)
            print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_file}")
            
            try:
                # PDF ë¶„ì„
                pdf_documents = self._analyze_pdf(pdf_path, pdf_file)
                all_documents.extend(pdf_documents)
                print(f"âœ… {pdf_file}: {len(pdf_documents)}ê°œ ë¬¸ì„œ ìƒì„±")
                
            except Exception as e:
                print(f"âŒ {pdf_file} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ë²¡í„° ì¸ë±ìŠ¤ì— ì—…ë¡œë“œ
        if all_documents:
            self._upload_to_search_index(all_documents)
            print(f"âœ… ì´ {len(all_documents)}ê°œ ë¬¸ì„œê°€ ë²¡í„° ì¸ë±ìŠ¤ì— ì €ì¥ë¨")
            
            # ìµœì¢… í™•ì¸
            final_count = self._get_document_count()
            print(f"ğŸ¯ ìµœì¢… ì¸ë±ìŠ¤ ë¬¸ì„œ ìˆ˜: {final_count}ê°œ")
        else:
            print("âš ï¸ ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")


    def _get_processed_pdfs(self) -> List[str]:
        """ì´ë¯¸ ì²˜ë¦¬ëœ PDF ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ì¸ë±ìŠ¤ì—ì„œ ê³ ìœ í•œ PDF ì´ë¦„ë“¤ ê°€ì ¸ì˜¤ê¸°
            results = self.search_client.search(
                "*",
                select=["pdf_name"],
                top=1000  # ì¶©ë¶„í•œ ìˆ˜
            )
            
            processed_pdfs = set()
            for result in results:
                pdf_name = result.get("pdf_name")
                if pdf_name:
                    processed_pdfs.add(pdf_name)
            
            return list(processed_pdfs)
            
        except Exception as e:
            print(f"ì²˜ë¦¬ëœ PDF í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    
    def _analyze_pdf(self, pdf_path: str, pdf_name: str) -> List[Dict]:
        """Azure Form Recognizerë¡œ PDF ë¶„ì„ - ì•ˆì „í•œ í‚¤ ìƒì„±"""
        
        documents = []
        
        with open(pdf_path, "rb") as pdf_file:
            # Form Recognizerë¡œ ë ˆì´ì•„ì›ƒ ë¶„ì„
            poller = self.form_recognizer_client.begin_analyze_document(
                "prebuilt-layout", pdf_file
            )
            result = poller.result()
            
            # í˜ì´ì§€ë³„ ì²˜ë¦¬
            for page_idx, page in enumerate(result.pages):
                page_content = {
                    "text_blocks": [],
                    "images": [],
                    "tables": [],
                    "layout_structure": []
                }
                
                # í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ì¶œ
                if result.paragraphs:
                    for para in result.paragraphs:
                        if para.bounding_regions and para.bounding_regions[0].page_number == page_idx + 1:
                            page_content["text_blocks"].append({
                                "content": para.content,
                                "bounding_box": self._extract_bounding_box(para.bounding_regions[0]),
                                "role": getattr(para, 'role', 'paragraph')
                            })
                
                # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
                if hasattr(page, 'images'):
                    for img in page.images:
                        page_content["images"].append({
                            "bounding_box": self._extract_bounding_box(img),
                            "confidence": getattr(img, 'confidence', 0.0)
                        })
                
                # í…Œì´ë¸” ì •ë³´ ì¶”ì¶œ
                if result.tables:
                    for table in result.tables:
                        if table.bounding_regions and table.bounding_regions[0].page_number == page_idx + 1:
                            page_content["tables"].append({
                                "row_count": table.row_count,
                                "column_count": table.column_count,
                                "bounding_box": self._extract_bounding_box(table.bounding_regions[0])
                            })
                
                # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                page_text = ""
                for line in page.lines:
                    page_text += line.content + "\n"
                
                # ë¬¸ì„œ ìƒì„± - ì•ˆì „í•œ í‚¤ ìƒì„± (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)
                if page_text.strip():
                    # PDF íŒŒì¼ëª…ì—ì„œ ì•ˆì „í•œ í‚¤ ìƒì„±
                    safe_pdf_name = self._create_safe_document_key(pdf_name)
                    doc_id = f"{safe_pdf_name}_page_{page_idx + 1}"
                    
                    documents.append({
                        "id": doc_id,
                        "pdf_name": pdf_name,  # ì›ë³¸ íŒŒì¼ëª…ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
                        "page_number": page_idx + 1,
                        "content_type": "magazine_layout",
                        "text_content": page_text.strip(),
                        "layout_info": json.dumps(page_content),
                        "image_info": json.dumps(page_content["images"])
                    })
        
        return documents

    def _create_safe_document_key(self, pdf_name: str) -> str:
        """Azure Cognitive Searchì— ì•ˆì „í•œ ë¬¸ì„œ í‚¤ ìƒì„±"""
        import re
        
        # .pdf í™•ì¥ì ì œê±°
        safe_name = pdf_name.replace('.pdf', '').replace('.PDF', '')
        
        # í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ìë¥¼ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
        # í—ˆìš©: ì˜ë¬¸ì, ìˆ«ì, ì–¸ë”ìŠ¤ì½”ì–´(_), ëŒ€ì‹œ(-), ë“±í˜¸(=)
        safe_name = re.sub(r'[^a-zA-Z0-9_\-=]', '_', safe_name)
        
        # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ í•˜ë‚˜ë¡œ ë³€ê²½
        safe_name = re.sub(r'_+', '_', safe_name)
        
        # ì‹œì‘ê³¼ ëì˜ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
        safe_name = safe_name.strip('_')
        
        # ê¸¸ì´ ì œí•œ (Azure Search í‚¤ëŠ” ìµœëŒ€ 1024ì, ì•ˆì „í•˜ê²Œ 100ìë¡œ ì œí•œ)
        if len(safe_name) > 100:
            safe_name = safe_name[:100]
        
        return safe_name

    
    def _extract_bounding_box(self, region) -> Dict:
        """ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        if hasattr(region, 'polygon'):
            points = [(point.x, point.y) for point in region.polygon]
            return {
                "points": points,
                "x": min(p[0] for p in points),
                "y": min(p[1] for p in points),
                "width": max(p[0] for p in points) - min(p[0] for p in points),
                "height": max(p[1] for p in points) - min(p[1] for p in points)
            }
        return {}
    
    def _create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Azure OpenAI Embeddings APIë¡œ ë²¡í„° ìƒì„± - v1.x ë°©ì‹"""
        embeddings = []
        
        print(f"ğŸ“Š {len(texts)}ê°œ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        for i, text in enumerate(texts):
            try:
                # Azure OpenAI v1.x API ì‚¬ìš©
                response = self.openai_client.embeddings.create(
                    input=text,
                    model=self.embedding_model
                )
                embeddings.append(response.data[0].embedding)
                
                if (i + 1) % 10 == 0:
                    print(f"   ì§„í–‰ë¥ : {i + 1}/{len(texts)} ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (í…ìŠ¤íŠ¸ {i+1}): {e}")
                # ê¸°ë³¸ ë²¡í„° (1536 ì°¨ì›)
                embeddings.append([0.0] * 1536)
        
        print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
        return embeddings
    
    def _create_embeddings_batch(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„± - íš¨ìœ¨ì„± í–¥ìƒ"""
        all_embeddings = []
        
        print(f"ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ë¡œ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            try:
                response = self.openai_client.embeddings.create(
                    input=batch_texts,  # ì—¬ëŸ¬ í…ìŠ¤íŠ¸ í•œ ë²ˆì— ì²˜ë¦¬
                    model=self.embedding_model
                )
                
                batch_embeddings = [data.embedding for data in response.data]
                all_embeddings.extend(batch_embeddings)
                
                print(f"âœ… ë°°ì¹˜ {i//batch_size + 1} ì™„ë£Œ: {len(batch_texts)}ê°œ ì„ë² ë”©")
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {i//batch_size + 1} ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ê¸°ë³¸ ë²¡í„°ë¡œ ì±„ì›€
                for _ in batch_texts:
                    all_embeddings.append([0.0] * 1536)
        
        return all_embeddings
    
    def _upload_to_search_index(self, documents: List[Dict]):
        """Azure Cognitive Search ì¸ë±ìŠ¤ì— ë¬¸ì„œ ì—…ë¡œë“œ"""
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„ë² ë”© ìƒì„±
        texts = [doc["text_content"] for doc in documents]
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„± (íš¨ìœ¨ì„± í–¥ìƒ)
        if len(texts) > 50:
            embeddings = self._create_embeddings_batch(texts)
        else:
            embeddings = self._create_embeddings(texts)
        
        # ë¬¸ì„œì— ë²¡í„° ì¶”ê°€
        for i, doc in enumerate(documents):
            doc["content_vector"] = embeddings[i]
        
        # ë°°ì¹˜ ì—…ë¡œë“œ
        try:
            result = self.search_client.upload_documents(documents)
            print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ")
            
            # ì—…ë¡œë“œ ê²°ê³¼ í™•ì¸
            failed_count = 0
            for res in result:
                if not res.succeeded:
                    failed_count += 1
                    print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {res.key} - {res.error_message}")
            
            if failed_count == 0:
                print(f"ğŸ‰ ëª¨ë“  ë¬¸ì„œ ì—…ë¡œë“œ ì„±ê³µ!")
            else:
                print(f"âš ï¸ {failed_count}ê°œ ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨")
                    
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def search_similar_layouts(self, query_text: str, content_type: str = None, top_k: int = 5) -> List[Dict]:
        """ìœ ì‚¬í•œ ë ˆì´ì•„ì›ƒ ê²€ìƒ‰"""
        
        try:
            # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
            query_embeddings = self._create_embeddings([query_text])
            query_vector = query_embeddings[0]
            
            # ë²¡í„° ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            vector_query = VectorizedQuery(
                vector=query_vector,
                k_nearest_neighbors=top_k,
                fields="content_vector"
            )
            
            # ê²€ìƒ‰ ì‹¤í–‰
            search_params = {
                "vector_queries": [vector_query],
                "top": top_k,
                "select": ["id", "pdf_name", "page_number", "content_type", "text_content", "layout_info", "image_info"]
            }
            
            if content_type:
                search_params["filter"] = f"content_type eq '{content_type}'"
            
            results = self.search_client.search(**search_params)
            
            # ê²°ê³¼ ì •ë¦¬
            similar_layouts = []
            for result in results:
                layout_info = json.loads(result.get("layout_info", "{}"))
                image_info = json.loads(result.get("image_info", "[]"))
                
                similar_layouts.append({
                    "id": result["id"],
                    "pdf_name": result["pdf_name"],
                    "page_number": result["page_number"],
                    "text_content": result["text_content"],
                    "layout_info": layout_info,
                    "image_info": image_info,
                    "score": result.get("@search.score", 0.0)
                })
            
            return similar_layouts
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_layout_recommendations(self, content_description: str, image_count: int) -> List[Dict]:
        """ì½˜í…ì¸  ì„¤ëª…ê³¼ ì´ë¯¸ì§€ ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë ˆì´ì•„ì›ƒ ì¶”ì²œ"""
        
        # ì´ë¯¸ì§€ ìˆ˜ì— ë”°ë¥¸ ê²€ìƒ‰ ì¿¼ë¦¬ ì¡°ì •
        if image_count <= 1:
            query = f"single image layout simple clean {content_description}"
        elif image_count <= 3:
            query = f"multiple images grid layout {content_description}"
        else:
            query = f"many images gallery layout complex {content_description}"
        
        return self.search_similar_layouts(query, "magazine_layout", top_k=3)
    
    def check_pdf_compatibility(self, template_folder: str):
        """PDF íŒŒì¼ í˜¸í™˜ì„± ì²´í¬"""
        
        if not os.path.exists(template_folder):
            return
            
        pdf_files = [f for f in os.listdir(template_folder) if f.endswith('.pdf')]
        
        print("ğŸ“‹ PDF íŒŒì¼ í˜¸í™˜ì„± ì²´í¬:")
        large_files = []
        
        for pdf_file in pdf_files:
            pdf_path = os.path.join(template_folder, pdf_file)
            file_size = os.path.getsize(pdf_path) / (1024 * 1024)  # MB
            
            if file_size > 50:
                status = "âŒ ë§¤ìš° í¼"
                large_files.append(pdf_file)
            elif file_size > 20:
                status = "âš ï¸ í° íŒŒì¼ (ë¶„í•  ì˜ˆì •)"
                large_files.append(pdf_file)
            else:
                status = "âœ… ì í•©"
            
            print(f"   {pdf_file}: {file_size:.2f}MB - {status}")
        
        if large_files:
            print(f"\nğŸ”ª ë¶„í•  ì˜ˆì • íŒŒì¼: {len(large_files)}ê°œ")
            print(f"   â†’ ìë™ìœ¼ë¡œ 20MB ì´í•˜ë¡œ ë¶„í• ë©ë‹ˆë‹¤")
            print(f"   â†’ ì›ë³¸ íŒŒì¼ì€ backup_large_pdfs/ í´ë”ë¡œ ì´ë™ë©ë‹ˆë‹¤")
